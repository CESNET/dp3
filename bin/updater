#!/usr/bin/env python3
"""
Updater - periodically issues tasks to trigger regular updates of all entities in database.

Each entity should have '_lru' (Last Regular Update) field containing date and time of the lru .

The module fetches a list of entities whose _lru field value is older than gcd (Greatest Common Denominator)
computed from all configured intervals. 

Intervals are configured for every entity type separately.

It is also possible to issue additional events for each entity (e.g. when re-processing of some data is needed after
a configuration change). In order to do this, create a file '<CONFIG_FILE_DIR>/updater_events' whose contents are:
<entity_type> <event_name> <max_time>
The event_name is the name of the event (should begin with '!') to issue
for each entity along with '!every1d'. Events are issued only if current time is less than max_time (RFC 3339 format).
Since we usually want to issue the event once for each entity, max_time should be set to exactly 24 hours in the future.
The file can contain multiple such entries, one per line.
When max_time elapses, the entry in the file has no meaning, so it can be removed.
The file is checked every time a new batch of events is to be issued, so it's not needed to restart updater.
"""

from datetime import datetime, timedelta, timezone
from importlib.machinery import all_suffixes
from apscheduler.schedulers.background import BlockingScheduler
import numpy 

import os
import sys
import signal
import logging
import argparse
import time

sys.path.insert(1, os.path.join(os.path.dirname(__file__), '..'))
from dp3.database.database import EntityDatabase, MissingTableError
from dp3.task_processing.task_queue import TaskQueueWriter
from dp3.common.config import MissingConfigError, read_config_dir, load_attr_spec
from dp3.common.utils import parse_rfc_time

CONFIG_FILE_NAME = "updater_events" # name of the file with additional events to issue

last_fetch_time = datetime(1970, 1, 1)

def parse_timedelta(value:str):
    """
    Parse timedelta from config
    value format: XY
    X -> integer
    Y in ["m", "h", "d", "w", "y"]
    (m = minute, h = hour, d = day, w = week, y = year)
    Examples: 10m, 5d 4w, 1y, 2y

    returns interval in minutes
    """

    if len(value) == 0:
        raise Exception(f"Interval has to be in format INT(m|h|d|w|y)")
    try:
        x = int(value[:-1])
    except ValueError:
        raise Exception(f"Interval has to be in format INT(m|h|d|w|y)")
    y = value[-1]
    if y == "m":
        pass
    elif y == "h":
        x = x*60
    elif y == "d":
        x = x*1440 # 60*24
    elif y == "w":
        x = x*10080 # 60*24*7
    elif y == "y":
        x = x*525600 # 60*24*365
    else:
        raise Exception(f"Interval has to be in format INT(m|h|d|w|y)")
    return x


def stop(signal, frame):
    """
    Stop receiving events.
    Will be evoked on catching SIGINT signal.
    """
    tqw.disconnect()
    scheduler.shutdown()


def issue_events(db,tqw,log,etypes,args,updater_config):
    global last_fetch_time
    start_time = datetime.utcnow()

    # Load the file with additional events
    additional_events_file = os.path.join(args.config_dir, CONFIG_FILE_NAME)
    additional_events = {etype: [] for etype in etypes}
    try:
        for line in open(additional_events_file, "r"):
            if not line.strip() or line.startswith("#"):
                continue # skip empty lines and comments
            etype, event, max_time = line.split(maxsplit=2)
            if etype not in etypes:
                raise ValueError(f"Unsupported entity type '{etype}'")
            try:
                max_time = parse_rfc_time(max_time)
            except ValueError:
                raise ValueError("Wrong timestamp format (RFC 3339 required)")
            if start_time > max_time:
                continue # expired entry, ignore
            additional_events[etype].append(event)
            log.debug(f"Additional event '{event}' will be issued for all entities of type '{etype}'")
    except FileNotFoundError:
        pass # File doesn't exist - that's OK, do nothing
    except Exception as e:
        # Other error - print message and continue
        log.error(f"Error in the file with additional events ('{additional_events_file}', line '{line}''): {e}")

    for etype in etypes:
        # Check if updater is configured for given etype 
        events = events_config.get(etype, {})
        ttl_tokens = ttl_tokens_config.get(etype, {})
        if not events and not ttl_tokens:
            # both events and ttl tokens are not configured for etype
            # nothing to do for etype
            continue

        # Get all intervals from config - needed for computing gcd
        intervals = [_ for _ in events.values()]
        ttl_tokens_intervals = [_ for _ in ttl_tokens.values()]

        # Updater calculates gcd (greatest common denominator) 
        # of all configured intervals for given etype.
        # Updater will than process every entity once per gcd hours
        # and check if any event needs to be issued
        all_intervals = intervals+ttl_tokens_intervals
        gcd = timedelta(minutes=int(numpy.gcd.reduce(all_intervals)))

        
        # Get list of IDs that needs to be checked
        # (i.e. all entities with _lru+gcd less then current time
        # AND greater than time of the last query - this is important since
        # _lru of an entity is set to correct value only after the update
        # is processed, which may take some time, and we don't want to
        # fetch the same entity twice). Number of fetched entities can't be 
        # limited. Because of the last query time ignored entities wouldn't get
        # processed at all.
        log.debug(f"Getting list of '{etype}' entities to update ...")
        entities = db.last_updated(etype,start_time-gcd,last_fetch_time-gcd)

        # Update _lru and possibly issue configured events
        count = 0 # number of issued events - needed for limit implementation
        for eid,lru,ts_added in entities:
            # Check record's TTL tokens, and either issue normal events or delete the record.
            old_ttl_tokens = db.get_attrib(etype, eid, "_ttl")
            new_ttl_tokens = {}

            # check if the record still has some valid TTL token. 
            # If not (all token has expired), the record is no longer valid and record is deleted.
            # if ttl token wasn't configured don't delete any record (only check for invalid tokens from previous configuration)        
            delete_entry = True if ttl_tokens else False
            print(f"zistujem ci sa ma vymazat {eid} {old_ttl_tokens}")
            for token,ts_created in old_ttl_tokens.items():
                created = parse_rfc_time(ts_created)
                try:
                    val = ttl_tokens[token]
                except KeyError:
                    log.error(f"Unknown TTL token '{token}'. Removing this token from record {eid}!")
                    continue
                # "*" is special value which will hold record in database indefinitely
                if val == "*":
                    delete_entry = False
                    new_ttl_tokens["token"] = created
                else:
                    print(f"{token} {created+timedelta(minutes=val)} {start_time}")
                    if created + timedelta(minutes=val) > start_time:
                        delete_entry = False
                        new_ttl_tokens[token] = ts_created

            if delete_entry:
                # no active token was found - remove record
                tqw.put_task(etype=etype, ekey=eid, delete=True, src="updater")
                continue

            # Time passed until last regular update
            delta_lru = lru - ts_added
            # Time passed until now
            delta_now = start_time - ts_added

            issue_events = []
            for event,interval in events.items():
                # From passed time we can calculate number of events 
                # that was/should be issued
                old_interval_count = (delta_lru) // timedelta(minutes=interval)
                new_interval_count = (delta_now) // timedelta(minutes=interval)

                # If new value is bigger than older we need to issue event
                # In case of longer unaviability of dp3 
                # max. one event of the same type will be issued.
                if old_interval_count < new_interval_count:
                    issue_events.append(event)

            # Additional events should be issued once per day
            old_interval_count = (delta_lru) // timedelta(days=1)
            new_interval_count = (delta_now) // timedelta(days=1)
            if old_interval_count < new_interval_count:
                for add_event in additional_events[etype]:
                    issue_events.append(add_event)

            # Set _lru and _ttl to new value 
            new_lru = ts_added + (delta_now // gcd) * gcd
            attr_updates = [{
                "op": "set", 
                "attr": "_lru", 
                "val": new_lru,
            },{
                "op": "set",
                "attr": "_ttl",
                "val": new_ttl_tokens
            }]
            count += 1
            # issue max. 'limit' events per second
            if count % args.limit == 0:
                log.debug(f"Updated {count} entities.")
                remaining_time = (start_time + timedelta(seconds=count//args.limit) - datetime.now()).total_seconds()
                if remaining_time > 0:
                    log.debug(f"Going to sleep for {remaining_time} seconds.")
                    time.sleep(remaining_time)

            # Each update request contains the corresponding events,
            # and a change of the '_lru' and "_ruc" attributes.
            tqw.put_task(etype=etype, ekey=eid, events=issue_events, attr_updates=attr_updates, src="updater")

    last_fetch_time = start_time 

if __name__ == "__main__":
    # parse arguments
    parser = argparse.ArgumentParser(
        prog="updater",
        description='Periodically issues update events for entities with NRU (next regular update) fields.'
    )
    parser.add_argument('app_name', metavar='APP_NAME',
                        help="Name of the application to distinct it from other DP3-based apps (it's used as a prefix of "
                            "RabbitMQ queue names, for example).")
    parser.add_argument('config_dir', metavar='CONFIG_DIRECTORY',
                        help="Path to a directory containing configuration files (e.g. /etc/my_app/config)")
    parser.add_argument('-l', '--limit', metavar='N', dest='limit', type=int,
                    help='Maximum number of entities for which the events would be issued per second. (default: 100000)', 
                    default=100000
    )
    parser.add_argument('-p', '--period', metavar='N', dest='period', type=int,
                    help='Number of seconds between two event issues. (default: 10)', 
                    default=10
    )
    group = parser.add_mutually_exclusive_group()
    group.add_argument("-v", dest="verbose", action="store_true", help="Verbose mode - print more info, including IDs of all records being updated.")
    group.add_argument("-q", dest="quiet", action="store_true", help="Quite mode - only print errors")
    args = parser.parse_args()

    # Set up logging
    LOGFORMAT = "%(asctime)-15s,%(name)s [%(levelname)s] %(message)s"
    LOGDATEFORMAT = "%Y-%m-%dT%H:%M:%S"
    # log level of all other loggers (those of dp3 internal components)
    log_level = logging.INFO if args.verbose else logging.WARNING
    logging.basicConfig(level=log_level, format=LOGFORMAT, datefmt=LOGDATEFORMAT)
    # logger for this script
    log = logging.getLogger('Updater')
    log.setLevel(logging.WARNING if args.quiet else logging.DEBUG)

    # Load configuration
    log.info(f"Loading config dir {args.config_dir}")
    config = read_config_dir(args.config_dir, recursive=True)
    updater_config = config.get("updater")
    db_config = config.get("database")
    attr_spec = load_attr_spec(config.get("db_entities"))
    rabbit_params = config.get('processing_core.rabbitmq', {})
    num_processes = config.get('processing_core.worker_processes')
    etypes = [etype for etype in attr_spec]
    try:
        events_config = {
            etype:{
                event: parse_timedelta(value) for event,value in etype_conf.items()
            } for etype,etype_conf in updater_config.get("events").items()
        }
        ttl_tokens_config = {
            etype:{
                ttl_token: parse_timedelta(value) for ttl_token,value in etype_conf.items()
            } for etype,etype_conf in updater_config.get("ttl_tokens").items()
        }
    except Exception as e:
        log.error(e)
        sys.exit(1)

    # Connect to DB and get list of entities to update
    log.info("Connecting to DB...")
    db = EntityDatabase(db_config, attr_spec)
    
    log.info("Connecting to RabbitMQ...")
    tqw = TaskQueueWriter(args.app_name, num_processes, rabbit_params)
    try:
        tqw.connect()
        tqw.check()
    except Exception as e:
        log.error(e)
        sys.exit(2)

    # Create scheduler
    log.info("Creating scheduler...")
    scheduler = BlockingScheduler(timezone="UTC")
    scheduler.add_job(lambda: issue_events(db,tqw,log,etypes,args,updater_config), trigger='cron', second='*/' + str(args.period))

    # Register SIGINT handler to stop the updater
    signal.signal(signal.SIGINT, stop)

    scheduler.start()
