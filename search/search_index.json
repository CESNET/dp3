{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dynamic Profile Processing Platform (DP\u00b3)","text":"<p>DP\u00b3 is a platform helps to keep a database of information (attributes) about individual entities (designed for IP addresses and other network identifiers, but may be anything), when the data constantly changes in time.</p> <p>DP\u00b3 doesn't do much by itself, it must be supplemented by application-specific modules providing and processing data.</p> <p>This is a basis of CESNET's \"Asset Discovery Classification and Tagging\" (ADiCT) project, focused on discovery and classification of network devices, but the platform itself is general and should be usable for any kind of data.</p> <p>For an introduction about how it works, see please check out the  architecture, data-model  and database config pages. The mechanics of how the platform handles history of data is described in the history management page.</p> <p>Then you should be able to create a DP\u00b3 app using the provided setup utility as described in the install page and start tinkering!</p>"},{"location":"#repository-structure","title":"Repository structure","text":"<ul> <li><code>dp3</code> - Python package containing code of the processing core and the API</li> <li><code>config</code> - default/example configuration</li> <li><code>install</code> - deployment configuration</li> </ul>"},{"location":"api/","title":"API","text":"<p>DP\u00b3's has HTTP API which you can use to post datapoints and to read data stored in DP\u00b3. As the API is made using FastAPI, there is also an interactive documentation available at <code>/docs</code> endpoint.</p> <p>There are several API endpoints:</p> <ul> <li><code>GET /</code>: check if API is running (just returns <code>It works!</code> message)</li> <li><code>POST /datapoints</code>: insert datapoints into DP\u00b3</li> <li><code>GET /entity/&lt;entity_type&gt;</code>: list current snapshots of all entities of given type</li> <li><code>GET /entity/&lt;entity_type&gt;/get</code>: get current snapshots of entities of entity type</li> <li><code>GET /entity/&lt;entity_type&gt;/count</code>: get total document count for query of entity type</li> <li><code>GET /entity/&lt;entity_type&gt;/&lt;entity_id&gt;</code>: get data of entity with given entity id</li> <li><code>GET /entity/&lt;entity_type&gt;/&lt;entity_id&gt;/get/&lt;attr_id&gt;</code>: get attribute value</li> <li><code>GET /entity/&lt;entity_type&gt;/&lt;entity_id&gt;/set/&lt;attr_id&gt;</code>: set attribute value</li> <li><code>GET /entity/&lt;entity_type&gt;/_/distinct/&lt;attr_id&gt;</code>: get distinct attribute values and their counts based on latest snapshots</li> <li><code>DELETE /entity/&lt;entity_type&gt;/&lt;entity_id&gt;</code>: delete entity data for given id</li> <li><code>POST /entity/&lt;entity_type&gt;/&lt;entity_id&gt;/ttl</code>: extend TTLs of the specified entity</li> <li><code>GET /entities</code>: list entity configuration</li> <li><code>GET /control/&lt;action&gt;</code>: send a pre-defined action into execution queue.</li> <li><code>GET /telemetry/sources_validity</code>: get information about the validity of the data sources</li> </ul>"},{"location":"api/#index","title":"Index","text":"<p>Health check.</p>"},{"location":"api/#request","title":"Request","text":"<p><code>GET /</code></p>"},{"location":"api/#response","title":"Response","text":"<p><code>200 OK</code>:</p> <p><code>{   \"detail\": \"It works!\" }</code></p>"},{"location":"api/#insert-datapoints","title":"Insert datapoints","text":""},{"location":"api/#request_1","title":"Request","text":"<p><code>POST /datapoints</code></p> <p>All data are written to DP\u00b3 in the form of datapoints. A datapoint sets a value of a given attribute of given entity.</p> <p>It is a JSON-encoded object with the set of keys defined in the table below. Presence of some keys depends on the primary type of the attribute (plain/observations/timseries).</p> <p>Payload to this endpoint is JSON array of datapoints. For example:</p> <pre><code>[\n   { DATAPOINT1 },\n   { DATAPOINT2 }\n]\n</code></pre> Key Description Data-type Required? Plain Observations Timeseries <code>type</code> Entity type string mandatory \u2714 \u2714 \u2714 <code>id</code> Entity identification string mandatory \u2714 \u2714 \u2714 <code>attr</code> Attribute name string mandatory \u2714 \u2714 \u2714 <code>v</code> The value to set, depends on attr. type and data-type, see below -- mandatory \u2714 \u2714 \u2714 <code>t1</code> Start time of the observation interval string (RFC 3339 format) mandatory -- \u2714 \u2714 <code>t2</code> End time of the observation interval string (RFC 3339 format) optional, default=<code>t1</code> -- \u2714 \u2714 <code>c</code> Confidence float (0.0-1.0) optional, default=1.0 -- \u2714 \u2714 <code>src</code> Identification of the information source string optional, default=\"\" \u2714 \u2714 \u2714 <p>More details depends on the particular type of the attribute.</p>"},{"location":"api/#examples-of-datapoints","title":"Examples of datapoints","text":""},{"location":"api/#plain","title":"Plain","text":"<pre><code>{\n  \"type\": \"ip\",\n  \"id\": \"192.168.0.1\",\n  \"attr\": \"note\",\n  \"v\": \"My home router\",\n  \"src\": \"web_gui\"\n}\n</code></pre>"},{"location":"api/#observations","title":"Observations","text":"<pre><code>{\n  \"type\": \"ip\",\n  \"id\": \"192.168.0.1\",\n  \"attr\": \"open_ports\",\n  \"v\": [22, 80, 443],\n  \"t1\": \"2022-08-01T12:00:00\",\n  \"t2\": \"2022-08-01T12:10:00\",\n  \"src\": \"open_ports_module\"\n}\n</code></pre>"},{"location":"api/#timeseries","title":"Timeseries","text":"<p><code>regular</code>:</p> <pre><code>{\n  ...\n  \"t1\": \"2022-08-01T12:00:00\",\n  \"t2\": \"2022-08-01T12:20:00\", // assuming time_step = 5 min\n  \"v\": {\n    \"a\": [1, 3, 0, 2]\n  }\n}\n</code></pre> <p><code>irregular</code>: timestamps must always be present</p> <pre><code>{\n  ...\n  \"t1\": \"2022-08-01T12:00:00\",\n  \"t2\": \"2022-08-01T12:05:00\",\n  \"v\": {\n    \"time\": [\"2022-08-01T12:00:00\", \"2022-08-01T12:01:10\", \"2022-08-01T12:01:15\", \"2022-08-01T12:03:30\"],\n    \"x\": [0.5, 0.8, 1.2, 0.7],\n    \"y\": [-1, 3, 0, 0]\n  }\n}\n</code></pre> <p><code>irregular_interval</code>:</p> <pre><code>{\n  ...\n  \"t1\": \"2022-08-01T12:00:00\",\n  \"t2\": \"2022-08-01T12:05:00\",\n  \"v\": {\n    \"time_first\": [\"2022-08-01T12:00:00\", \"2022-08-01T12:01:10\", \"2022-08-01T12:01:15\", \"2022-08-01T12:03:30\"],\n    \"time_last\": [\"2022-08-01T12:01:00\", \"2022-08-01T12:01:15\", \"2022-08-01T12:03:00\", \"2022-08-01T12:03:40\"],\n    \"x\": [0.5, 0.8, 1.2, 0.7],\n    \"y\": [-1, 3, 0, 0]\n  }\n}\n</code></pre>"},{"location":"api/#relations","title":"Relations","text":"<p>Can be represented using both plain attributes and observations. The difference will be only in time specification. Two examples using observations:</p> <p>no data - <code>link&lt;mac&gt;</code>: Sent as a dictionary with a single <code>\"eid\"</code> key.</p> <pre><code>{\n  \"type\": \"ip\",\n  \"id\": \"192.168.0.1\",\n  \"attr\": \"mac_addrs\",\n  \"v\": {\"eid\": \"AA:AA:AA:AA:AA\"},\n  \"t1\": \"2022-08-01T12:00:00\",\n  \"t2\": \"2022-08-01T12:10:00\"\n}\n</code></pre> <p>with additional data - <code>link&lt;ip, int&gt;</code>: Sent as a dictionary with <code>\"eid\"</code> and <code>\"data\"</code> keys.</p> <pre><code>{\n  \"type\": \"ip\",\n  \"id\": \"192.168.0.1\",\n  \"attr\": \"ip_dep\",\n  \"v\": {\"eid\": \"192.168.0.2\", \"data\": 22},\n  \"t1\": \"2022-08-01T12:00:00\",\n  \"t2\": \"2022-08-01T12:10:00\"\n}\n</code></pre>"},{"location":"api/#response_1","title":"Response","text":"<p><code>200 OK</code>:</p> <pre><code>Success\n</code></pre> <p><code>400 Bad request</code>:</p> <p>Returns some validation error message, for example:</p> <pre><code>1 validation error for DataPointObservations_some_field\nv -&gt; some_embedded_dict_field\n  field required (type=value_error.missing)\n</code></pre>"},{"location":"api/#list-entities","title":"List entities","text":"<p>Deprecated</p> <p>This endpoint is deprecated and will be removed in the future,  Use <code>GET /entity/&lt;entity_type&gt;/get</code> to get paged documents and <code>GET /entity/&lt;entity_type&gt;/count</code> to get total document count for query.</p> <p>List latest snapshots of all ids present in database under entity type,  filtered by <code>generic_filter</code> and <code>fulltext_filters</code>. Contains only the latest snapshot per entity. </p> <p>Counts all results for given query.</p>"},{"location":"api/#request_2","title":"Request","text":"<p><code>GET /entity/&lt;entity_type&gt;</code></p> <p>Optional query parameters:</p> <ul> <li>skip: how many entities to skip (default: 0)</li> <li>limit: how many entities to return (default: 20)</li> <li>fulltext_filters: dictionary of fulltext filters (default: no filters)</li> <li>generic_filter: dictionary of generic filters (default: no filters)</li> </ul>"},{"location":"api/#response_2","title":"Response","text":"<pre><code>{\n  \"time_created\": \"2023-07-04T12:10:38.827Z\",\n  \"data\": [\n    {}\n  ]\n}\n</code></pre>"},{"location":"api/#get-entities","title":"Get entities","text":"<p>Get a list of latest snapshots of all ids present in database under entity type, filtered by <code>generic_filter</code> and <code>fulltext_filters</code>. Contains only the latest snapshot per entity.</p> <p>Uses pagination, default limit is 20, setting to 0 will return all results.</p> <p>Fulltext filters are interpreted as regular expressions. Only string values may be filtered this way. There's no validation that queried attribute can be fulltext filtered. Only plain and observation attributes with string-based data types can be queried. Array and set data types are supported as well as long as they are not multi value at the same time. If you need to filter EIDs, use attribute <code>eid</code>.</p> <p>Generic filter allows filtering using generic MongoDB query (including <code>$and</code>, <code>$or</code>,<code>$lt</code>, etc.). For querying non-JSON-native types, you can use the following magic strings, as are defined by the search &amp; replace <code>magic</code> module.</p> <p>There are no attribute name checks (may be added in the future).</p> <p>Generic and fulltext filters are merged - fulltext overrides conflicting keys.</p>"},{"location":"api/#request_3","title":"Request","text":"<p><code>GET /entity/&lt;entity_type&gt;</code></p> <p>Optional query parameters:</p> <ul> <li>skip: how many entities to skip (default: 0)</li> <li>limit: how many entities to return (default: 20)</li> <li>fulltext_filters: dictionary of fulltext filters (default: no filters)</li> <li>generic_filter: dictionary of generic filters (default: no filters)</li> </ul>"},{"location":"api/#response_3","title":"Response","text":"<pre><code>{\n  \"time_created\": \"2023-07-04T12:10:38.827Z\",\n  \"data\": [\n    {}\n  ]\n}\n</code></pre>"},{"location":"api/#count-entities","title":"Count entities","text":"<p>Count latest snapshots of all ids present in database under entity type, filtered by <code>generic_filter</code> and <code>fulltext_filters</code>. See <code>GET /entity/&lt;entity_type&gt;/get</code> for details on filter format.</p>"},{"location":"api/#request_4","title":"Request","text":"<p><code>GET /entity/&lt;entity_type&gt;/count</code></p> <p>Optional query parameters:</p> <ul> <li>fulltext_filters: dictionary of fulltext filters (default: no filters)</li> <li>generic_filter: dictionary of generic filters (default: no filters)</li> </ul>"},{"location":"api/#response_4","title":"Response","text":"<pre><code>{\n  \"total_count\": 0\n}\n</code></pre>"},{"location":"api/#get-eid-data","title":"Get Eid data","text":"<p>Get data of entity type's eid.</p> <p>Contains all snapshots and master record. Snapshots are ordered by ascending creation time.</p>"},{"location":"api/#request_5","title":"Request","text":"<p><code>GET /entity/&lt;entity_type&gt;/&lt;entity_id&gt;</code></p> <p>Optional query parameters:</p> <ul> <li>date_from: date-time string</li> <li>date_to: date-time string</li> </ul>"},{"location":"api/#response_5","title":"Response","text":"<pre><code>{\n  \"empty\": true,\n  \"master_record\": {},\n  \"snapshots\": [\n    {}\n  ]\n}\n</code></pre>"},{"location":"api/#get-attr-value","title":"Get attr value","text":"<p>Get attribute value</p> <p>Value is either of:</p> <ul> <li>current value: in case of plain attribute</li> <li>current value and history: in case of observation attribute</li> <li>history: in case of timeseries attribute</li> </ul>"},{"location":"api/#request_6","title":"Request","text":"<p><code>GET /entity/&lt;entity_type&gt;/&lt;entity_id&gt;/get/&lt;attr_id&gt;</code></p> <p>Optional query parameters:</p> <ul> <li>date_from: date-time string</li> <li>date_to: date-time string</li> </ul>"},{"location":"api/#response_6","title":"Response","text":"<pre><code>{\n  \"attr_type\": 1,\n  \"current_value\": \"string\",\n  \"history\": []\n}\n</code></pre>"},{"location":"api/#set-attr-value","title":"Set attr value","text":"<p>Set current value of attribute</p> <p>Internally just creates datapoint for specified attribute and value.</p> <p>This endpoint is meant for <code>editable</code> plain attributes -- for direct user edit on DP3 web UI.</p>"},{"location":"api/#request_7","title":"Request","text":"<p><code>POST /entity/&lt;entity_type&gt;/&lt;entity_id&gt;/set/&lt;attr_id&gt;</code></p> <p>Required request body:</p> <pre><code>{\n  \"value\": \"string\"\n}\n</code></pre>"},{"location":"api/#response_7","title":"Response","text":"<pre><code>{\n  \"detail\": \"OK\"\n}\n</code></pre>"},{"location":"api/#get-distinct-values","title":"Get distinct values","text":"<p>Gets distinct attribute values and their counts based on latest snapshots</p> <p>Useful for displaying <code>&lt;select&gt;</code> enumeration fields.</p> <p>Works for all plain and observation data types except <code>dict</code> and <code>json</code>.</p>"},{"location":"api/#request_8","title":"Request","text":"<p><code>GET /entity/&lt;entity_type&gt;/_/distinct/&lt;attr_id&gt;</code></p>"},{"location":"api/#response_8","title":"Response","text":"<pre><code>{\n  \"value1\": 10,\n  \"value2\": 5,\n  \"value3\": 43\n}\n</code></pre>"},{"location":"api/#delete-eid-data","title":"Delete Eid data","text":"<p>Delete master record and snapshots with the specified <code>etype</code> and <code>eid</code>.</p> <p>Raw datapoints are not deleted, and the entity can be restored by sending new datapoints with the same <code>etype</code> and <code>eid</code>.</p>"},{"location":"api/#request_9","title":"Request","text":"<p><code>DELETE /entity/&lt;entity_type&gt;/&lt;entity_id&gt;</code></p>"},{"location":"api/#response_9","title":"Response","text":"<pre><code>{\n  \"detail\": \"OK\"\n}\n</code></pre>"},{"location":"api/#extend-ttls","title":"Extend TTLs","text":"<p>Extend TTLs of the specified entity.</p> <p>Raw datapoints are not deleted, and the entity can be restored by sending new datapoints with the same <code>etype</code> and <code>eid</code>.</p>"},{"location":"api/#request_10","title":"Request","text":"<p><code>POST /entity/&lt;entity_type&gt;/&lt;entity_id&gt;/ttl</code></p> <p>The request body must be a dictionary of TTLs to extend, with string keys to identify the type of TTL. The values must be UTC timestamps, for example:</p> <pre><code>{\n  \"user_interaction\": \"2024-10-01T12:03:00\",\n  \"api_dependency\": \"2024-10-08T12:00:00\"\n}\n</code></pre> <p>TTLs of the same name will be extended, and you add as many TTL names as you want.</p>"},{"location":"api/#response_10","title":"Response","text":"<pre><code>{\n  \"detail\": \"OK\"\n}\n</code></pre>"},{"location":"api/#entities","title":"Entities","text":"<p>List entity types</p> <p>Returns dictionary containing all entity types configured -- their simplified configuration and current state information.</p>"},{"location":"api/#request_11","title":"Request","text":"<p><code>GET /entities</code></p>"},{"location":"api/#response_11","title":"Response","text":"<pre><code>{\n  \"&lt;entity_id&gt;\": {\n    \"id\": \"&lt;entity_id&gt;\",\n    \"id_data_type\": \"&lt;entity_spec.id_data_type&gt;\",\n    \"name\": \"&lt;entity_spec.name&gt;\",\n    \"attribs\": \"&lt;MODEL_SPEC.attribs(e_id)&gt;\",\n    \"eid_estimate_count\": \"&lt;DB.estimate_count_eids(e_id)&gt;\"\n  },\n  ...\n}\n</code></pre>"},{"location":"api/#control","title":"Control","text":"<p>Execute Action - Sends the given action into execution queue.</p> <p>You can see the enabled actions in <code>/config/control.yml</code>, available are:</p> <ul> <li><code>make_snapshots</code> - Makes an out-of-order snapshot of all entities</li> <li><code>refresh_on_entity_creation</code> - Re-runs the <code>on_entity_creation</code> callback for selected <code>etype</code></li> <li><code>refresh_module_config</code> - Re-runs the <code>load_config</code> for selected module and will refresh the values derived by the module when configured to do so</li> </ul> <p>You can learn more about the actions in the Actions section of the <code>Control</code> configuration documentation.</p>"},{"location":"api/#request_12","title":"Request","text":"<p><code>GET /control/&lt;action&gt;</code></p>"},{"location":"api/#response_12","title":"Response","text":"<pre><code>{\n  \"detail\": \"OK\"\n}\n</code></pre>"},{"location":"api/#telemetry","title":"Telemetry","text":"<p>Returns information about the validity of the data sources, i.e. when the last datapoint was received from each source.</p>"},{"location":"api/#request_13","title":"Request","text":"<p><code>GET /telemetry/sources_validity</code></p>"},{"location":"api/#response_13","title":"Response","text":"<pre><code>{\n  \"module1@collector1\": \"2023-10-03T11:59:58.063000\",\n  \"module2@collector1\": \"2023-12-06T09:09:37.165000\",\n  \"module3@collector2\": \"2023-12-08T15:52:55.282000\",\n  ...\n}\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":"<p>DP\u00b3 is generic platform for data processing.  It's currently used in systems for management of network devices in CESNET,  but during development we focused on making DP\u00b3 as universal as possible.</p> <p>This page describes the high-level architecture of DP\u00b3 and the individual components.</p>"},{"location":"architecture/#data-points","title":"Data-points","text":"<p>The base unit of data that DP\u00b3 uses is called a data-point, which looks like this:</p> <pre><code>{\n  \"type\": \"ip\", // (1)!\n  \"id\": \"192.168.0.1\", // (2)!\n  \"attr\": \"open_ports\", // (3)!\n  \"v\": [22, 80, 443], // (4)!\n  \"t1\": \"2022-08-01T12:00:00\", // (5)!\n  \"t2\": \"2022-08-01T12:10:00\",\n  \"src\": \"open_ports_module\" // (6)!\n}\n</code></pre> <ol> <li>A data-point's value belongs to a specific (user-defined) entity type, declared by the <code>type</code>.</li> <li>The exact entity is using its entity id in <code>id</code>. </li> <li>Each entity has multiple defined attributes, the <code>attr</code> field specifies the attribute of the data-point.</li> <li>Finally, the data-point's value is sent in the <code>v</code> field.</li> <li>Data-point validity interval is defined using the <code>t1</code> and <code>t2</code> field. </li> <li>To easily determine the data source of this data-point, you can optionally provide an identifier using the <code>src</code> field.</li> </ol> <p>This example shows an example of an observations data-point (given it has a validity interval), to learn more about the different types of data-points, please see the API documentation.</p>"},{"location":"architecture/#platform-architecture","title":"Platform Architecture","text":"DP\u00b3 architecture <p>The DP\u00b3 architecture as shown in the figure above consists of several components,  where the DP\u00b3 provided components are shown in blue:</p> <ul> <li>The HTTP API (built with Fast API) validates incoming data-points and sends them    for processing to the task distribution queues.   It also provides access to the database for web or scripts.</li> <li>The task distribution is done using RabbitMQ queues, which distribute tasks between workers.</li> <li>The main code of the platform runs in parallel worker processes.    In the worker processes is a processing core,   which performs all updates and communicates with core modules and   application-specific secondary modules when appropriate.</li> <li>Both the HTTP API and worker processes use the database API to access the entity database,   currently implemented in MongoDB.</li> <li>The generic web interface, or a set of generic components has been implemented as   Grafana plugin.   It's primary use-case is to help quickly and intuitively visualize data while retaining   genericity.   Ultimately, web interface is application-specific. However, we wanted to provide a quick start   alternative.   Implementation as Grafana plugin also implies option to combine DP\u00b3 with other data sources.</li> </ul> <p>The application-specific components, shown in yellow-orange, are as following:</p> <ul> <li>The entity configuration via <code>yml</code> files determines the entities and their attributes,   together with the specifics of platform behavior on these entities.    For details of entity configuration, please see the database entities configuration page.</li> <li>The distinction between primary and secondary modules is such that primary modules   send data-points into the system using the HTTP API, while secondary modules react   to the data present in the system, e.g.: altering the data-flow in an application-specific manner,   deriving additional data based on incoming data-points or performing data correlation on entity snapshots.   For primary module implementation, the API documentation may be useful,    also feel free to check out the dummy_sender script in <code>/scripts/dummy_sender.py</code>.   Feel free to check out the secondary module API documentation, or you can also    refer to the CallbackRegistrar code reference or    check out the test modules in <code>/modules/</code> or <code>/tests/modules/</code>.</li> </ul>"},{"location":"architecture/#data-flow","title":"Data flow","text":"<p>This section describes the data flow within the platform.</p> <p> </p> DP\u00b3 Data flow <p>The above figure shows a zoomed in view of the worker-process from the architecture figure. Incoming Tasks, which carry data-points from the API,  are passed to secondary module callbacks configured on new data point, or around entity creation. These modules may create additional data points or perform any other action.  When all registered callbacks are processed, the resulting data is written to two collections: The data-point (DP) history collection, where the raw data-points are stored until archivation, and the profile history collection, where a document is stored for each entity id with the relevant history. You can find these collections in the database under the names <code>{entity}#raw</code> and <code>{entity}#master</code>.</p> <p>DP\u00b3 periodically creates new profile snapshots, triggered by the Scheduler. Snapshots take the profile history, and compute the current value of the profile,  reducing each attribute history to a single value.  The snapshot creation frequency is configurable. Snapshots are created on a per-entity basis, but all linked entities are processed at the same time. This means that when snapshots are created, the registered snapshot callbacks can access any linked entities for their data correlation needs.  After all the correlation callbacks are called, the snapshot is written to the profile snapshot collection, for which it can be accessed via the API. The collection is accessible under the name <code>{entity}#snapshots</code>.</p>"},{"location":"data_model/","title":"DP\u00b3 data model","text":"<p>Basic elements of the DP\u00b3 data model are entity types (like classes), each entity (or class instance) has a set of attributes. Each attribute has some value (associated to a particular entity type), timestamp (history of previous values can be stored) and optionally confidence value.</p> <p>Entities may be mutually connected. See Relationships below.</p>"},{"location":"data_model/#exemplary-system","title":"Exemplary system","text":"<p>In this chapter, we will illustrate details on an exemplary system. Imagine you are developing data model for bus tracking system. You have to store these data:</p> <ul> <li>label: Custom label for the bus set by administrator in web interface.</li> <li>location: Location of the bus in a particular time. Value are GPS   coordinates (array of latitude and longitude).</li> <li>speed: Speed of the bus in a particular time.</li> <li>passengers getting in and out: Number of passengers getting in or out of   the bus. Distinguished by the doors used (front, middle, back). Bus control   unit sends counters value every 10 minutes.</li> </ul> <p>Also, map displaying current position of all buses is required.</p> <p>(In case you are interested, configuration of database entities for this system is available in DB entities chapter.)</p> <p>To make everything clear and more readable, all example references below are typesetted as quotes.</p>"},{"location":"data_model/#types-of-attributes","title":"Types of attributes","text":"<p>There are 3 types of attributes:</p>"},{"location":"data_model/#plain","title":"Plain","text":"<p>Common attributes with only one value of some data type. There's no history stored, but timestamp of last change is available.</p> <p>Very useful for:</p> <ul> <li> <p>data from external source, when you only need to have current value</p> </li> <li> <p>notes and other manually entered information</p> </li> </ul> <p>This is exactly what we need for label in our bus tracking system. Administor labels particular bus inside web interface and we use this label until it's changed - particularly display label next to a marker on a map. No history is needed and it has 100% confidence.</p> <p>Detailed information about plain attribute configuration can be found in DB configuration.</p>"},{"location":"data_model/#observations","title":"Observations","text":"<p>Attributes with history of values at some time or interval of time. Consequently, we can derive value at any time (most often not now) from these values.</p> <p>Each value may have associated confidence.</p> <p>These attributes may be single or multi value (multiple current values in one point in time).</p> <p>Very useful for data where both current value and history is needed.</p> <p>In our example, location is great use-case for observations type. We need to track position of the bus in time and store the history. Current location is very important. Let's suppose, we also need to do oversampling by predicting where is the bus now, eventhout we received last data-point 2 minutes ago. This is all possible (predictions using custom secondary modules).</p> <p>The same applies to speed. It can also be derived from location.</p> <p>Detailed information about observations attribute configuration can be found in DB configuration. The History mangement page describes how DP\u00b3 handles history.</p>"},{"location":"data_model/#timeseries","title":"Timeseries","text":"<p>One or more numeric values for a particular time.</p> <p>In this attribute type: history &gt; current value. In fact, no explicit current value is provided.</p> <p>Very useful for:</p> <ul> <li> <p>any kind of history-based analysis</p> </li> <li> <p>logging of events/changes</p> </li> </ul> <p>May be:</p> <ul> <li> <p>regular: sampling is regular   Example: datapoint is created every x minutes</p> </li> <li> <p>irregular: sampling is irregular   Example: datapoint is created when some event occurs</p> </li> <li> <p>irregular intervals: sampling is irregular and includes two timestamps (from when till when were provided data gathered)   Example: Some event triggers 5 minute monitoring routine. When this routine finishes, it creates datapoint containing all the data from past 5 minutes.</p> </li> </ul> <p>Timeseries are very useful for passengers getting in and out (from our example). As we need to count two directions (in/out) for three doors (front/middle/back), we create 6 series (e.g. <code>front_in</code>, <code>front_out</code>, ..., <code>back_out</code>). Counter data-points are received in 10 minute interval, so regular timeseries are best fit for this use-case. Every 10 minutes we receive values for all 6 series and store them. Current value is not important as these data are only useful for passenger flow analysis throught whole month/year/...</p> <p>Detailed information about timeseries attribute configuration can be found in DB configuration.</p>"},{"location":"data_model/#relationships","title":"Relationships","text":"<p>Relationships between entities can be represented with or without history. They are realized using the link attribute type. Depedning on whether the history is important, they can be configured using as the mentioned plain data or observations.</p> <p>Relationships can contain additional data, if that fits the modelling needs of your use case. </p> <p>Very useful for:</p> <ul> <li>any kind of relationship between entities</li> <li>linkning dynamic entities to entities with static data</li> </ul> <p>Relationships can also be mirrored, meaning that the having the relationship from entity <code>A</code> to entity <code>B</code> will automatically create the relationship from entity <code>B</code> to entity <code>A</code> in snapshots. This is useful if you need to track a relationship in both directions, but managing both directions is not reasonable.</p> <p>Did you know?</p> <p>Relationships can be placed into arrays and sets, just like any other attribute type. This allows you to model many-to-many relationships without the need for using a multi-value attribute type.</p> <p>As our example so far contains only one entity type, we currently have no need for relationships. However, if we wanted to track the different bus drivers driving individual buses,  relationships would come in quite handy.  The bus driver is a separate entity type, and can drive multiple buses during the day. The current bus driver will be represented as an observation link between the bus and the driver, as can be seen in the resulting configuration.</p>"},{"location":"data_model/#continue-to","title":"Continue to ...","text":"<p>Now that you have an understanding of the data model and the types of attributes, you might want to check out the details of DB configuration, where you will find the parameters for each attribute type and the data types supported by the platform.</p>"},{"location":"extending/","title":"Extending Documentation","text":"<p>This page provides the basic info on where to start with writing documentation. If you feel lost at any point, please check out the documentation of MkDocs and Material for MkDocs, with which this documentation is built.</p>"},{"location":"extending/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml            # The configuration file.\ndocs/\n    index.md          # The documentation homepage.\n    gen_ref_pages.py  # Script for generating the code reference.\n    ...               # Other markdown pages, images and other files.\n</code></pre> <p>The <code>docs/</code> folder contains all source Markdown files for the documentation.</p> <p>You can find all documentation settings in <code>mkdocs.yml</code>. See the <code>nav</code> section for mapping of the left navigation tab and the Markdown files.</p>"},{"location":"extending/#local-instance-commands","title":"Local instance &amp; commands","text":"<p>To see the changes made to the documentation page locally, a local instance of <code>mkdocs</code> is required. You can install all the required packages using:</p> <pre><code>pip install -r requirements.doc.txt\n</code></pre> <p>After installing, you can use the following <code>mkdocs</code> commands:</p> <ul> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"extending/#text-formatting-and-other-features","title":"Text formatting and other features","text":"<p>As the entire documentation is written in Markdown, all base Markdown syntax is supported. This means headings, bold text, italics, <code>inline code</code>, tables and many more.</p> <p>This set of options can be further extended, if you ever find the need. See the possibilities in the Material theme reference.</p> Some of the enabled extensions <ul> <li>This is an example of a collapsable admonition with a custom title.</li> <li>Admonitions are one of the enabled markdown extensions, an another example would be the TODO checklist syntax:<ul> <li> Unchecked item</li> <li> Checked item</li> </ul> </li> <li>See the <code>markdown_extensions</code> section in <code>mkdocs.yml</code> for all enabled extensions.</li> </ul>"},{"location":"extending/#links-and-references","title":"Links and references","text":"<p>To reference an anchor within a page, such as a heading, use a Markdown link to the specific anchor, for example: Commands. If you're not sure which identifier to use, you can look at a heading's anchor by clicking the heading in your Web browser, either in the text itself, or in the table of contents. If the URL is <code>https://example.com/some/page/#anchor-name</code> then you know that this item is possible to link to with <code>[&lt;displayed text&gt;](#anchor-name)</code>. (Tip taken from mkdocstrings)</p> <p>To make a reference to another page within the documentation, use the path to the Markdown source file, followed by the desired anchor. For example, this link was created as <code>[link](index.md#repository-structure)</code>.</p> <p>When making references to the generated Code Reference, there are two options. Links can be made either using the standard Markdown syntax, where some reverse-engineering of the generated files is required, or, with the support of mkdocstrings, using the <code>[example][full.path.to.object]</code> syntax. A real link like this can be for example this one to the Platform Model Specification.</p>"},{"location":"extending/#code-reference-generation","title":"Code reference generation","text":"<p>Code reference is generated using mkdocstrings and the Automatic code reference pages recipe from their documentation. The generation of pages is done using the <code>docs/gen_ref_pages.py</code> script. The script is a slight modification of what is recommended within the mentioned recipe.</p> <p>Mkdocstrings itself enables generating code documentation from its docstrings using a <code>path.to.object</code> syntax. Here is an example of documentation for <code>dp3.snapshots.snapshot_hooks.SnapshotTimeseriesHookContainer.register</code> method:</p> <p>There are additional options that can be specified, which affect the way the documentation is presented. For more on these options, see here.</p> <p>Even if you create a duplicate code reference description, the mkdocstring-style link still leads to the code reference, as you can see here.</p>"},{"location":"extending/#dp3.snapshots.snapshot_hooks.SnapshotTimeseriesHookContainer.register","title":"register","text":"<pre><code>register(hook: Callable[[str, str, list[dict]], list[DataPointTask]], entity_type: str, attr_type: str)\n</code></pre> <p>Registers passed timeseries hook to be called during snapshot creation.</p> <p>Binds hook to specified entity_type and attr_type (though same hook can be bound multiple times). If entity_type and attr_type do not specify a valid timeseries attribute, a ValueError is raised. Args:     hook: <code>hook</code> callable should expect entity_type, attr_type and attribute         history as arguments and return a list of <code>Task</code> objects.     entity_type: specifies entity type     attr_type: specifies attribute type</p>"},{"location":"extending/#deployment","title":"Deployment","text":"<p>The documentation is updated and deployed automatically with each push to selected branches thanks to the configured GitHub Action, which can be found in: <code>.github/workflows/deploy.yml</code>.</p>"},{"location":"grafana_plugin/","title":"Grafana plugin","text":"<p>To simplify access and visualization of DP\u00b3 instance data, we've created DP\u00b3 Grafana plugin.</p> <p>The plugin includes DP\u00b3 data source support, panel for displaying history of multi value attributes, as well as a dashboard generator.</p> <p>This approach allows you to combine multiple data sources (including non-DP\u00b3) and visualize them together in single unified interface.</p>"},{"location":"grafana_plugin/#installation-overview","title":"Installation overview","text":"<p>DP\u00b3 Grafana plugin has been reviewed by Grafana Labs and is available signed from standard plugin catalog.</p> <p>You can install it from GUI or use CLI interface:</p> <pre><code>grafana-cli plugins install cesnet-dp3-app\n</code></pre>"},{"location":"grafana_plugin/#details","title":"Details","text":"<p>For more details, please follow README in Grafana plugin catalog: https://grafana.com/grafana/plugins/cesnet-dp3-app/</p> <p>Source code is available at: https://github.com/CESNET/dp3-grafana/</p>"},{"location":"history_management/","title":"History management","text":"<p>Here we explain several concepts that are used to manage data of attributes with history, meaning they are observations. The configuration is described in the History Manager configuration.</p> <p>DP\u00b3 bases its work with these attributes around the idea of the current value of the attribute at a given time. This is how all values are accessible in the correlation hook API - DP\u00b3 creates a slice for a particular time from the history of each entity, and presents only these current values of all attributes. This is trivial with plain attributes, as they have only one value, but with observations, it's a bit more complicated.</p> <p>There is a lot to cover - the different kinds of validity intervals, attributes having confidence, attributes with overlapping datapoints.  Also, adjacent datapoints with the same value are merged together, values are interpolated and extrapolated from nearby data points, etc. This page describes all this time-related stuff.</p>"},{"location":"history_management/#data-point-validity","title":"Data point validity","text":"<p>Let's start with the basics. We will use an example configuration of an attribute to explain the concepts. Let's use the <code>speed</code> attribute from the example configuration:</p> <pre><code>speed:\n  name: Speed\n  description: Speed of the bus in a particular time. In km/h.\n  type: observations\n  data_type: float\n</code></pre> <p>Each data point has a validity interval, which is a time interval in which the value of the attribute is considered to be valid. This interval is specified in the datapoint itself, by the start and end timestamps, <code>t1</code> and <code>t2</code>, respectively. These timestamps should be set to the interval when this value was actually observed, and so  the value is considered to be valid in the interval <code>[t1, t2]</code>, and invalid outside of it.</p> <p>This basic interval is extended via the <code>pre_validity</code> and <code>post_validity</code> parameters, specified in the attribute definition. You can read more about them in the database entities section of configuration docs. Our example attribute has these set to the default <code>0s</code>, so let's set them to something else to better showcase this:</p> <pre><code>speed:\n  name: Speed\n  description: Speed of the bus in a particular time. In km/h.\n  type: observations\n  data_type: float\n  history_params: # (1)!\n    pre_validity: 5m\n    post_validity: 10m\n</code></pre> <ol> <li>Here we added the <code>history_params</code> section to the attribute definition, and set the <code>pre_validity</code> and <code>post_validity</code> parameters to <code>5m</code> and <code>10m</code>, respectively.</li> </ol> <p>With this configuration, the validity interval of each data point is extended by 5 minutes before and 10 minutes after the interval specified in the data point itself. In the generic case, the validity interval of a data point with timestamps <code>t1</code> and <code>t2</code> is <code>[t1 - pre_validity, t2 + post_validity]</code>.</p> <p>This can be visualized like this:</p> <p> </p> Validity intervals for a single datapoint."},{"location":"history_management/#confidence","title":"Confidence","text":"<p>Let's now add the <code>confidence</code> parameter to the attribute definition.  This parameter specifies whether DP\u00b3 should track a confidence value associated with each data point. The confidence value is a number between 0 and 1, where 0 means the value is completely unreliable (invalid even), and 1 means it's completely reliable. Now the configuration looks like this:</p> <pre><code>speed:\n  name: Speed\n  description: Speed of the bus in a particular time. In km/h.\n  type: observations\n  data_type: float\n  confidence: true # (1)!\n  history_params:\n    pre_validity: 5m\n    post_validity: 10m\n</code></pre> <ol> <li>Here we added the <code>confidence: true</code>, meaning we will send a confidence value with each data point. Let's say the measurement is done using GPS, and we have a confidence value based on the signal strength.</li> </ol> <p>As mentioned, all datapoints should now include a <code>c</code> confidence value, which is a number between 0 and 1. In the interval between <code>t1</code> and <code>t2</code> of that datapoint, the confidence value is kept constant. In the <code>pre</code> / <code>post_validity</code> intervals, the confidence value is linearly interpolated between 0 and the value of <code>c</code> in the strict validity interval <code>[t1, t2]</code>. We can see this in action in the following figure:</p> <p> </p> Confidence interpolation for a single datapoint."},{"location":"history_management/#overlapping-data-points","title":"Overlapping data points","text":"<p>What happens when we have two data points with overlapping validity intervals? Let's see the following figure:</p> <p> </p> Finding the current value with overlapping datapoint validities. <p>We have two data points, one with value <code>v1</code> and the other with value <code>v2</code>. Now, the strict intervals <code>[t1, t2]</code> do not overlap, but the extended intervals <code>[t1 - pre_validity, t2 + post_validity]</code> do. As you can see in the figure, the current value is considered to be the one, where the interpolated confidence value is higher.</p> <p>If the attribute had no confidence values as before, the result would be the same, but the base \"fake\" confidence value used for the calculation would be always 1.</p>"},{"location":"history_management/#multi-value-data-points","title":"Multi-value data points","text":"<p>Let's have a different example for this one. Let's say we will give each passenger an ID, and want to track what passengers were on the bus at a given time.</p> <pre><code>passengers:\n  name: Passengers\n  description: IDs of passengers on the bus at a given time.\n  type: observations\n  data_type: int\n  multi_value: true # (1)!\n  history_params:\n    pre_validity: 5m\n    post_validity: 5m\n</code></pre> <ol> <li>Here we added the <code>multi_value: true</code> parameter, meaning multiple values can be valid at the same time.</li> </ol> <p>The <code>multi_value</code> parameter is used to specify whether multiple values can be valid at the same time. If <code>multi_value</code> is set to <code>true</code>, DP\u00b3 will adapt to this and handle the data points accordingly. Most importantly, despite the fact that we send single datapoints with a value of type <code>int</code>, the current value will be a list of integers, and the current confidence will be a list of floats, when enabled.</p> <p>Let's see an example:</p> <p> </p> Multi-value attribute behavior. <p>Here we can se that we get a list of integers with the current passenger IDs, where each value's history is tracked separately.</p> <p>In case you had this exact data, but did not set the <code>multi_value: true</code> parameter, the behavior would be getting the current value as a single integer, and the current confidence (when set) as a single float. The current value would be the one with the highest confidence value. In case of multiple overlapping data points with equal confidence, one will be chosen by the current implementation based on the current ordering, but no exact criteria is defined.</p> <p>Generally speaking, if <code>multi_value: false</code> is set, your input modules are responsible for not sending overlapping datapoints, otherwise the exact behavior is undefined.</p> <p>Arrays vs Multi-value</p> <p>Are you unsure how to model your vector of data into a DP\u00b3 attribute? Check out Arrays vs Multi-value attributes.</p>"},{"location":"history_management/#data-point-merging","title":"Data point merging","text":"<p>To save disk space, DP\u00b3 merges consecutive data points with the same value and overlapping validity intervals. This is also referred to as data point aggregation. The threshold for what is considered consecutive for two data points, <code>A</code> and <code>B</code>, is defined as <code>A.t2 + post_validity &gt;= B.t1 - pre_validity</code>.  In other words, as long as there is any overlap between the validity intervals of two data points, they are considered consecutive.</p> <p> </p> Two consecutive datapoints of the same value are merged. <p>This aggregation is performed periodically, so even if the data points arrive out-of-order, they will be merged correctly.</p> <p>If the values have a confidence set, the resulting confidence will be the average of the two values, As you can see in the next figure:</p> <p> </p> The confidence of merged datapoints will be averaged. <p>With multi-value attributes, this merging happens in the same way, but for each value separately. This means that if you have two data points with overlapping validity intervals, but different values, they will not be merged, but will be kept as separate data points. Data points with the same value will be merged, as before. This is illustrated in the following figure:</p> <p> </p> Each value of a multi-value attribute will be merged separately."},{"location":"history_management/#other-resources","title":"Other resources","text":"<p>We hope this cleared up the history aspect of data point handling in DP\u00b3. What's next?</p> <p>The configuration of observations attributes are described in the  DB entities page, where you can find the additional history parameters configuration description.</p> <p>How often is datapoint aggregation performed is set in the <code>history_manager</code> configuration,  which is described here.</p> <p>You can read more about data flow in DP\u00b3 and where you can get the current value on the  Architecture page.</p>"},{"location":"install/","title":"Installing DP\u00b3 platform","text":"<p>When talking about installing the DP\u00b3 platform, a distinction must be made between installing for platform development, installing for application development (i.e. platform usage)  and installing for application deployment using supervisor.  We will cover all three cases separately.</p>"},{"location":"install/#application-development","title":"Application development","text":"<p>Pre-requisites: Python 3.9 or higher, <code>pip</code> (with <code>virtualenv</code> installed), <code>git</code>, <code>Docker</code> and <code>Docker Compose</code>.</p> <p>Create a virtualenv and install the DP\u00b3 platform using:</p> <pre><code>python3 -m venv venv  # (1)!\nsource venv/bin/activate  # (2)!\npip install --upgrade pip  # (3)!\npip install git+https://github.com/CESNET/dp3.git#egg=dp-cubed\n</code></pre> <ol> <li>We recommend using virtual environment. If you are not familiar with it, please read     this first.    Note for Windows: If <code>python3</code> does not work, try <code>py -3</code> or <code>python</code> instead.</li> <li>Windows: <code>venv/Scripts/activate.bat</code></li> <li>We require <code>pip&gt;=21.0.1</code> for the <code>pyproject.toml</code> support.    If your pip is up-to-date, you can skip this step.</li> </ol>"},{"location":"install/#creating-a-dp3-application","title":"Creating a DP\u00b3 application","text":"<p>DP\u00b3 comes with a <code>dp3</code> utility, which is used to create a new DP\u00b3 application and run it. To create a new DP\u00b3 application we use the <code>setup</code> command. Run:</p> <pre><code>dp3 setup &lt;application_directory&gt; &lt;your_application_name&gt; \n</code></pre> <p>So for example, to create an application called <code>my_app</code> in the current directory, run:</p> <pre><code>dp3 setup . my_app\n</code></pre> <p>This produces the following directory structure: <pre><code> \ud83d\udcc2 .\n \u251c\u2500\u2500 \ud83d\udcc1 config  # (1)! \n \u2502   \u251c\u2500\u2500 \ud83d\udcc4 api.yml\n \u2502   \u251c\u2500\u2500 \ud83d\udcc4 control.yml\n \u2502   \u251c\u2500\u2500 \ud83d\udcc4 database.yml\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 db_entities # (2)!\n \u2502   \u251c\u2500\u2500 \ud83d\udcc4 event_logging.yml\n \u2502   \u251c\u2500\u2500 \ud83d\udcc4 history_manager.yml\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 modules # (3)!\n \u2502   \u251c\u2500\u2500 \ud83d\udcc4 processing_core.yml\n \u2502   \u2514\u2500\u2500 \ud83d\udcc4 snapshots.yml\n \u251c\u2500\u2500 \ud83d\udcc1 docker # (4)!\n \u2502   \u251c\u2500\u2500 \ud83d\udcc1 python\n \u2502   \u2514\u2500\u2500 \ud83d\udcc1 rabbitmq\n \u251c\u2500\u2500 \ud83d\udcc4 docker-compose.app.yml\n \u251c\u2500\u2500 \ud83d\udcc4 docker-compose.yml\n \u251c\u2500\u2500 \ud83d\udcc1 modules # (5)!\n \u2502   \u2514\u2500\u2500 \ud83d\udcc4 test_module.py\n \u251c\u2500\u2500 \ud83d\udcc4 README.md # (6)!\n \u2514\u2500\u2500 \ud83d\udcc4 requirements.txt\n</code></pre></p> <ol> <li>The <code>config</code> directory contains the configuration files for the DP\u00b3 platform. For more details,    please check out the configuration documentation.</li> <li>The <code>config/db_entities</code> directory contains the database entities of the application.    This defines the data model of your application.     For more details, you may want to check out the data model and the    DB entities documentation.</li> <li>The <code>config/modules</code> directory is where you can place the configuration specific to your modules.</li> <li>The <code>docker</code> directory contains the Dockerfiles for the RabbitMQ and python images,     tailored to your application. </li> <li>The <code>modules</code> directory contains the modules of your application. To get started,    a single module called <code>test_module</code> is included.     For more details, please check out the Modules page.</li> <li>The <code>README.md</code> file contains some instructions to get started.     Edit it to your liking.</li> </ol>"},{"location":"install/#running-the-application","title":"Running the Application","text":"<p>To run the application, we first need to set up the other services the platform depends on, such as the MongoDB database, the RabbitMQ message distribution and the Redis database. This can be done using the supplied <code>docker-compose.yml</code> file. Simply run:</p> <pre><code>docker compose up -d --build  # (1)!\n</code></pre> <ol> <li>The <code>-d</code> flag runs the services in the background, so you can continue working in the same terminal.    The <code>--build</code> flag forces Docker to rebuild the images, so you can be sure you are running the latest version.    If you want to run the services in the foreground, omit the <code>-d</code> flag.</li> </ol> Docker Compose basics <p>The state of running containers can be checked using:</p> <pre><code>docker compose ps\n</code></pre> <p>which will display the state of running processes. The logs of the services can be displayed using:</p> <pre><code>docker compose logs\n</code></pre> <p>which will display the logs of all services, or:</p> <pre><code>docker compose logs &lt;service name&gt;\n</code></pre> <p>which will display only the logs of the given service.    (In this case, the services are rabbitmq, mongo, mongo_express, and redis)</p> <p>We can now focus on running the platform and developing or testing. After you are done, simply run:</p> <pre><code>docker compose down\n</code></pre> <p>which will stop and remove all containers, networks and volumes created by <code>docker compose up</code>.</p> <p>There are two main ways to run the application itself. First is a little more hand-on,  and allows easier debugging.  There are two main kinds of processes in the application: the API and the worker processes.</p> <p>To run the API, simply run:</p> <pre><code>APP_NAME=my_app CONF_DIR=config dp3 api\n</code></pre> <p>The starting configuration sets only a single worker process, which you can run using:</p> <pre><code>dp3 worker my_app config 0     \n</code></pre> <p>The second way is to use the <code>docker-compose.app.yml</code> file, which runs the API and the worker processes in separate containers. To run the API, simply run:</p> <pre><code>docker compose -f docker-compose.app.yml up -d --build\n</code></pre> <p>Either way, to test that everything is running properly, you can run: <pre><code>curl -X 'GET' 'http://localhost:5000/' \\\n     -H 'Accept: application/json' \n</code></pre></p> <p>Which should return a JSON response with the following content: <pre><code>{\n   \"detail\": \"It works!\"\n}\n</code></pre></p> <p>Final note, to simplify the experience of adjusting the app configuration,  especially that of the DB entities, we provide the <code>dp3 check</code> command. The command simply loads the configuration and checks that it is valid, but if not, it tries really hard to pinpoint where exactly you went wrong. This can be used as follows:</p> <pre><code>dp3 check &lt;config_directory&gt;\n</code></pre> <p>You are now ready to start developing your application!</p>"},{"location":"install/#application-deployment","title":"Application deployment","text":"<p>The application development installation above is not suitable for production use. For production use, we recommend using the <code>supervisor</code> process manager, which greatly simplifies having multiple worker processes. We recommend <code>gunicorn</code> as the API server,  hidden behind <code>nginx</code> as a reverse proxy.</p> <p>We will assume that you have a prototype of your application available on the server you will be deploying on,  if not, you can use <code>dp3 setup</code> to create the application skeleton after installing the DP3 package, as described in the previous section.</p> <p>To start, install the pre-requisites and explicitly dependent packages:</p> <pre><code>sudo dnf install git wget nginx supervisor redis\n</code></pre> <p>Application placement</p> <p>We highly recommend using a virtualenv for you python environment   which can be created using the command:</p> <pre><code>python3 -m venv &lt;path&gt;\n</code></pre> <p>We recommend placing your application, as well as the virtualenv,   outside you home directory, for example in <code>/opt/&lt;APP_NAME&gt;</code>.   This will greatly simplify the permissions management.</p> <p>Inside your virtualenv, install DP3 with the <code>deploy</code> extras, which includes the <code>gunicorn</code> server:</p> <pre><code>pip install \"git+https://github.com/CESNET/dp3.git#egg=dp-cubed[deploy]\"\n</code></pre> <p>We will assume that you have the python environment activated for the rest of the installation.</p> <p>We want to run your app under a special user, where the username should be the same as the name of your app.  Create the user and group:</p> <pre><code>sudo useradd &lt;APP_NAME&gt;\nsudo groupadd &lt;APP_NAME&gt;\n</code></pre>"},{"location":"install/#installing-dependencies","title":"Installing dependencies","text":"<p>We must first cover the dependencies which were set up in the docker-compose file in the development installation.</p>"},{"location":"install/#redis","title":"Redis","text":"<p>You have already installed redis as a package, just enable and start the service with default configuration:</p> <pre><code>sudo systemctl start redis\nsudo systemctl enable redis\n</code></pre>"},{"location":"install/#mongodb","title":"MongoDB","text":"<p>If you already have an existing MongoDB instance with credentials matching your configuration, you can skip this step.</p> <p>To install, please follow the official guide for your platform from MongoDB's webpage. This is a quick rundown of the installation instructions for an RPM-based Linux (Oracle Linux 9).</p> <p>First, add the MongoDB repository:</p> <pre><code>cat &gt; /etc/yum.repos.d/mongodb-org-6.0.repo &lt;&lt;EOF\n[mongodb-org-6.0]\nname=MongoDB Repository\nbaseurl=https://repo.mongodb.org/yum/redhat/\\$releasever/mongodb-org/6.0/x86_64/\ngpgcheck=1\nenabled=1\ngpgkey=https://www.mongodb.org/static/pgp/server-6.0.asc\nEOF\n</code></pre> <p>Then install the MongoDB packages:</p> <pre><code>sudo dnf -y install mongodb-org mongodb-mongosh\n</code></pre> <p>Start and enable the service:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl start mongod\nsudo systemctl enable mongod\n</code></pre> <p>Your DP3 app will need a user with the correct permissions to access the database, so we will create a new user for this purpose using <code>mongosh</code>:</p> <pre><code>use admin\ndb.createUser(\n {\n  user: \"&lt;USER&gt;\",\n  pwd: \"&lt;PASSWORD&gt;\",\n  roles:[\"readWrite\", \"dbAdmin\"]\n }\n);\n</code></pre> <p>Now the API should start OK.</p>"},{"location":"install/#rabbitmq","title":"RabbitMQ","text":"<p>This is the most painful part of the installation, so do not get discouraged, it gets only easier from here. For the most up-to date instructions, pick and follow an installation guide for your platform from RabbitMQ's webpage.  In this section we will just briefly go through the installation process on an RPM-based Linux (Oracle Linux 9).</p> <p>As we will be adding RabbitMQ's and Erlang repositories, which have individual signing keys for their packages, we first need to add these keys:</p> <pre><code>sudo rpm --import 'https://github.com/rabbitmq/signing-keys/releases/download/2.0/rabbitmq-release-signing-key.asc'\nsudo rpm --import 'https://dl.cloudsmith.io/public/rabbitmq/rabbitmq-erlang/gpg.E495BB49CC4BBE5B.key'\nsudo rpm --import 'https://dl.cloudsmith.io/public/rabbitmq/rabbitmq-server/gpg.9F4587F226208342.key'\n</code></pre> <p>Then add the repositories themselves, into a new file: <code>/etc/yum.repos.d/rabbitmq.repo</code>.  RabbitMQ provides a listing of these depending on the exact distribution version,  so just look for \"Add Yum Repositories for RabbitMQ and Modern Erlang\" on the guide page and copy the relevant contents.</p> <p>Finally, we can install the package itself:</p> <pre><code>sudo dnf update -y\nsudo dnf install socat logrotate -y\nsudo dnf install -y erlang-25.3.2.3 rabbitmq-server\n</code></pre> <p>We want RabbitMQ to run on localhost, so add a file <code>/etc/rabbitmq/rabbitmq-env.conf</code> and paste into it the following:</p> <pre><code>HOSTNAME=localhost\n</code></pre> <p>Enable and start the service:</p> <pre><code>sudo systemctl enable rabbitmq-server\nsudo systemctl start rabbitmq-server\n</code></pre> <p>Enable the web management interface:</p> <pre><code>sudo rabbitmq-plugins enable rabbitmq_management\n</code></pre> <p>Almost ready. RabbitMQ uses an HTTP API for its configuration, but it provides a <code>rabbitmqadmin</code>  CLI interface that abstracts us from the details of this API. We need to get this, as our configuration scripts depend on it. With the RabbitMQ server running, we can download the CLI script and place it on the <code>PATH</code>:</p> <pre><code>wget 'http://localhost:15672/cli/rabbitmqadmin'\nsudo chmod +x rabbitmqadmin\nsudo mv rabbitmqadmin /usr/bin/\n</code></pre> <p>Finally, we have to configure the appropriate queues and exchanges,  which is done using a provided <code>rmq_reconfigure.sh</code> script, which can be run using the <code>dp3-script</code> entrypoint:</p> <pre><code>sudo venv/bin/dp3-script rmq_reconfigure.sh &lt;APP_NAME&gt; &lt;NUM_WORKERS&gt;\n</code></pre>"},{"location":"install/#nginx","title":"Nginx","text":"<p>Having the app dependencies installed, and running, we can now set up the webserver. We have already installed <code>nginx</code> in the beginning of this guide,  so all that is left to do is to configure the server.  DP3 provides a basic configuration that assumes only DP3 is running on the webserver, so if that is not your case, please adjust the configuration to your liking.</p> <p>To get the configuration, run: <pre><code>sudo $(which dp3) config nginx \\\n  --hostname &lt;SERVER_HOSTNAME&gt; \\ # (1)! \n  --app-name &lt;APP_NAME&gt; \\ \n  --www-root &lt;DIRECTORY&gt; # (2)! \n</code></pre></p> <ol> <li>e.g. dp3.example.com</li> <li>Where to place HTML, e.g. /var/www/dp3</li> </ol> <p>This will set up a simple landing page for the server, and proxies for the API,  its docs and RabbitMQ management interface. With this ready, you can enable and start <code>nginx</code>:</p> <pre><code>sudo systemctl enable nginx\nsudo systemctl start nginx\n</code></pre> <p>In order to reach your server, you will also have to open the firewall:</p> <pre><code># Get firewalld running\nsudo systemctl unmask firewalld\nsudo systemctl enable firewalld\nsudo systemctl start firewalld\n# Open http/tcp\nsudo firewall-cmd --add-port 80/tcp --permanent\nsudo firewall-cmd --reload\n</code></pre> <p>Now you should be able to go to your server and see the landing page.  The RabbitMQ management interface should be up and running with the  default credentials <code>guest/guest</code>. You will notice however, that the API is currently not running, so let's fix that.</p>"},{"location":"install/#setting-up-supervisor-control-of-all-dp3-processes","title":"Setting up Supervisor control of all DP3 processes","text":"<p>We will set up a supervisor configuration for your DP3 app in <code>/etc/APPNAME</code>.  For the base configuration, run:</p> <pre><code>sudo $(which dp3) config supervisor --config &lt;CONFIG_DIR&gt; --app-name &lt;APP_NAME&gt;\n</code></pre> <p>Enable the service:</p> <pre><code>sudo systemctl enable &lt;APP_NAME&gt;\nsudo systemctl start &lt;APP_NAME&gt;\n</code></pre> <p>Now a new executable should be on your path, <code>&lt;APPNAME&gt;ctl</code>, which you can use to control the app. It is a shortcut for <code>supervisorctl -c /etc/APP_NAME/supervisord.conf</code>,  so you can use it to start the app:</p> <pre><code>&lt;APPNAME&gt;ctl start all\n</code></pre> <p>You can also use it to check the status of the app:</p> <pre><code>&lt;APPNAME&gt;ctl status\n</code></pre> <p>For more information on <code>supervisorctl</code>, see its documentation.</p> <p>You can view the generated configuration in <code>/etc/&lt;APP_NAME&gt;</code> and the full logs of the app's processes in <code>/var/log/&lt;APP_NAME&gt;</code>.</p>"},{"location":"install/#platform-development","title":"Platform development","text":"<p>Pre-requisites: Python 3.9 or higher, <code>pip</code> (with <code>virtualenv</code> installed), <code>git</code>, <code>Docker</code> and <code>Docker Compose</code>.</p> <p>Pull the repository and install using:</p> <pre><code>git clone git@github.com:CESNET/dp3.git dp3 \ncd dp3\npython3 -m venv venv  # (1)!\nsource venv/bin/activate  # (2)!\npython -m pip install --upgrade pip  # (3)!\npip install --editable \".[dev]\" # (4)!\npre-commit install  # (5)!\n</code></pre> <ol> <li>We recommend using virtual environment. If you are not familiar with it, please read     this first.    Note for Windows: If <code>python3</code> does not work, try <code>py -3</code> or <code>python</code> instead.</li> <li>Windows: <code>venv/Scripts/activate.bat</code></li> <li>We require <code>pip&gt;=21.0.1</code> for the <code>pyproject.toml</code> support.    If your pip is up-to-date, you can skip this step.</li> <li>Install using editable mode to allow for changes in the code to be reflected in the installed package.    Also, install the development dependencies, such as <code>pre-commit</code> and <code>mkdocs</code>.</li> <li>Install <code>pre-commit</code> hooks to automatically format and lint the code before committing.</li> </ol> <p>With the dependencies, the pre-commit package is installed. You can verify the installation using <code>pre-commit --version</code>. Pre-commit is used to automatically unify code formatting and perform code linting. The hooks configured in <code>.pre-commit-config.yaml</code> should now run automatically on every commit.</p> <p>In case you want to make sure, you can run <code>pre-commit run --all-files</code> to see it in action.</p>"},{"location":"install/#running-the-dependencies-and-the-platform","title":"Running the dependencies and the platform","text":"<p>The DP\u00b3 platform is now installed and ready for development. To run it, we first need to set up the other services the platform depends on, such as the MongoDB database, the RabbitMQ message distribution and the Redis database. This can be done using the supplied <code>docker-compose.yml</code> file. Simply run:</p> <pre><code>docker compose up -d --build  # (1)!\n</code></pre> <ol> <li>The <code>-d</code> flag runs the services in the background, so you can continue working in the same terminal.    The <code>--build</code> flag forces Docker to rebuild the images, so you can be sure you are running the latest version.    If you want to run the services in the foreground, omit the <code>-d</code> flag.</li> </ol> On Docker Compose <p>Docker Compose can be installed as a standalone (older v1) or as a plugin (v2),  the only difference is when executing the command:</p> <p>Note that Compose standalone uses the dash compose syntax instead of current\u2019s standard syntax (space compose). For example: type <code>docker-compose up</code> when using Compose standalone, instead of <code>docker compose up</code>.</p> <p>This documentation uses the v2 syntax, so if you have the standalone version installed, adjust accordingly.</p> <p>After the first <code>compose up</code> command, the images for RabbitMQ, MongoDB and Redis will be downloaded, their images will be built according to the configuration and all three services will be started. On subsequent runs, Docker will use the cache, so if the configuration does not change, the download and build steps will not be repeated.</p> <p>The configuration is taken implicitly from the <code>docker-compose.yml</code> file in the current directory. The <code>docker-compose.yml</code> configuration contains the configuration for the services, as well as a testing setup of the DP\u00b3 platform itself.  The full configuration is in <code>tests/test_config</code>. The setup includes one worker process and one API process to handle requests.  The API process is exposed on port 5000, so you can send requests to it using <code>curl</code> or from your browser:</p> <p><pre><code>curl -X 'GET' 'http://localhost:5000/' \\\n     -H 'Accept: application/json' \n</code></pre> <pre><code>curl -X 'POST' 'http://localhost:5000/datapoints' \\\n     -H 'Content-Type: application/json' \\\n     --data '[{\"type\": \"test_entity_type\", \"id\": \"abc\", \"attr\": \"test_attr_int\", \"v\": 123, \"t1\": \"2023-07-01T12:00:00\", \"t2\": \"2023-07-01T13:00:00\"}]'\n</code></pre></p> Docker Compose basics <p>The state of running containers can be checked using:</p> <pre><code>docker compose ps\n</code></pre> <p>which will display the state of running processes. The logs of the services can be displayed using:</p> <pre><code>docker compose logs\n</code></pre> <p>which will display the logs of all services, or:</p> <pre><code>docker compose logs &lt;service name&gt;\n</code></pre> <p>which will display only the logs of the given service.    (In this case, the services are rabbitmq, mongo, redis, receiver_api and worker)</p> <p>We can now focus on running the platform and developing or testing. After you are done, simply run:</p> <pre><code>docker compose down\n</code></pre> <p>which will stop and remove all containers, networks and volumes created by <code>docker compose up</code>.</p>"},{"location":"install/#testing","title":"Testing","text":"<p>With the testing platform setup running, we can now run tests.  Tests are run using the <code>unittest</code> framework and can be run using:</p> <pre><code>python -m unittest discover \\\n       -s tests/test_common \\\n       -v\nCONF_DIR=tests/test_config \\\npython -m unittest discover \\\n       -s tests/test_api \\\n       -v\n</code></pre>"},{"location":"install/#documentation","title":"Documentation","text":"<p>For extending of this documentation, please refer to the Extending page.</p>"},{"location":"modules/","title":"Modules","text":"<p>DP\u00b3 enables its users to create custom modules to perform application specific data analysis. Modules are loaded using a plugin-like architecture and can influence the data flow from the very first moment upon handling the data-point push request.</p> <p>As described in the Architecture page, DP\u00b3 uses a categorization of modules into primary and secondary modules.  The distinction between primary and secondary modules is such that primary modules send data-points into the system using the HTTP API, while secondary modules react to the data present in the system, e.g.: altering the data-flow in an application-specific manner, deriving additional data based on incoming data-points or performing data correlation on entity snapshots.</p> <p>This page covers the DP\u00b3 API for secondary modules,  for primary module implementation, the API documentation may be useful,  also feel free to check out the dummy_sender script in <code>/scripts/dummy_sender.py</code>.</p>"},{"location":"modules/#creating-a-new-module","title":"Creating a new Module","text":"<p>First, make a directory that will contain all modules of the application. For example, let's assume that the directory will be called <code>/modules/</code>.</p> <p>As mentioned in the Processing core configuration page, the modules directory must be specified in the <code>modules_dir</code> configuration option. Let's create the main module file now - assuming the module will be called <code>my_awesome_module</code>, create a file <code>/modules/my_awesome_module.py</code>. </p> <p>Finally, to make the processing core load the module, add the module name to the <code>enabled_modules</code> configuration option, e.g.:</p> Enabling the module in processing_core.yml<pre><code>modules_dir: \"/modules/\"\nenabled_modules:\n  - \"my_awesome_module\"\n</code></pre> <p>Here is a basic skeleton for the module file:</p> <pre><code>from dp3.common.base_module import BaseModule\nfrom dp3.common.callback_registrar import CallbackRegistrar\nfrom dp3.common.config import PlatformConfig\n\nclass MyAwesomeModule(BaseModule):\n    def __init__(self,\n        platform_config: PlatformConfig, \n        module_config: dict, \n        registrar: CallbackRegistrar\n    ):\n        super().__init__(platform_config, module_config, registrar)\n        # (1)!\n\n    def load_config(self, config: PlatformConfig, module_config: dict) -&gt; None:\n        ... # (2)!\n</code></pre> <ol> <li>After calling the <code>BaseModule</code> constructor, the module should and register its callbacks in the <code>__init__</code> method.</li> <li>The <code>load_config</code> method is called by the <code>BaseModule</code>'s <code>__init__</code> method, and is used to load the module-specific configuration. It can be also called during system runtime to reload the configuration.</li> </ol> <p>You should place loading of module's configuration inside the <code>load_config</code> method.  This method is called by the <code>BaseModule</code>'s <code>__init__</code> method, as you can see bellow. It can be also called during system runtime to reload the configuration using the Control API,  which enables you to change the module configuration without restarting the DP\u00b3 worker.</p> <p>The registering of callbacks should be done in the <code>__init__</code> method however,  as there is currently no support to alter a callback once it has been registered without restarting.</p> <p>All modules must subclass the <code>BaseModule</code> class. If a class does not subclass the <code>BaseModule</code> class, it will not be loaded and activated by the main DP\u00b3 worker. The declaration of <code>BaseModule</code> is as follows:</p> <pre><code>class BaseModule:\n    def __init__(\n        self, \n        platform_config: PlatformConfig,\n        module_config: dict,\n        registrar: CallbackRegistrar\n    ):\n        self.refresh: SharedFlag = SharedFlag(\n            False, banner=f\"Refresh {self.__class__.__name__}\"\n        )\n        self.log: logging.Logger = logging.getLogger(self.__class__.__name__)\n\n        self.load_config(platform_config, module_config)\n</code></pre> <p>As you can see, the <code>BaseModule</code> class provides a <code>refresh</code> flag used internally for refreshing and a <code>log</code> object for the module to use for logging. </p> <p>At initialization, each module receives a <code>PlatformConfig</code>, a <code>module_config</code> dictionary and a  <code>CallbackRegistrar</code>. For the module to do anything, it must read the provided configuration from <code>platform_config</code>and <code>module_config</code> and register callbacks to perform data analysis using the <code>registrar</code> object. Let's go through them one at a time.</p>"},{"location":"modules/#configuration","title":"Configuration","text":"<p><code>PlatformConfig</code> contains the entire DP\u00b3 platform configuration, which includes the application name, worker counts, which worker processes is the module running in and a <code>ModelSpec</code> which contains the entity specification.</p> <p>If you want to create configuration specific to the module itself, create a <code>.yml</code> configuration file  named as the module itself inside the <code>modules/</code> folder, as described in the modules configuration page. This configuration will be then loaded into the <code>module_config</code> dictionary for convenience. Please place code that loads the module configuration into the <code>load_config</code> method, where you will recieve both the <code>platform_config</code> and <code>module_config</code> as arguments.</p>"},{"location":"modules/#reloading-configuration","title":"Reloading configuration","text":"<p>The module configuration can be reloaded during runtime using the Control API, specifically the <code>refresh_module_config</code> action. This will cause the <code>load_config</code> method to be called again, with the new configuration. For this reason it is recommended to place all configuration loading code into the <code>load_config</code> method.</p> <p>Some callbacks may be called only sparsely in the lifetime of an entity, and it may be useful to refresh all the values derived by the module when the configuration changes. This is implemented for the <code>on_entity_creation</code>  and <code>on_new_attr</code> callbacks,  and you can enable it by passing the <code>refresh</code> keyword argument to the callback registration. See the Callbacks section for more details.</p>"},{"location":"modules/#type-of-eid","title":"Type of <code>eid</code>","text":"<p>Specifying the <code>eid</code> type</p> <p>At runtime, the <code>eid</code> will be exactly the type as specified in the entity specification.</p> <p>All the examples on this page will show the <code>eid</code> as a string, as that is the default type. The type of the <code>eid</code> is can be configured in the entity specification, as is detailed here.</p> <p>The typehint of the <code>eid</code> used in callback registration definitions is the  <code>AnyEidT</code> type, which is a type alias of Union of all the allowed types of the <code>eid</code> in the entity specification.</p>"},{"location":"modules/#callbacks","title":"Callbacks","text":"<p>The <code>registrar:</code> <code>CallbackRegistrar</code> object provides the API to register callbacks to be called during the data processing.</p>"},{"location":"modules/#cron-trigger-periodic-callbacks","title":"CRON Trigger Periodic Callbacks","text":"<p>For callbacks that need to be called periodically,  the <code>scheduler_register</code> is used.  The specific times the callback will be called are defined using the CRON schedule expressions. Here is a simplified example from the HistoryManager module:</p> <pre><code>registrar.scheduler_register(\n    self.delete_old_dps, minute=\"*/10\"  # (1)!\n)\nregistrar.scheduler_register(\n    self.archive_old_dps, minute=0, hour=2  # (2)!\n)  \n</code></pre> <ol> <li>At every 10th minute.</li> <li>Every day at 2 AM.</li> </ol> <p>By default, the callback will receive no arguments, but you can pass static arguments for every call using the <code>func_args</code> and <code>func_kwargs</code> keyword arguments.  The function return value will always be ignored.</p> <p>The complete documentation can be found at the  <code>scheduler_register</code> page. As DP\u00b3 utilizes the APScheduler package internally to realize this functionality, specifically the <code>CronTrigger</code>, feel free to check their documentation for more details.</p>"},{"location":"modules/#callbacks-within-processing","title":"Callbacks within processing","text":"<p>There are a number of possible places to register callback functions during data-point processing.</p>"},{"location":"modules/#task-on_task_start-hook","title":"Task <code>on_task_start</code> hook","text":"<p>A hook will be called on task processing start. The callback is registered using the  <code>register_task_hook</code> method. Required signature is <code>Callable[[DataPointTask], Any]</code>, as the return value is ignored. It may be useful for implementing custom statistics.</p> <pre><code>def task_hook(task: DataPointTask):\n    print(task.etype)\n\nregistrar.register_task_hook(\"on_task_start\", task_hook)\n</code></pre>"},{"location":"modules/#entity-allow_entity_creation-hook","title":"Entity <code>allow_entity_creation</code> hook","text":"<p>Receives eid and Task, may prevent entity record creation (by returning False). The callback is registered using the  <code>register_allow_entity_creation_hook</code> method. Required signature is <code>Callable[[AnyEidT, DataPointTask], bool]</code>.</p> <pre><code>def entity_creation(\n        eid: str,  # (1)! \n        task: DataPointTask,\n) -&gt; bool:\n    return eid.startswith(\"1\")\n\nregistrar.register_allow_entity_creation_hook(\n    entity_creation, \"test_entity_type\"\n)\n</code></pre> <ol> <li><code>eid</code> may not be string, depending on the entity configuration, see Type of    <code>eid</code>.</li> </ol>"},{"location":"modules/#entity-on_entity_creation-hook","title":"Entity <code>on_entity_creation</code> hook","text":"<p>Receives eid and Task, may return new DataPointTasks.</p> <p>Callbacks which are called once when an entity is created are registered using the  <code>register_on_entity_creation_hook</code> method. Required signature is <code>Callable[[AnyEidT, DataPointTask], list[DataPointTask]]</code>.</p> <pre><code>def processing_function(\n        eid: str,  # (1)! \n        task: DataPointTask\n) -&gt; list[DataPointTask]:\n    output = does_work(task)\n    return [DataPointTask(\n        model_spec=task.model_spec,\n        etype=\"mac\",\n        eid=eid,\n        data_points=[{\n            \"etype\": \"test_enitity_type\",\n            \"eid\": eid,\n            \"attr\": \"derived_on_creation\",\n            \"src\": \"secondary/derived_on_creation\",\n            \"v\": output\n        }]\n    )]\n\nregistrar.register_on_entity_creation_hook(\n    processing_function, \"test_entity_type\"\n)\n</code></pre> <ol> <li><code>eid</code> may not be string, depending on the entity configuration, see Type of    <code>eid</code>.</li> </ol> <p>The <code>register_on_entity_creation_hook</code> method also allows for refreshing of values derived  by the registered hook. This can be done using the <code>refresh</code> keyword argument, (expecting a <code>SharedFlag</code> object, which is created by default for all modules) and the <code>may_change</code> keyword argument, which lists all the attributes that the hook may change. For the above example, the registration would look like this:</p> <pre><code>registrar.register_on_entity_creation_hook(\n    processing_function, \n    \"test_entity_type\", \n    refresh=self.refresh,\n    may_change=[[\"derived_on_creation\"]]\n)\n</code></pre>"},{"location":"modules/#attribute-hooks","title":"Attribute hooks","text":"<p>Callbacks that are called on every incoming datapoint of an attribute are registered using the  <code>register_on_new_attr_hook</code> method. The callback allways receives eid, attribute and Task, and may return new DataPointTasks. The required signature is <code>Callable[[AnyEidT, DataPointBase], Union[None, list[DataPointTask]]]</code>.</p> <pre><code>def attr_hook(\n        eid: str,  # (1)!\n        dp: DataPointBase,\n) -&gt; list[DataPointTask]:\n    ...\n    return []\n\nregistrar.register_on_new_attr_hook(\n    attr_hook, \"test_entity_type\", \"test_attr_type\",\n)\n</code></pre> <ol> <li><code>eid</code> may not be string, depending on the entity configuration, see Type of    <code>eid</code>.</li> </ol> <p>This hook can be refreshed on configuration changes if you feel like the attribute value may change too slowly to catch up naturally.  This can be done using the <code>refresh</code> keyword argument, (expecting a <code>SharedFlag</code> object, which is created by default for all modules) and the <code>may_change</code> keyword argument, which lists all the attributes that the hook may change. For the above example, the registration would look like this:</p> <pre><code>registrar.register_on_new_attr_hook(\n    attr_hook, \n    \"test_entity_type\", \n    \"test_attr_type\", \n    refresh=self.refresh,\n    may_change=[]  # (1)!\n)\n</code></pre> <ol> <li>If the hook may change the value of any attributes, they must be listed here.</li> </ol>"},{"location":"modules/#timeseries-hook","title":"Timeseries hook","text":"<p>Timeseries hooks are run before snapshot creation, and allow to process the accumulated timeseries data into observations / plain attributes to be accessed in snapshots.</p> <p>Callbacks are registered using the  <code>register_timeseries_hook</code> method. The expected callback signature is <code>Callable[[str, str, list[dict]], list[DataPointTask]]</code>, as the callback should expect entity_type, attr_type and attribute history as arguments  and return a list of DataPointTask objects.</p> <pre><code>def timeseries_hook(\n        entity_type: str, attr_type: str, attr_history: list[dict]\n) -&gt; list[DataPointTask]:\n    ...\n    return []\n\nregistrar.register_timeseries_hook(\n    timeseries_hook, \"test_entity_type\", \"test_attr_type\",\n)\n</code></pre>"},{"location":"modules/#correlation-callbacks","title":"Correlation callbacks","text":"<p>Correlation callbacks are called during snapshot creation, and allow to perform analysis on the data of the snapshot.</p>"},{"location":"modules/#snapshots-correlation-hook","title":"Snapshots Correlation Hook","text":"<p>The <code>register_correlation_hook</code> method expects a callable with the following signature:  <code>Callable[[str, dict], Union[None, list[DataPointTask]]]</code>, where the first argument is the entity type, and the second is a dict containing the current values of the entity and its linked entities. The method can optionally return a list of DataPointTask objects to be inserted into the system.</p> <p>As correlation hooks can depend on each other, the hook inputs and outputs must be specified using the depends_on and may_change arguments. Both arguments are lists of lists of strings, where each list of strings is a path from the specified entity type to individual attributes (even on linked entities). For example, if the entity type is <code>test_entity_type</code>, and the hook depends on the attribute <code>test_attr_type1</code>, the path is simply <code>[[\"test_attr_type1\"]]</code>. If the hook depends on the attribute <code>test_attr_type1</code>  of an  entity linked using <code>test_attr_link</code>, the path will be <code>[[\"test_attr_link\", \"test_attr_type1\"]]</code>.</p> <pre><code>def correlation_hook(entity_type: str, values: dict):\n    ...\n\nregistrar.register_correlation_hook(\n    correlation_hook, \"test_entity_type\", [[\"test_attr_type1\"]], [[\"test_attr_type2\"]]\n)\n</code></pre> <p>The order of running callbacks is determined automatically, based on the dependencies. If there is a cycle in the dependencies, a <code>ValueError</code> will be raised at registration. Also, if the provided dependency / output paths are invalid, a <code>ValueError</code> will be raised.</p>"},{"location":"modules/#snapshots-init-hook","title":"Snapshots Init Hook","text":"<p>This hook is called before each snapshot creation run begins. The use case is to enable your module to perform some initialization before the snapshot creation and associated correlation callbacks are executed. Your hook will recieve no arguments, and you may return a list of DataPointTask objects to be inserted into the system from this hook.</p> <pre><code>def snapshot_init_hook() -&gt; list[DataPointTask]:\n    ...\n    return [] \n\nregistrar.register_snapshot_init_hook(snapshot_init_hook)\n</code></pre>"},{"location":"modules/#snapshots-finalize-hook","title":"Snapshots Finalize Hook","text":"<p>This hook is called after each snapshot creation run ends. The use case is to enable your module to finish up after the snapshot creation and associated correlation callbacks are executed. Your hook will recieve no arguments, and you may again return a list of DataPointTask objects to be inserted into the system from this hook.</p> <pre><code>def snapshot_finalize_hook() -&gt; list[DataPointTask]:\n    ...\n    return [] \n\nregistrar.register_snapshot_finalize_hook(snapshot_init_hook)\n</code></pre>"},{"location":"modules/#periodic-update-callbacks","title":"Periodic Update Callbacks","text":"<p>Snapshots, which are designed to execute as quickly as possible parallelized over all workers, may not fit every use-case for updates, especially when the updates are not tied only to the entity's state. For example, fetching data from an external system where rate limits are a concern.</p> <p>This is where the updater module comes in. The updater module is responsible for updating all entities in the database over a longer time frame. Learn more about the updater module in the updater configuration page.</p>"},{"location":"modules/#periodic-update-hook","title":"Periodic Update Hook","text":"<p>The <code>register_periodic_update_hook</code> method expects a callable with the following signature: <code>Callable[[str, AnyEidT, dict], list[DataPointTask]]</code>, where the arguments are the entity type, entity ID and master record.  The callable should return a list of DataPointTask objects to perform (possibly empty).</p> <p>You must also pass a unique <code>hook_id</code> string when registering the hook, the entity type and  the period over which the hook should be called for all entities. The following example shows how to register a periodic update hook for an entity type <code>test_entity_type</code>. The hook will be called for all entities of this type every day.</p> <pre><code>def periodic_update_hook(\n        entity_type: str,\n        eid: str,  # (1)!\n        record: dict,\n) -&gt; list[DataPointTask]:\n    ...\n    return []\n\nregistrar.register_periodic_update_hook(\n    periodic_update_hook, \"test_id\", \"test_entity_type\", \"1d\"\n)\n</code></pre> <ol> <li><code>eid</code> may not be string, depending on the entity configuration, see Type of    <code>eid</code>.</li> </ol> <p>Set a Realistic Update Period</p> <p>Try to configure the period to match the real execution time of the registered hooks,  as when the period is too short, the hooks may not finish before the next batch is run, leading to missed runs and potentially even to doubling of the effective update period.</p>"},{"location":"modules/#periodic-update-eid-hook","title":"Periodic Update EID Hook","text":"<p>The periodic eid update hook is similar to the previous periodic update hook, but the entity record is not passed to the callback. This hook is useful when the entity record is not needed for the update, meaning the record data does not have to be fetched from the database.</p> <p>The <code>register_periodic_eid_update_hook</code> method expects a callable with the following signature: <code>Callable[[str, AnyEidT], list[DataPointTask]]</code>, where the first argument is the entity type and the second is the entity ID. The callable should return a list of DataPointTask objects to perform (possibly empty). All other arguments are the same as for the periodic update hook.</p> <pre><code>def periodic_eid_update_hook(\n        entity_type: str,\n        eid: str,  # (1)!\n) -&gt; list[DataPointTask]:\n    ...\n    return []\n\nregistrar.register_periodic_eid_update_hook(\n    periodic_eid_update_hook, \"test_id\", \"test_entity_type\", \"1d\"\n)\n</code></pre> <ol> <li><code>eid</code> may not be string, depending on the entity configuration, see Type of    <code>eid</code>.</li> </ol>"},{"location":"modules/#running-module-code-in-a-separate-thread","title":"Running module code in a separate thread","text":"<p>The module is free to run its own code in separate threads or processes. To synchronize such code with the platform, use the <code>start()</code> and <code>stop()</code> methods of the <code>BaseModule</code> class. the <code>start()</code> method is called after the platform is initialized, and the <code>stop()</code> method is called before the platform is shut down.</p> <pre><code>class MyModule(BaseModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._thread = None\n        self._stop_event = threading.Event()\n        self.log = logging.getLogger(\"MyModule\")\n\n    def start(self):\n        self._thread = threading.Thread(target=self._run, daemon=True)\n        self._thread.start()\n\n    def stop(self):\n        self._stop_event.set()\n        self._thread.join()\n\n    def _run(self):\n        while not self._stop_event.is_set():\n            self.log.info(\"Hello world!\")\n            time.sleep(1)\n</code></pre>"},{"location":"scripts/","title":"Scripts","text":"<p>DP\u00b3 provides a set of scripts for to help you with various tasks. They are exposed using the <code>dp3-script</code> command. You can list the available scripts by running </p> <pre><code>dp3-script ls\n</code></pre> <p>And you can run the script by running</p> <pre><code>dp3-script &lt;script-name&gt; [args]\n</code></pre>"},{"location":"scripts/#script-requirements","title":"Script requirements","text":"<p>Some of the scripts which manipulate the datapoint logs require pandas to be installed. All the script requirements are placed into an optional <code>scripts</code> dependency, which you can install by running</p> <pre><code>pip install dp-cubed[scripts]\n</code></pre>"},{"location":"scripts/#notable-scripts","title":"Notable scripts","text":""},{"location":"scripts/#dp3-script-dummy_senderpy","title":"<code>dp3-script dummy_sender.py</code>","text":"<p>Simple datapoint sender script for testing local DP3 instance.  Reads the JSON datapoint log from the API and sends it to the specified DP3 instance.</p>"},{"location":"scripts/#dp3-script-rmq_reconfiguresh","title":"<code>dp3-script rmq_reconfigure.sh</code>","text":"<p>Used during RabbitMQ (re)configuration, sets up the appropriate queues and exchanges for  the specified number of workers. For details, see the deployment installation guide.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>DP\u00b3 configuration folder consists of these files and folders:</p> <ul> <li><code>\ud83d\udcc1 db_entities</code> - Database entities configuration folder. This is your data model.</li> <li><code>\ud83d\udcc1 modules</code> - Modules configuration folder.</li> <li><code>\ud83d\udcc4 api.yml</code> - API configuration file.</li> <li><code>\ud83d\udcc4 control.yml</code> - Configuration file controlling allowed <code>/control</code> endpoint actions.</li> <li><code>\ud83d\udcc4 database.yml</code> - Connection to the DB.</li> <li><code>\ud83d\udcc4 event_logging.yml</code> - Tracking the app operation using Redis.</li> <li><code>\ud83d\udcc4 garbage_collector.yml</code> - Removing entities with expired lifetimes.</li> <li><code>\ud83d\udcc4 history_manager.yml</code> - How often is history management performed.</li> <li><code>\ud83d\udcc4 processing_core.yml</code> - Settings of main application workers.</li> <li><code>\ud83d\udcc4 snapshots.yml</code> - How often are entity snapshots taken.</li> <li><code>\ud83d\udcc4 updater.yml</code> - Periodic updates of all entities over a longer time frame.</li> </ul> <p>The details of their meaning and usage is explained in their relative pages.</p>"},{"location":"configuration/#example-configuration","title":"Example configuration","text":"<p>Example configuration is included <code>config/</code> folder in DP\u00b3 repository.</p>"},{"location":"configuration/api/","title":"API","text":"<p>The API configuration file is currently used only to configure the logging of datapoints. The file <code>api.yml</code> looks like this:</p> <pre><code>cors:\n  allow_origins: [\"*\"]\n\ndatapoint_logger:\n  good_log: false\n  bad_log: false  # (1)!\n</code></pre> <ol> <li>Use an absolute path, for example <code>/tmp/bad_dp.json.log</code></li> </ol> <p>The <code>cors</code> section is used to configure the CORS headers. The <code>allow_origins</code> parameter is a list of allowed origins. The default value is <code>[\"*\"]</code>, which means that any origin is allowed.</p> <p>Logging of datapoints is set in the <code>datapoint_logger</code> section. The <code>good_log</code> and <code>bad_log</code> parameters are paths to files where the datapoints will be logged. If set to <code>false</code>, the logging is disabled.</p>"},{"location":"configuration/control/","title":"Control","text":"<p>The <code>Control</code> module serves the <code>/control/...</code> endpoints. This is used to execute actions that are not part of the normal operation of the application, at a time when the user decides to do so. Useful for management of the application, especially when there are changes made to the configuration.</p> <p>The <code>control.yml</code> file looks like this: </p> <pre><code>allowed_actions: [\n  make_snapshots, # (1)!\n  refresh_on_entity_creation, # (2)!\n  refresh_module_config, # (3)!\n]\n</code></pre> <ol> <li>Makes an out-of-order snapshot of all entities</li> <li>Re-runs the <code>on_entity_creation</code> callback for selected <code>etype</code></li> <li>Re-runs the <code>load_config</code> for selected module and will refresh the values derived by the module</li> </ol> <p>The <code>allowed_actions</code> parameter is a list of actions that are allowed to be executed. The way to use this file is to simply comment out the actions that you do not want to be allowed.</p>"},{"location":"configuration/control/#actions","title":"Actions","text":"<p>The swagger API documentation provides a simple GUI for executing the actions, but they can also be executed directly by sending a <code>GET</code> request to the endpoint. These are the actions that are currently available:</p>"},{"location":"configuration/control/#make_snapshots","title":"<code>make_snapshots</code>","text":"<p>This action will make an out-of-order snapshot of all entities.</p>"},{"location":"configuration/control/#refresh_on_entity_creation","title":"<code>refresh_on_entity_creation</code>","text":"<p>This action will re-run the <code>on_entity_creation</code> callback for a selected <code>etype</code>, which is passed as a query parameter. Will fail if the <code>etype</code> is not provided.</p>"},{"location":"configuration/control/#refresh_module_config","title":"<code>refresh_module_config</code>","text":"<p>This action will re-run the <code>load_config</code> for selected module and will refresh the values derived by the module. The module name is to be passed as a query parameter and the action will fail without it.</p> <p>The refreshing part of this action covers the <code>on_entity_creation</code> and <code>on_new_attr</code> callbacks when the <code>refresh</code> argument is set on callback registration, otherwise no refresh is performed, only the <code>load_config</code> is re-run.</p>"},{"location":"configuration/database/","title":"Database","text":"<p>File <code>database.yml</code> specifies mainly MongoDB database connection details and credentials.</p> <p>It looks like this:</p> <pre><code># MongoDB Connection configuration\nusername: \"dp3_user\"\npassword: \"dp3_password\"\ndb_name: \"dp3_database\"\n\n# Standalone - single host, just specify the connection\nconnection:\n  mode: \"standalone\"\n  host:\n    address: \"127.0.0.1\"\n    port: 27017\n</code></pre>"},{"location":"configuration/database/#authentication-and-database","title":"Authentication and database","text":"<p>The base of configuration is as follows:</p> Parameter Data-type Default value Description <code>username</code> string <code>dp3</code> Username for connection to DB. Escaped using <code>urllib.parse.quote_plus</code>. <code>password</code> string <code>dp3</code> Password for connection to DB. Escaped using <code>urllib.parse.quote_plus</code>. <code>db_name</code> string <code>dp3</code> Database name to be utilized by DP\u00b3."},{"location":"configuration/database/#connection","title":"Connection","text":"<p>There are two modes of connection to MongoDB:</p> <ul> <li><code>standalone</code> - single host, just specify the connection</li> <li><code>replica_set</code> - multiple hosts, specify the connection and replica set name</li> </ul> Parameter Data-type Default value Description <code>mode</code> string - Connection mode. Must be either \"standalone\" or \"replica\". <code>replica_set</code> string <code>dp3</code> Replica set name, only applicable in \"replica\" mode. <code>address</code> string <code>localhost</code> IP address or hostname for connection to DB. <code>port</code> int 27017 Listening port of DB."},{"location":"configuration/database/#standalone-connection","title":"Standalone connection","text":"<pre><code>connection:\n  mode: \"standalone\"\n  host:\n    address: \"127.0.0.1\"\n    port: 27017\n</code></pre>"},{"location":"configuration/database/#replica-set-connection","title":"Replica set connection","text":"<pre><code>connection:\n  mode: \"replica\"\n  replica_set: \"dp3_replica\"\n  hosts:\n    - address: \"127.0.0.1\"\n      port: 27017\n    - address: \"127.0.0.2\"\n      port: 27017\n    - address: \"127.0.0.3\"\n      port: 27017\n</code></pre>"},{"location":"configuration/db_entities/","title":"DB entities","text":"<p>Files in <code>db_entities</code> folder describe entities and their attributes. You can think of entity as class from object-oriented programming.  This serves as sort of schema for the database DP\u00b3 uses. How DP\u00b3 deals with changes to the <code>db_entities</code> is described in the schema tracking section.</p> <p>Below is YAML file (e.g. <code>db_entities/bus.yml</code>) corresponding to bus tracking system example from Data model chapter.</p> <pre><code>entity:\n  id: bus\n  name: Bus\n  snapshot: true\nattribs:\n  # Attribute `label`\n  label:\n    name: Label\n    description: Custom label for the bus.\n    type: plain\n    data_type: string\n    editable: true\n\n  # Attribute `location`\n  location:\n    name: Location\n    description: Location of the bus in a particular time. Value are GPS \\\n      coordinates (array of latitude and longitude).\n    type: observations\n    data_type: array&lt;float&gt;\n    history_params:\n      pre_validity: 1m\n      post_validity: 1m\n      max_age: 30d\n\n  # Attribute `speed`\n  speed:\n    name: Speed\n    description: Speed of the bus in a particular time. In km/h.\n    type: observations\n    data_type: float\n    history_params:\n      pre_validity: 1m\n      post_validity: 1m\n      max_age: 30d\n\n  # Attribute `passengers_in_out`\n  passengers_in_out:\n    name: Passengers in/out\n    description: Number of passengers getting in or out of the bus. Distinguished by the doors used (front, middle, back). Regularly sampled every 10 minutes.\n    type: timeseries\n    timeseries_type: regular\n    timeseries_params:\n      max_age: 14d\n    time_step: 10m\n    series:\n      front_in:\n        data_type: int\n      front_out:\n        data_type: int\n      middle_in:\n        data_type: int\n      middle_out:\n        data_type: int\n      back_in:\n        data_type: int\n      back_out:\n        data_type: int\n\n  # Attribute `driver` to link the driver of the bus at a given time.\n  driver:\n    name: Driver\n    description: Driver of the bus at a given time.\n    type: observations\n    data_type: link&lt;driver&gt;\n    history_params:\n      pre_validity: 1m\n      post_validity: 1m\n      max_age: 30d\n</code></pre>"},{"location":"configuration/db_entities/#entity","title":"Entity","text":"<p>Entity is described simply by:</p> Parameter Data-type Default value Description <code>id</code> string (identifier) (mandatory) Short string identifying the entity type, it's machine name (must match regex <code>[a-zA-Z_][a-zA-Z0-9_-]*</code>). Lower-case only is recommended. <code>id_data_type</code> string \"string\" Data type of the entity id (<code>eid</code>) value, see Supported eid data types. <code>name</code> string (mandatory) Attribute name for humans. May contain any symbols. <code>snapshot</code> bool (mandatory) Whether to create snapshots of the entity. See Architecture for more details. <code>lifetime</code> <code>Lifetime Spec</code> <code>Immortal Lifetime</code> Defines the lifetime of the entitiy, entities are never deleted by default. See the Entity Lifetimes for details."},{"location":"configuration/db_entities/#supported-entity-id-data-types","title":"Supported entity id data types","text":"<p>Only a subset of primitive data types is supported for entity ids. The supported data types are:</p> <ul> <li><code>string</code> (default)</li> <li><code>int</code>: 32-bit signed integer (range from -2147483648 to +2147483647)</li> <li><code>ipv4</code>: IPv4 address, represented as IPv4Address (passed as dotted-decimal string)</li> <li><code>ipv6</code>: IPv6 address, represented as IPv6Address (passed as string in short or full format)</li> <li><code>mac</code>: MAC address, represented as MACAddress  (passed as string)</li> </ul> <p>Whenever writing a piece of code independent of a specific configuration, the <code>AnyEidT</code> type alias should be used.</p>"},{"location":"configuration/db_entities/#attributes","title":"Attributes","text":"<p>Each attribute is specified by the following set of parameters:</p>"},{"location":"configuration/db_entities/#base","title":"Base","text":"<p>These apply to all types of attributes (plain, observations and timeseries).</p> Parameter Data-type Default value Description <code>id</code> string (identifier) (mandatory) Short string identifying the attribute, it's machine name (must match this regex <code>[a-zA-Z_][a-zA-Z0-9_-]*</code>). Lower-case only is recommended. <code>type</code> string (mandatory) Type of attribute. Can be either <code>plain</code>, <code>observations</code> or <code>timeseries</code>. <code>name</code> string (mandatory) Attribute name for humans. May contain any symbols. <code>ttl</code> timedelta <code>0</code> Optional extension of TTL of the entity, will be ignored if the lifetime type does not match. The time extension is calculated from <code>t2</code> if possible, otherwise from the current time (for plain attributes). <code>description</code> string <code>\"\"</code> Longer description of the attribute, if needed."},{"location":"configuration/db_entities/#plain-specific-parameters","title":"Plain-specific parameters","text":"Parameter Data-type Default value Description <code>data_type</code> string (mandatory) Data type of attribute value, see Supported data types. <code>editable</code> bool <code>false</code> Whether value of this attribute is editable via web interface."},{"location":"configuration/db_entities/#observations-specific-parameters","title":"Observations-specific parameters","text":"Parameter Data-type Default value Description <code>data_type</code> string (mandatory) Data type of attribute value, see Supported data types. <code>editable</code> bool <code>false</code> Whether value of this attribute is editable via web interface. <code>confidence</code> bool <code>false</code> Whether a confidence value should be stored along with data value or not. More details. <code>multi_value</code> bool <code>false</code> Whether multiple values can be set at the same time. More details, Arrays vs Multi-value attributes. <code>history_params</code> object, see below (mandatory) History and time aggregation parameters. A subobject with fields described in the table below. <code>history_force_graph</code> bool <code>false</code> By default, if data type of attribute is array, we show it's history on web interface as table. This option can force tag-like graph with comma-joined values of that array as tags."},{"location":"configuration/db_entities/#history-params","title":"History params","text":"<p>Description of <code>history_params</code> subobject (see table above).</p> Parameter Data-type Default value Description <code>max_age</code> <code>&lt;int&gt;&lt;s/m/h/d&gt;</code> (e.g. <code>30s</code>, <code>12h</code>, <code>7d</code>) <code>null</code> How many seconds/minutes/hours/days of history to keep (older data-points/intervals are removed). <code>max_items</code> int (&gt; 0) <code>null</code> How many data-points/intervals to store (oldest ones are removed when limit is exceeded). Currently not implemented. <code>expire_time</code> <code>&lt;int&gt;&lt;s/m/h/d&gt;</code> or <code>inf</code> (infinity) infinity How long after the end time (<code>t2</code>) is the last value considered valid (i.e. is used as \"current value\"). Zero (<code>0</code>) means to strictly follow <code>t1</code>, <code>t2</code>. Zero can be specified without a unit (<code>s/m/h/d</code>). Currently not implemented. <code>pre_validity</code> <code>&lt;int&gt;&lt;s/m/h/d&gt;</code> (e.g. <code>30s</code>, <code>12h</code>, <code>7d</code>) <code>0s</code> Max time before <code>t1</code> for which the data-point's value is still considered to be the \"current value\" if there's no other data-point closer in time. <code>post_validity</code> <code>&lt;int&gt;&lt;s/m/h/d&gt;</code> (e.g. <code>30s</code>, <code>12h</code>, <code>7d</code>) <code>0s</code> Max time after <code>t2</code> for which the data-point's value is still considered to be the \"current value\" if there's no other data-point closer in time. <code>aggregate</code> <code>bool</code> <code>true</code> Whether to aggregate data-points in DB master records. Currently only identical value aggregation is supported. More details <p>Note: At least one of <code>max_age</code> and <code>max_items</code> SHOULD be defined, otherwise the amount of stored data can grow unbounded.</p>"},{"location":"configuration/db_entities/#timeseries-specific-parameters","title":"Timeseries-specific parameters","text":"Parameter Data-type Default value Description <code>timeseries_type</code> string (mandatory) One of: <code>regular</code>, <code>irregular</code> or <code>irregular_intervals</code>. See chapter Data model for explanation. <code>series</code> object of objects, see below (mandatory) Configuration of series of data represented by this timeseries attribute. <code>timeseries_params</code> object, see below Other timeseries parameters. A subobject with fields described by the table below."},{"location":"configuration/db_entities/#series","title":"Series","text":"<p>Description of <code>series</code> subobject (see table above).</p> <p>Key for <code>series</code> object is <code>id</code> - short string identifying the series (e.g. <code>bytes</code>, <code>temperature</code>, <code>parcels</code>).</p> Parameter Data-type Default value Description <code>type</code> string (mandatory) Data type of series. Only <code>int</code> and <code>float</code> are allowed (also <code>time</code>, but that's used internally, see below). <p>Time <code>series</code> (axis) is added implicitly by DP\u00b3 and this behaviour is specific to selected <code>timeseries_type</code>:</p> <ul> <li>regular: <code>\"time\": { \"data_type\": \"time\" }</code></li> <li>irregular: <code>\"time\": { \"data_type\": \"time\" }</code></li> <li>irregular_timestamps: <code>\"time_first\": { \"data_type\": \"time\" }, \"time_last\": { \"data_type\": \"time\" }</code></li> </ul>"},{"location":"configuration/db_entities/#timeseries-params","title":"Timeseries params","text":"<p>Description of <code>timeseries_params</code> subobject (see table above).</p> Parameter Data-type Default value Description <code>max_age</code> <code>&lt;int&gt;&lt;s/m/h/d&gt;</code> (e.g. <code>30s</code>, <code>12h</code>, <code>7d</code>) <code>null</code> How many seconds/minutes/hours/days of history to keep (older data-points/intervals are removed). <code>time_step</code> <code>&lt;int&gt;&lt;s/m/h/d&gt;</code> (e.g. <code>30s</code>, <code>12h</code>, <code>7d</code>) (mandatory) for regular timeseries, <code>null</code> otherwise \"Sampling rate in time\" of this attribute. For example, with <code>time_step = 10m</code> we expect data-point at 12:00, 12:10, 12:20, 12:30,... Only relevant for regular timeseries. <p>Note: <code>max_age</code> SHOULD be defined, otherwise the amount of stored data can grow unbounded.</p>"},{"location":"configuration/db_entities/#supported-data-types","title":"Supported data types","text":"<p>List of supported values for parameter <code>data_type</code>:</p>"},{"location":"configuration/db_entities/#primitive-types","title":"Primitive types","text":"<ul> <li><code>tag</code>: set/not_set (When the attribute is set, its value is always assumed to be <code>true</code>, the \"v\" field doesn't have to be stored.)</li> <li><code>binary</code>: <code>true</code>/<code>false</code>/not_set (Attribute value is <code>true</code> or <code>false</code>, or the attribute is not set at all.)</li> <li><code>string</code></li> <li><code>int</code>: 32-bit signed integer (range from -2147483648 to +2147483647)</li> <li><code>int64</code>: 64-bit signed integer (use when the range of normal <code>int</code> is not sufficent)</li> <li><code>float</code></li> <li><code>time</code>: Timestamp in <code>YYYY-MM-DD[T]HH:MM[:SS[.ffffff]][Z or [\u00b1]HH[:]MM]</code> format or timestamp since 1.1.1970 in seconds or milliseconds.</li> <li><code>ipv4</code>: IPv4 address, represented as IPv4Address (passed as dotted-decimal string)</li> <li><code>ipv6</code>: IPv6 address, represented as IPv6Address (passed as string in short or full format)</li> <li><code>mac</code>: MAC address, represented as MACAddress  (passed as string)</li> <li><code>json</code>: Any JSON object can be stored, all processing is handled by user's code. This is here for special cases which can't be mapped to any other data type.</li> </ul>"},{"location":"configuration/db_entities/#composite-types","title":"Composite types","text":"<ul> <li><code>category&lt;data_type; category1, category2, ...&gt;</code>: Categorical values. Use only when a fixed set of values should be allowed, which should be specified in the second part of the type definition. The first part of the type definition describes the data_type of the category.</li> <li><code>array&lt;data_type&gt;</code>: An array of values of specified data type (which must be one of the primitive types above or a link to another entity), e.g. <code>array&lt;int&gt;</code>. Deciding whether to use array or multi-value attribute is not always trivial, see Arrays vs Multi-value attributes.</li> <li><code>set&lt;data_type&gt;</code>: Same as array, but values can't repeat and order is irrelevant.</li> <li><code>dict&lt;keys&gt;</code>: Dictionary (object) containing multiple values as subkeys. keys should contain a comma-separated list of key names and types separated by colon, e.g. <code>dict&lt;port:int, protocol:string, tag?:string&gt;</code>. Whitespace is allowed after colons. By default, all fields are mandatory (i.e. a data-point missing some subkey will be refused), to mark a field as optional, put <code>?</code> after its name. Only the primitive data types can be used here, multi-level dicts are not supported.</li> </ul>"},{"location":"configuration/db_entities/#relationships","title":"Relationships","text":"<ul> <li><code>link&lt;entity_type&gt;</code>: Link to a record of the specified type, e.g. <code>link&lt;ip&gt;</code></li> <li><code>link&lt;entity_type,data_type&gt;</code>: Link to a record of the specified type, carrying additional data, e.g. <code>link&lt;ip,int&gt;</code></li> <li><code>link&lt;...;mirror=attr_name&gt;</code>: A mirrored link - at the end of link specification,    you may enter a <code>mirror=attr_name</code> declaration, where <code>attr_name</code> is the name of an attribute in the linked entity.   This attribute will be automatically defined in the target entity, do not define it in configuration. When a relationship   is mirrored, the relationship from entity <code>A</code> to entity <code>B</code> will automatically create a    relationship from entity <code>B</code> to entity <code>A</code> in snapshots.   This is useful if you need to track a relationship in both directions, but managing both directions is not reasonable.</li> </ul>"},{"location":"configuration/db_entities/#schema-tracking","title":"Schema Tracking","text":"<p>In order to maintain a consistent database state, DP\u00b3 tracks changes to the <code>db_entities</code> folder. The current schema is stored in the database, and is updated automatically on worker start-up when the <code>db_entities</code> folder is changed.</p> <p>For additive changes (adding new entities or attributes), the changes are applied automatically, as only the schema itself needs to be modified. For changes that would require modification to the entity collections (e.g. changing the data-type of an attribute or deleting it),  the changes are not applied automatically to protect the database contents against accidental deletion. The workers will refuse to start, prompting you to run <code>dp3 schema-update</code> in their logs. You then have to run <code>dp3 schema-update</code> manually to confirm the application of the changes. Find out more about the <code>dp3 schema-update</code> using the <code>--help</code> option.</p>"},{"location":"configuration/db_entities/#faq","title":"FAQ","text":""},{"location":"configuration/db_entities/#arrays-vs-multi-value-attributes","title":"Arrays vs Multi-value attributes","text":"<p>Let's say you have data in an array, and are unsure how to model it into DP\u00b3 attributes.</p> <p>Choose <code>data_type: array&lt;...&gt;</code> when:</p> <ul> <li>You're working with numerical data,</li> <li>The list is ordered, and the order is important (or you're modelling a tuple),</li> <li>The individual values in the list generally do not repeat between different datapoints, and    does not make sense to aggregate them.</li> </ul> <p>In that case, you should set <code>multi_value</code> to <code>false</code>, and DP\u00b3 will handle the data as a single value. You are responsible for not sending overlapping datapoints, where a datapoint contains the whole array. The aggregation of this attribute will be limited, but that is usually desirable.</p> <p>Choose <code>multi_value: true</code> when:</p> <ul> <li>You're working with elements of categorical data or data that has a composite type (i.e. a struct or dictionary),</li> <li>The list is unordered or the order is not important,</li> <li>The individual values in the list generally repeat between different datapoints,   represent some state of the entity, and it makes sense to aggregate them.</li> </ul> <p>Then you should set the <code>data_type</code> to the element type. Your datapoints should contain a single value, but DP\u00b3 will handle the data as a list of values, and you can send overlapping datapoints.  Value aggregation will be done on a per-element basis.</p>"},{"location":"configuration/event_logging/","title":"Event logging","text":"<p>Event logging is done using Redis and allows to count arbitrary events across multiple processes (using shared counters in Redis) and in various time intervals.</p> <p>More information can be found in Github repository of EventCountLogger.</p> <p>Configuration file <code>event_logging.yml</code> looks like this:</p> <pre><code>redis:\n  host: localhost\n  port: 6379\n  db: 1\n\ngroups:\n  # Main events of Task execution\n  te:\n    events:\n      - task_processed\n      - task_processing_error\n    intervals: [ \"5m\", \"2h\" ] # (1)!\n    sync-interval: 1 # (2)!\n  # Number of processed tasks by their \"src\" attribute\n  tasks_by_src:\n    events: [ ]\n    auto_declare_events: true\n    intervals: [ \"5s\", \"5m\" ]\n    sync-interval: 1\n</code></pre> <ol> <li>Two intervals - 5 min and 2 hours for longer-term history in Munin/Icinga</li> <li>Cache counts locally, push to Redis every second</li> </ol>"},{"location":"configuration/event_logging/#redis","title":"Redis","text":"<p>This section describes Redis connection details:</p> Parameter Data-type Default value Description <code>host</code> string <code>localhost</code> IP address or hostname for connection to Redis. <code>port</code> int 6379 Listening port of Redis. <code>db</code> int 0 Index of Redis DB used for the counters (it shouldn't be used for anything else)."},{"location":"configuration/event_logging/#groups","title":"Groups","text":"<p>The default configuration groups enables logging of events in task execution, namely <code>task_processed</code> and <code>task_processing_error</code>.</p> <p>To learn more about the group configuration for EventCountLogger,  please refer to the official documentation.</p>"},{"location":"configuration/garbage_collector/","title":"Garbage Collector","text":"<p>The <code>GarbageCollector</code> is responsible for removing entities that have expired lifetimes.</p> <p>There is only one parameter for the <code>GarbageCollector</code> as of now, the <code>collection_rate</code>. It is a cron schedule for running the garbage collector procedures, for collecting entities with both TTL and weak lifetimes. See CronExpression docs for details. By default, this will be set to once a day at 3:00 AM.</p> <p>File <code>garbage_collector.yml</code> looks like this:</p> <pre><code>collection_rate:\n  minute: \"5,25,45\"\n</code></pre>"},{"location":"configuration/history_manager/","title":"History manager","text":"<p>The concepts of history management in DP3 are described here. History manager is responsible for:</p> <ul> <li>Datapoint aggregation - merging identical value datapoints in master records</li> <li>Deleting old datapoints from master records</li> <li>Deleting old snapshots</li> <li>Archiving old datapoints from raw collections</li> </ul> <p>Configuration file <code>history_manager.yml</code> is very simple:</p> <pre><code>aggregation_schedule:  # (1)!\n  minute: \"*/10\"  \n\nmark_datapoints_schedule: # (2)!\n  hour: \"7,19\"\n  minute: \"45\"\ndatapoint_cleaning_schedule:  # (3)!\n  minute: \"*/30\"\n\nsnapshot_cleaning:\n  schedule: {minute: \"15,45\"}  # (4)!\n  older_than: 7d  # (5)!\n\ndatapoint_archivation:\n  schedule: {hour: 2, minute: 0}  # (6)!\n  older_than: 7d  # (7)!\n  archive_dir: \"data/datapoints/\"  # (8)!\n</code></pre> <ol> <li>Parameter <code>aggregation_schedule</code> sets the interval for DP\u00b3 to aggregate observation datapoints in master records. This should be scheduled more often than cleaning of datapoints.</li> <li>Parameter <code>mark_datapoints_schedule</code> sets the interval when the datapoint timestamps are marked for all entities in a master collection. This should be scheduled very rarely, as it's a very expensive operation. </li> <li>Parameter <code>datapoint_cleaning_schedule</code> sets interval when should DP\u00b3 check if any data in master record of observations and timeseries attributes isn't too old and if there's something too old, removes it. To control what is considered as \"too old\", see parameter <code>max_age</code> in Database entities configuration.</li> <li>Parameter <code>snapshot_cleaning.schedule</code> sets the interval for DP\u00b3 to clean the snapshots collection. Optimally should be scheduled outside the snapshot creation window. See Snapshots configuration for more.  </li> <li>Parameter <code>snapshot_cleaning.older_than</code> sets how old must a snapshot be to be deleted.</li> <li>Parameter <code>datapoint_archivation.schedule</code> sets interval for DP\u00b3 to archive datapoints from raw collections.</li> <li>Parameter <code>datapoint_archivation.older_than</code> sets how old must a datapoint be to be archived.</li> <li>Parameter <code>datapoint_archivation.archive_dir</code> sets directory where should be archived old datapoints. If directory doesn't exist, it will be created, but write priviledges must be set correctly. Can be also set to <code>null</code> (or not set) to disable archivation and only delete old data.</li> </ol> <p>The schedule dictionaries are transformed to cron expressions, see CronExpression docs for details.</p>"},{"location":"configuration/lifetimes/","title":"Entity Lifetimes","text":"<p>There are currently three supported types of lifetimes:</p> <ul> <li>Immortal - these entities are never removed unless explicitly ordered by the user. (default)</li> <li>Time-to-live tokens - The platform will keep a list of TTL tokens for each entity, and will remove the entity when all tokens expire.</li> <li>Weak entities - These entities are removed when they are no longer referenced by any other entity.</li> </ul> <p>When the lifetime of an entity ends, the entity is removed from the database, and all its data is deleted. How often this happens is configured in the Garbage Collector configuration.</p> <p>Let us examine the options in detail.</p>"},{"location":"configuration/lifetimes/#immortal-entities","title":"Immortal entities","text":"<p>Immortal entities are never removed unless explicitly ordered by the user. There are no special options for this lifetime type as of now, and this is the default lifetime type, i.e. it is used when no lifetime options are specified.</p> <p>An explicit configuration might look like this:</p> <pre><code>entity:\n  id: bus_line\n  name: Bus Line\n  snapshot: false\n  lifetime:\n    type: immortal  # (1)!\n</code></pre> <ol> <li>Buses and drivers may change, but we do not remove the serviced bus lines</li> </ol>"},{"location":"configuration/lifetimes/#time-to-live-tokens","title":"Time-to-live tokens","text":"<p>The platform will keep a list of TTL tokens for each entity, and will remove the entity when all of its tokens expire. There are several options for this lifetime type:</p> Option name type default description <code>on_create</code> <code>timedelta</code> required The base lifetime of an entity that is set on creation <code>mirror_data</code> <code>bool</code> <code>True</code> If <code>True</code>, the lifetime of the entity is extended by the             <code>max_age</code> of the incoming observations and timeseries data-points. <p>TTL tokens can be attached to new data datapoints on per-attribute basis (see attribute configuration), set to mirror the lifetime of the data (<code>mirror_data</code>), or sent explicitly using the API (see <code>/entity/{etype}/{eid}/ttl</code>).</p> <p>Here are a couple of examples of how this type of lifetime can be configured:</p>"},{"location":"configuration/lifetimes/#the-timer","title":"The Timer","text":"<p>The entity will receive a TTL token on its creation, and will be automatically deleted after it expires. You can supplement this by explicitly extending the lifetime of the entity using the API,  or explicitly configuring a TTL on an attribute that will extend the lifetime.</p> <pre><code>entity:\n  id: traffic_jam\n  name: Traffic Jam\n  snapshot: true\n  lifetime:\n    type: ttl\n    on_create: 1h\n    mirror_data: false\n</code></pre>"},{"location":"configuration/lifetimes/#data-driven-lifetime","title":"Data-driven lifetime","text":"<p>This is the default behavior of <code>mirror_data</code> option. The entity will be kept alive for as long as there are observations data for it. The lifetime of the entity will be extended by the <code>max_age</code> of the incoming data-points.</p> <pre><code>entity:\n  id: passenger\n  name: Passenger \n  snapshot: true\n  lifetime:\n    type: ttl\n    on_create: 30m  # (1)!\n    mirror_data: true\n</code></pre> <ol> <li>30 minutes.</li> </ol>"},{"location":"configuration/lifetimes/#mixture-of-both","title":"Mixture of both","text":"<p>You can combine the two previous examples to create a lifetime that is extended by the incoming observation data, but has additional <code>ttl</code> set on incoming plain datapoints, or can be extended explicitly using the API. The lifetime of the entity will be extended using all the sources you choose to use.</p>"},{"location":"configuration/lifetimes/#weak-entities","title":"Weak entities","text":"<p>The entity will be removed when it is no longer referenced by any other entity. This is useful when you want to track some data that relates to multiple of your entities, but want to discard it if no such entity exists anymore. There are no special options for this lifetime type as of now.</p> <pre><code>entity:\n  id: shared_ticket\n  name: Shared Ticket\n  snapshot: true\n  lifetime:\n    type: weak\n</code></pre> Remember the other side <p>For the weak entity lifetime to work, there has to be a reference from an another entity, for example:</p> <pre><code>entity:\n  id: passenger\n  name: Passenger \n  snapshot: true\n  lifetime:\n    type: ttl\n    on_create: 30m\nattribs:\n  shared_tickets:\n    name: Shared Tickets\n    description: Tickets this passenger shares with other passengers.\n    type: observations\n    data_type: link&lt;shared_ticket&gt;\n    multi_value: true\n    history_params:\n      pre_validity: 0\n      post_validity: 0\n      max_age: 14d\n      aggregate: false\n</code></pre>"},{"location":"configuration/lifetimes/#continue-to","title":"Continue to","text":"<p>When the lifetime of an entity ends, the entity is removed from the database, and all its data is deleted. How often this happens is configured in the Garbage Collector configuration.</p>"},{"location":"configuration/modules/","title":"Modules","text":"<p>Folder <code>modules/</code> optionally contains any module-specific configuration.</p> <p>This configuration doesn't have to follow any required format (except being YAML files).</p> <p>In secondary modules, you can access the configuration:</p> <pre><code>from dp3 import g\n\nprint(g.config[\"modules\"][\"MODULE_NAME\"])\n</code></pre> <p>Here, the <code>MODULE_NAME</code> corresponds to <code>MODULE_NAME.yml</code> file in <code>modules/</code> folder.</p>"},{"location":"configuration/processing_core/","title":"Processing core","text":"<p>Processing core's configuration in <code>processing_core.yml</code> file looks like this:</p> <pre><code>msg_broker:\n  host: localhost\n  port: 5672\n  virtual_host: /\n  username: dp3_user\n  password: dp3_password\nworker_processes: 2\nworker_threads: 16\nmodules_dir: \"../dp3_modules\"\nenabled_modules:\n  - \"module_one\"\n  - \"module_two\"\n</code></pre>"},{"location":"configuration/processing_core/#message-broker","title":"Message broker","text":"<p>Message broker section describes connection details to RabbitMQ (or compatible) broker.</p> Parameter Data-type Default value Description <code>host</code> string <code>localhost</code> IP address or hostname for connection to broker. <code>port</code> int 5672 Listening port of broker. <code>virtual_host</code> string <code>/</code> Virtual host for connection to broker. <code>username</code> string <code>guest</code> Username for connection to broker. <code>password</code> string <code>guest</code> Password for connection to broker."},{"location":"configuration/processing_core/#worker-processes","title":"Worker processes","text":"<p>Number of worker processes. This has to be at least 1.</p> <p>If changing number of worker processes, the following process must be followed:</p> <ol> <li>stop all inputs writing to task queue (e.g. API)</li> <li>when all queues are empty, stop all workers</li> <li>reconfigure queues in RabbitMQ using script found in <code>/scripts/rmq_reconfigure.sh</code></li> <li>change the settings here and in init scripts for worker processes (e.g. supervisor)</li> <li>reload workers (e.g. using <code>supervisorctl</code>) and start all inputs again</li> </ol>"},{"location":"configuration/processing_core/#worker-threads","title":"Worker threads","text":"<p>Number of worker threads per process.</p> <p>This may be higher than number of CPUs, because this is not primarily intended to utilize computational power of multiple CPUs (which Python cannot do well anyway due to the GIL), but to mask long I/O operations (e.g. queries to external services via network).</p>"},{"location":"configuration/processing_core/#modules-directory","title":"Modules directory","text":"<p>Path to directory with plug-in (secondary) modules.</p> <p>Relative path is evaluated relative to location of this configuration file.</p>"},{"location":"configuration/processing_core/#enabled-modules","title":"Enabled modules","text":"<p>List of plug-in modules which should be enabled in processing pipeline.</p> <p>Name of module filename without <code>.py</code> extension must be used!</p>"},{"location":"configuration/snapshots/","title":"Snapshots","text":"<p>Snapshots configuration is straightforward. You can set the <code>creation_rate</code> - a cron schedule for creating new snapshots (every 30 minutes by default). See CronExpression docs for details.</p> <p>The second option is <code>keep_empty</code>. If set to <code>true</code>, the links to entities that are <code>null</code> (i.e., no such entity exists) will create an empty entity for the purpose of the snapshot, and these entities will be saved as snapshots that batch. Otherwise, the <code>null</code> links will be deleted. This option is <code>true</code> by default.</p> <p>File <code>snapshots.yml</code> looks like this:</p> <pre><code>creation_rate:\n  minute: \"*/30\"\n\nkeep_empty: true\n</code></pre>"},{"location":"configuration/updater/","title":"Updater","text":"<p>The updater module is responsible for updating all entities in the database over a longer time frame. The main use-case is to execute queries to external systems and update the entities with the results, avoiding the present rate limits and other restrictions. However, it can be used for any kind of long-running operation. This functionality is exposed to modules using the registrar API.</p> <p>For better robustness, the updates happen in batches, which are executed on a configurable schedule.</p> <p>The global updater configuration requires setting of the batch update period. This is done using the <code>update_batch_cron</code> - a cron schedule for the batch update. See CronExpression docs for details. The second item is <code>update_batch_period</code> - the period of the batch update. This is a string matching the format of <code>\\d+[smhd]</code> (for second, minute, hour, day respectively).</p> <p><code>update_batch_cron</code> and <code>update_batch_period</code> must be set to equivalent values!</p> <p>Both items are required and must be set to equivalent time values. For example, if <code>update_batch_cron</code> is set to run every 30 seconds -  <code>{ minute: \"*\", second: \"*/30\" }</code>, <code>update_batch_period</code> should be set to <code>30s</code>.</p> <p>The default configuration of <code>updater.yml</code> looks like this:</p> <pre><code>update_batch_cron: { minute: \"*/5\", second: \"0\" }\nupdate_batch_period: \"5m\"\n</code></pre> <p>Try to find a balance between having batches run too often (some batches may execute empty, leading to unnecessary overhead) and too rarely.</p> <p>Additional options are available for the updater module, but they are not required. They are used to configure when cache management runs and the maximum number of entries in the cache. These are the options with their default values:</p> <pre><code>cache_management_cron: { hour: \"2\", minute: \"0\", second: \"0\" }  # (1)!\ncache_max_entries: 32  # (2)!\n</code></pre> <ol> <li><code>cache_management_cron</code> - a cron schedule for the cache management. See CronExpression docs for details.</li> <li><code>cache_max_entries</code> - the maximum number of entries in the cache. If the number of cache items (counted for each individual period) exceeds this number, the oldest entries are removed.</li> </ol>"},{"location":"reference/","title":"dp3","text":""},{"location":"reference/#dp3","title":"dp3","text":""},{"location":"reference/#dp3--dynamic-profile-processing-platform-dp3","title":"Dynamic Profile Processing Platform (DP\u00b3)","text":"<p>Platform directory structure:</p> <ul> <li> <p>Worker - The main worker process.</p> </li> <li> <p>API - The HTTP API implementation.</p> </li> <li> <p>Binaries under <code>dp3.bin</code> - Executables for running the platform.</p> </li> <li> <p>Common - Common modules which are used throughout the platform.</p> <ul> <li>Config, EntitySpec and AttrSpec - Models for reading, validation and representing platform configuration of entities and their attributes. datatype is also used within this context.</li> <li>Scheduler - Allows modules to run callbacks at specified times</li> <li>Task - Model for a single task processed by the platform</li> <li>Utils - Auxiliary utility functions</li> </ul> </li> <li> <p>Database.EntityDatabase - A wrapper responsible for communication with the database server.</p> </li> <li> <p>HistoryManagement.HistoryManager - Module responsible for managing history saved in database, currently to clean old data.</p> </li> <li> <p>Snapshots - SnapShooter, a module responsible for snapshot creation and running configured data correlation and fusion hooks, and Snapshot Hooks, which manage the registered hooks and their dependencies on one another.</p> </li> <li> <p>TaskProcessing - Module responsible for task distribution, processing and running configured hooks. Task distribution is possible due to the task queue.</p> </li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>dp3<ul> <li>api<ul> <li>internal<ul> <li>config</li> <li>dp_logger</li> <li>entity_response_models</li> <li>helpers</li> <li>models</li> <li>response_models</li> </ul> </li> <li>main</li> <li>routers<ul> <li>control</li> <li>entity</li> <li>root</li> <li>telemetry</li> </ul> </li> </ul> </li> <li>bin<ul> <li>api</li> <li>check</li> <li>cli</li> <li>config</li> <li>schema_update</li> <li>setup</li> <li>worker</li> </ul> </li> <li>common<ul> <li>attrspec</li> <li>base_module</li> <li>callback_registrar</li> <li>config</li> <li>context</li> <li>control</li> <li>datapoint</li> <li>datatype</li> <li>entityspec</li> <li>mac_address</li> <li>scheduler</li> <li>state</li> <li>task</li> <li>types</li> <li>utils</li> </ul> </li> <li>core<ul> <li>collector</li> <li>link_manager</li> <li>updater</li> </ul> </li> <li>database<ul> <li>config</li> <li>database</li> <li>encodings</li> <li>exceptions</li> <li>magic</li> <li>schema_cleaner</li> <li>snapshots</li> </ul> </li> <li>history_management<ul> <li>history_manager</li> <li>telemetry</li> </ul> </li> <li>scripts<ul> <li>add_hashes</li> <li>add_min_t2s</li> <li>datapoint_log_converter</li> <li>dummy_sender</li> <li>migrate_snapshots</li> <li>script_runner</li> </ul> </li> <li>snapshots<ul> <li>snapshooter</li> <li>snapshot_hooks</li> </ul> </li> <li>task_processing<ul> <li>task_distributor</li> <li>task_executor</li> <li>task_hooks</li> <li>task_queue</li> </ul> </li> <li>worker</li> </ul> </li> </ul>"},{"location":"reference/worker/","title":"worker","text":""},{"location":"reference/worker/#dp3.worker","title":"dp3.worker","text":"<p>Code of the main worker process.</p> <p>Don't run directly. Import and run the main() function.</p>"},{"location":"reference/worker/#dp3.worker.load_modules","title":"load_modules","text":"<pre><code>load_modules(modules_dir: str, enabled_modules: str, log: Logger, registrar: CallbackRegistrar, platform_config: PlatformConfig) -&gt; dict[str, BaseModule]\n</code></pre> <p>Load plug-in modules</p> <p>Import Python modules with names in 'enabled_modules' from 'modules_dir' directory and return all found classes derived from BaseModule class.</p> Source code in <code>dp3/worker.py</code> <pre><code>def load_modules(\n    modules_dir: str,\n    enabled_modules: str,\n    log: logging.Logger,\n    registrar: CallbackRegistrar,\n    platform_config: PlatformConfig,\n) -&gt; dict[str, BaseModule]:\n    \"\"\"Load plug-in modules\n\n    Import Python modules with names in 'enabled_modules' from 'modules_dir' directory\n    and return all found classes derived from BaseModule class.\n    \"\"\"\n    # Get list of all modules available in given folder\n    # [:-3] is for removing '.py' suffix from module filenames\n    available_modules = []\n    for item in os.scandir(modules_dir):\n        # A module can be a Python file or a Python package\n        # (i.e. a directory with \"__init__.py\" file)\n        if item.is_file() and item.name.endswith(\".py\"):\n            available_modules.append(item.name[:-3])  # name without .py\n        if item.is_dir() and \"__init__.py\" in os.listdir(os.path.join(modules_dir, item.name)):\n            available_modules.append(item.name)\n\n    log.debug(f\"Available modules: {', '.join(available_modules)}\")\n    log.debug(f\"Enabled modules: {', '.join(enabled_modules)}\")\n\n    # Check if all desired modules are in modules folder\n    missing_modules = set(enabled_modules) - set(available_modules)\n    if missing_modules:\n        log.fatal(\n            \"Some of desired modules are not available (not in modules folder), \"\n            f\"specifically: {missing_modules}\"\n        )\n        sys.exit(2)\n\n    # Do imports of desired modules from 'modules' folder\n    # (rewrite sys.path to modules_dir, import all modules and rewrite it back)\n    log.debug(\"Importing modules ...\")\n    sys.path.insert(0, modules_dir)\n    imported_modules: list[tuple[str, str, type[BaseModule]]] = [\n        (module_name, name, obj)\n        for module_name in enabled_modules\n        for name, obj in inspect.getmembers(import_module(module_name))\n        if inspect.isclass(obj) and BaseModule in obj.__bases__\n    ]\n    del sys.path[0]\n\n    # Loaded modules dict will contain main classes from all desired modules,\n    # which has BaseModule as parent\n    modules_main_objects = {}\n    for module_name, _, obj in imported_modules:\n        # Append instance of module class (obj is class --&gt; obj() is instance)\n        # --&gt; call init, which registers handler\n        module_config = platform_config.config.get(f\"modules.{module_name}\", {})\n        modules_main_objects[module_name] = obj(platform_config, module_config, registrar)\n        log.info(f\"Module loaded: {module_name}:{obj.__name__}\")\n\n    return modules_main_objects\n</code></pre>"},{"location":"reference/worker/#dp3.worker.main","title":"main","text":"<pre><code>main(app_name: str, config_dir: str, process_index: int, verbose: bool) -&gt; None\n</code></pre> <p>Run worker process. Args:     app_name: Name of the application to distinct it from other DP3-based apps.         For example, it's used as a prefix for RabbitMQ queue names.     config_dir: Path to directory containing configuration files.     process_index: Index of this worker process. For each application         there must be N processes running simultaneously, each started with a         unique index (from 0 to N-1). N is read from configuration         ('worker_processes' in 'processing_core.yml').     verbose: More verbose output (set log level to DEBUG).</p> Source code in <code>dp3/worker.py</code> <pre><code>def main(app_name: str, config_dir: str, process_index: int, verbose: bool) -&gt; None:\n    \"\"\"\n    Run worker process.\n    Args:\n        app_name: Name of the application to distinct it from other DP3-based apps.\n            For example, it's used as a prefix for RabbitMQ queue names.\n        config_dir: Path to directory containing configuration files.\n        process_index: Index of this worker process. For each application\n            there must be N processes running simultaneously, each started with a\n            unique index (from 0 to N-1). N is read from configuration\n            ('worker_processes' in 'processing_core.yml').\n        verbose: More verbose output (set log level to DEBUG).\n    \"\"\"\n    ##############################################\n    # Initialize logging mechanism\n    threading.current_thread().name = f\"MainThread-{process_index}\"\n    LOGFORMAT = \"%(asctime)-15s,%(threadName)s,%(name)s,[%(levelname)s] %(message)s\"\n    LOGDATEFORMAT = \"%Y-%m-%dT%H:%M:%S\"\n\n    logging.basicConfig(\n        level=logging.DEBUG if verbose else logging.INFO, format=LOGFORMAT, datefmt=LOGDATEFORMAT\n    )\n    log = logging.getLogger()\n\n    # Disable INFO and DEBUG messages from some libraries\n    logging.getLogger(\"requests\").setLevel(logging.WARNING)\n    logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n    logging.getLogger(\"amqpstorm\").setLevel(logging.WARNING)\n\n    ##############################################\n    # Load configuration\n    config_base_path = os.path.abspath(config_dir)\n    log.debug(f\"Loading config directory {config_base_path}\")\n\n    # Whole configuration should be loaded\n    config = read_config_dir(config_base_path, recursive=True)\n    try:\n        model_spec = ModelSpec(config.get(\"db_entities\"))\n    except ValidationError as e:\n        log.fatal(\"Invalid model specification: %s\", e)\n        sys.exit(2)\n\n    # Print whole attribute specification\n    log.debug(model_spec)\n\n    num_processes = config.get(\"processing_core.worker_processes\")\n\n    platform_config = PlatformConfig(\n        app_name=app_name,\n        config_base_path=config_base_path,\n        config=config,\n        model_spec=model_spec,\n        process_index=process_index,\n        num_processes=num_processes,\n    )\n    ##############################################\n    # Create instances of core components\n    log.info(f\"***** {app_name} worker {process_index} of {num_processes} start *****\")\n\n    # EventCountLogger\n    ecl = EventCountLogger(\n        platform_config.config.get(\"event_logging.groups\"),\n        platform_config.config.get(\"event_logging.redis\"),\n    )\n    elog = ecl.get_group(\"te\") or DummyEventGroup()\n    elog_by_src = ecl.get_group(\"tasks_by_src\") or DummyEventGroup()\n\n    db = EntityDatabase(config, model_spec, num_processes, process_index, elog)\n    if process_index == 0:\n        db.update_schema()\n    else:\n        db.await_updated_schema()\n\n    global_scheduler = scheduler.Scheduler()\n    task_executor = TaskExecutor(db, platform_config, elog, elog_by_src)\n    snap_shooter = SnapShooter(\n        db,\n        TaskQueueWriter(app_name, num_processes, config.get(\"processing_core.msg_broker\")),\n        platform_config,\n        global_scheduler,\n        elog,\n    )\n    updater = Updater(\n        db,\n        TaskQueueWriter(app_name, num_processes, config.get(\"processing_core.msg_broker\")),\n        platform_config,\n        global_scheduler,\n        elog,\n    )\n    registrar = CallbackRegistrar(global_scheduler, task_executor, snap_shooter, updater)\n\n    LinkManager(db, platform_config, registrar)\n    HistoryManager(db, platform_config, registrar)\n    Telemetry(db, platform_config, registrar)\n    GarbageCollector(db, platform_config, registrar)\n\n    # Lock used to control when the program stops.\n    daemon_stop_lock = threading.Lock()\n    daemon_stop_lock.acquire()\n\n    # Signal handler releasing the lock on SIGINT or SIGTERM\n    def sigint_handler(signum, frame):\n        log.debug(\n            \"Signal {} received, stopping worker\".format(\n                {signal.SIGINT: \"SIGINT\", signal.SIGTERM: \"SIGTERM\"}.get(signum, signum)\n            )\n        )\n        daemon_stop_lock.release()\n\n    signal.signal(signal.SIGINT, sigint_handler)\n    signal.signal(signal.SIGTERM, sigint_handler)\n    signal.signal(signal.SIGABRT, sigint_handler)\n\n    task_distributor = TaskDistributor(task_executor, platform_config, registrar, daemon_stop_lock)\n\n    control = Control(platform_config)\n    control.set_action_handler(ControlAction.make_snapshots, snap_shooter.make_snapshots)\n    control.set_action_handler(\n        ControlAction.refresh_on_entity_creation,\n        partial(refresh_on_entity_creation, task_distributor, task_executor),\n    )\n    modules = {}\n    control.set_action_handler(\n        ControlAction.refresh_module_config,\n        partial(reload_module_config, log, platform_config, modules),\n    )\n    global_scheduler.register(control.control_queue.watchdog, second=\"15,45\")\n\n    ##############################################\n    # Load all plug-in modules\n\n    module_dir = config.get(\"processing_core.modules_dir\")\n    module_dir = os.path.abspath(os.path.join(config_base_path, module_dir))\n\n    loaded_modules = load_modules(\n        module_dir,\n        config.get(\"processing_core.enabled_modules\"),\n        log,\n        registrar,\n        platform_config,\n    )\n    modules.update(loaded_modules)\n\n    ################################################\n    # Initialization completed, run ...\n\n    # Run update manager thread\n    log.info(\"***** Initialization completed, starting all modules *****\")\n\n    # Run modules that have their own threads (TODO: there are no such modules, should be kept?)\n    # (if they don't, the start() should do nothing)\n    for module in loaded_modules.values():\n        module.start()\n\n    core_modules = [\n        updater,  # Updater will throw exceptions when misconfigured (best start first)\n        task_distributor,  # TaskDistributor (which starts TaskExecutors in several worker threads)\n        db,\n        snap_shooter,\n        control,\n        global_scheduler,\n    ]\n    running_core_modules = []\n\n    try:\n        for module in core_modules:\n            module.start()\n            running_core_modules.append(module)\n\n        # Wait until someone wants to stop the program by releasing this Lock.\n        # It may be a user by pressing Ctrl-C or some program module.\n        # (try to acquire the lock again,\n        # effectively waiting until it's released by signal handler or another thread)\n        if os.name == \"nt\":\n            # This is needed on Windows in order to catch Ctrl-C, which doesn't break the waiting.\n            while not daemon_stop_lock.acquire(timeout=1):\n                pass\n        else:\n            daemon_stop_lock.acquire()\n    except Exception as e:\n        log.exception(e)\n\n    ################################################\n    # Finalization &amp; cleanup\n    # Set signal handlers back to their defaults,\n    # so the second Ctrl-C closes the program immediately\n    signal.signal(signal.SIGINT, signal.SIG_DFL)\n    signal.signal(signal.SIGTERM, signal.SIG_DFL)\n    signal.signal(signal.SIGABRT, signal.SIG_DFL)\n\n    log.info(\"Stopping running components ...\")\n    for module in reversed(running_core_modules):\n        module.stop()\n\n    for module in loaded_modules.values():\n        module.stop()\n\n    log.info(\"***** Finished, main thread exiting. *****\")\n    logging.shutdown()\n</code></pre>"},{"location":"reference/api/","title":"api","text":""},{"location":"reference/api/#dp3.api","title":"dp3.api","text":"<p>Platfrom HTTP API implementation.</p> <p>See the individual routers to see the API endpoint implementation.</p> <ul> <li><code>root</code> - catch-all router for the root path <code>/*</code>.   Implements the datapoint ingestion endpoint.</li> <li><code>entity</code> - implements the entity endpoints, <code>/{entity_type}/*</code>.</li> <li><code>control</code> - implements the control endpoints, <code>/{control_action}</code>.</li> </ul>"},{"location":"reference/api/main/","title":"main","text":""},{"location":"reference/api/main/#dp3.api.main","title":"dp3.api.main","text":""},{"location":"reference/api/internal/","title":"internal","text":""},{"location":"reference/api/internal/#dp3.api.internal","title":"dp3.api.internal","text":""},{"location":"reference/api/internal/config/","title":"config","text":""},{"location":"reference/api/internal/config/#dp3.api.internal.config","title":"dp3.api.internal.config","text":""},{"location":"reference/api/internal/config/#dp3.api.internal.config.ConfigEnv","title":"ConfigEnv","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration environment variables container</p>"},{"location":"reference/api/internal/dp_logger/","title":"dp_logger","text":""},{"location":"reference/api/internal/dp_logger/#dp3.api.internal.dp_logger","title":"dp3.api.internal.dp_logger","text":""},{"location":"reference/api/internal/dp_logger/#dp3.api.internal.dp_logger.DPLogger","title":"DPLogger","text":"<pre><code>DPLogger(config: dict)\n</code></pre> <p>Datapoint logger</p> <p>Logs good/bad datapoints into file for further analysis. They are logged in JSON format. Bad datapoints are logged together with their error message.</p> <p>Logging may be disabled in <code>api.yml</code> configuration file:</p> <pre><code># ...\ndatapoint_logger:\n  good_log: false\n  bad_log: false\n# ...\n</code></pre> Source code in <code>dp3/api/internal/dp_logger.py</code> <pre><code>def __init__(self, config: dict):\n    if not config:\n        config = {}\n\n    good_log_file = config.get(\"good_log\", False)\n    bad_log_file = config.get(\"bad_log\", False)\n\n    # Setup loggers\n    self._good_logger = self.setup_logger(\"GOOD\", good_log_file)\n    self._bad_logger = self.setup_logger(\"BAD\", bad_log_file)\n</code></pre>"},{"location":"reference/api/internal/dp_logger/#dp3.api.internal.dp_logger.DPLogger.setup_logger","title":"setup_logger","text":"<pre><code>setup_logger(name: str, log_file: str)\n</code></pre> <p>Creates new logger instance with <code>log_file</code> as target</p> Source code in <code>dp3/api/internal/dp_logger.py</code> <pre><code>def setup_logger(self, name: str, log_file: str):\n    \"\"\"Creates new logger instance with `log_file` as target\"\"\"\n    # Create log handler\n    if log_file:\n        parent_path = pathlib.Path(log_file).parent\n        if not parent_path.exists():\n            raise FileNotFoundError(\n                f\"The directory {parent_path} does not exist,\"\n                \" check the configured path or create the directory.\"\n            )\n        log_handler = logging.FileHandler(log_file)\n        log_handler.setFormatter(self.LOG_FORMATTER)\n    else:\n        log_handler = logging.NullHandler()\n\n    # Get logger instance\n    logger = logging.getLogger(name)\n    logger.addHandler(log_handler)\n    logger.setLevel(logging.INFO)\n    logger.propagate = False\n\n    return logger\n</code></pre>"},{"location":"reference/api/internal/dp_logger/#dp3.api.internal.dp_logger.DPLogger.log_good","title":"log_good","text":"<pre><code>log_good(dps: list[DataPointBase], src: str = UNKNOWN_SRC_MSG)\n</code></pre> <p>Logs good datapoints</p> <p>Datapoints are logged one-by-one in processed form. Source should be IP address of incomping request.</p> Source code in <code>dp3/api/internal/dp_logger.py</code> <pre><code>def log_good(self, dps: list[DataPointBase], src: str = UNKNOWN_SRC_MSG):\n    \"\"\"Logs good datapoints\n\n    Datapoints are logged one-by-one in processed form.\n    Source should be IP address of incomping request.\n    \"\"\"\n    for dp in dps:\n        self._good_logger.info(dp.model_dump(), extra={\"src\": src})\n</code></pre>"},{"location":"reference/api/internal/dp_logger/#dp3.api.internal.dp_logger.DPLogger.log_bad","title":"log_bad","text":"<pre><code>log_bad(validation_error_msg: str, src: str = UNKNOWN_SRC_MSG)\n</code></pre> <p>Logs validation error message, which includes bad input datapoints</p> <p>Should be called for each individual error (JSON string is expected). Source should be IP address of incoming request.</p> Source code in <code>dp3/api/internal/dp_logger.py</code> <pre><code>def log_bad(self, validation_error_msg: str, src: str = UNKNOWN_SRC_MSG):\n    \"\"\"Logs validation error message, which includes bad input datapoints\n\n    Should be called for each individual error (JSON string is expected).\n    Source should be IP address of incoming request.\n    \"\"\"\n    self._bad_logger.info(validation_error_msg, extra={\"src\": src})\n</code></pre>"},{"location":"reference/api/internal/entity_response_models/","title":"entity_response_models","text":""},{"location":"reference/api/internal/entity_response_models/#dp3.api.internal.entity_response_models","title":"dp3.api.internal.entity_response_models","text":""},{"location":"reference/api/internal/entity_response_models/#dp3.api.internal.entity_response_models.EntityState","title":"EntityState","text":"<p>               Bases: <code>BaseModel</code></p> <p>Entity specification and current state</p> <p>Merges (some) data from DP3's <code>EntitySpec</code> and state information from <code>Database</code>. Provides estimate count of master records in database.</p>"},{"location":"reference/api/internal/entity_response_models/#dp3.api.internal.entity_response_models.EntityEidList","title":"EntityEidList","text":"<p>               Bases: <code>BaseModel</code></p> <p>List of entity eids and their data based on latest snapshot</p> <p>Includes timestamp of latest snapshot creation, count of returned documents and total count of documents available under specified filter.</p> <p>Data does not include history of observations attributes and timeseries.</p>"},{"location":"reference/api/internal/entity_response_models/#dp3.api.internal.entity_response_models.EntityEidCount","title":"EntityEidCount","text":"<p>               Bases: <code>BaseModel</code></p> <p>Total count of documents available under specified filter.</p>"},{"location":"reference/api/internal/entity_response_models/#dp3.api.internal.entity_response_models.EntityEidData","title":"EntityEidData","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data of entity eid</p> <p>Includes all snapshots and master record.</p> <p><code>empty</code> signalizes whether this eid includes any data.</p>"},{"location":"reference/api/internal/entity_response_models/#dp3.api.internal.entity_response_models.EntityEidAttrValueOrHistory","title":"EntityEidAttrValueOrHistory","text":"<p>               Bases: <code>BaseModel</code></p> <p>Value and/or history of entity attribute for given eid</p> <p>Depends on attribute type: - plain: just (current) value - observations: (current) value and history stored in master record (optionally filtered) - timeseries: just history stored in master record (optionally filtered)</p>"},{"location":"reference/api/internal/entity_response_models/#dp3.api.internal.entity_response_models.EntityEidAttrValue","title":"EntityEidAttrValue","text":"<p>               Bases: <code>BaseModel</code></p> <p>Value of entity attribute for given eid</p> <p>The value is fetched from master record.</p>"},{"location":"reference/api/internal/helpers/","title":"helpers","text":""},{"location":"reference/api/internal/helpers/#dp3.api.internal.helpers","title":"dp3.api.internal.helpers","text":""},{"location":"reference/api/internal/helpers/#dp3.api.internal.helpers.api_to_dp3_datapoint","title":"api_to_dp3_datapoint","text":"<pre><code>api_to_dp3_datapoint(api_dp_values: dict) -&gt; DataPointBase\n</code></pre> <p>Converts API datapoint values to DP3 datapoint</p> <p>If etype-attr pair doesn't exist in DP3 config, raises <code>ValueError</code>. If values are not valid, raises pydantic's ValidationError.</p> Source code in <code>dp3/api/internal/helpers.py</code> <pre><code>def api_to_dp3_datapoint(api_dp_values: dict) -&gt; DataPointBase:\n    \"\"\"Converts API datapoint values to DP3 datapoint\n\n    If etype-attr pair doesn't exist in DP3 config, raises `ValueError`.\n    If values are not valid, raises pydantic's ValidationError.\n    \"\"\"\n    etype = api_dp_values[\"type\"]\n    attr = api_dp_values[\"attr\"]\n\n    # Convert to DP3 datapoint format\n    dp3_dp_values = api_dp_values\n    dp3_dp_values[\"etype\"] = etype\n    dp3_dp_values[\"eid\"] = api_dp_values[\"id\"]\n\n    # Get attribute-specific model\n    try:\n        model = MODEL_SPEC.attr(etype, attr).dp_model\n    except KeyError as e:\n        raise ValueError(f\"Combination of type '{etype}' and attr '{attr}' doesn't exist\") from e\n\n    # Parse using the model\n    # This may raise pydantic's ValidationError, but that's intensional (to get\n    # a JSON-serializable trace as a response from API).\n    return model.model_validate(dp3_dp_values)\n</code></pre>"},{"location":"reference/api/internal/models/","title":"models","text":""},{"location":"reference/api/internal/models/#dp3.api.internal.models","title":"dp3.api.internal.models","text":""},{"location":"reference/api/internal/models/#dp3.api.internal.models.EntityId","title":"EntityId  <code>module-attribute</code>","text":"<pre><code>EntityId = Annotated[Union[tuple(entity_id_models)], Field(discriminator='type')]\n</code></pre> <p>Dummy model for entity id</p> <p>Attributes:</p> Name Type Description <code>type</code> <p>Entity type</p> <code>id</code> <p>Entity ID</p>"},{"location":"reference/api/internal/models/#dp3.api.internal.models.DataPoint","title":"DataPoint","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data-point for API</p> <p>Contains single raw data value received on API. This is generic class for plain, observation and timeseries datapoints.</p> <p>Provides front line of validation for this data value.</p> <p>This differs slightly compared to <code>DataPoint</code> from DP3 in naming of attributes due to historic reasons.</p> <p>After validation of this schema, datapoint is validated using attribute-specific validator to ensure full compilance.</p>"},{"location":"reference/api/internal/response_models/","title":"response_models","text":""},{"location":"reference/api/internal/response_models/#dp3.api.internal.response_models","title":"dp3.api.internal.response_models","text":""},{"location":"reference/api/internal/response_models/#dp3.api.internal.response_models.HealthCheckResponse","title":"HealthCheckResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Healthcheck endpoint response</p>"},{"location":"reference/api/internal/response_models/#dp3.api.internal.response_models.SuccessResponse","title":"SuccessResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Generic success response</p>"},{"location":"reference/api/internal/response_models/#dp3.api.internal.response_models.ErrorResponse","title":"ErrorResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Generic error response</p>"},{"location":"reference/api/internal/response_models/#dp3.api.internal.response_models.RequestValidationError","title":"RequestValidationError","text":"<pre><code>RequestValidationError(loc, msg)\n</code></pre> <p>               Bases: <code>HTTPException</code></p> <p>HTTP exception wrapper to simplify path and query validation</p> Source code in <code>dp3/api/internal/response_models.py</code> <pre><code>def __init__(self, loc, msg):\n    super().__init__(422, [{\"loc\": loc, \"msg\": msg, \"type\": \"value_error\"}])\n</code></pre>"},{"location":"reference/api/routers/","title":"routers","text":""},{"location":"reference/api/routers/#dp3.api.routers","title":"dp3.api.routers","text":""},{"location":"reference/api/routers/control/","title":"control","text":""},{"location":"reference/api/routers/control/#dp3.api.routers.control","title":"dp3.api.routers.control","text":""},{"location":"reference/api/routers/control/#dp3.api.routers.control.refresh_on_entity_creation","title":"refresh_on_entity_creation  <code>async</code>","text":"<pre><code>refresh_on_entity_creation(etype: str) -&gt; SuccessResponse\n</code></pre> <p>Sends the action <code>refresh_on_entity_creation</code> into execution queue.</p> <p>This action is only accepted with the <code>etype</code> parameter.</p> Source code in <code>dp3/api/routers/control.py</code> <pre><code>@router.get(\"/refresh_on_entity_creation\")\nasync def refresh_on_entity_creation(etype: str) -&gt; SuccessResponse:\n    \"\"\"Sends the action `refresh_on_entity_creation` into execution queue.\n\n    This action is only accepted with the `etype` parameter.\n    \"\"\"\n    CONTROL_WRITER.broadcast_task(\n        ControlMessage(action=ControlAction.refresh_on_entity_creation, kwargs={\"etype\": etype})\n    )\n    return SuccessResponse(detail=\"Action sent.\")\n</code></pre>"},{"location":"reference/api/routers/control/#dp3.api.routers.control.refresh_module_config","title":"refresh_module_config  <code>async</code>","text":"<pre><code>refresh_module_config(module: EnabledModules) -&gt; SuccessResponse\n</code></pre> <p>Sends the action <code>refresh_module_config</code> into execution queue.</p> <p>This action is only accepted with the <code>module</code> parameter.</p> Source code in <code>dp3/api/routers/control.py</code> <pre><code>@router.get(\"/refresh_module_config\")\nasync def refresh_module_config(module: EnabledModules) -&gt; SuccessResponse:\n    \"\"\"Sends the action `refresh_module_config` into execution queue.\n\n    This action is only accepted with the `module` parameter.\n    \"\"\"\n    CONTROL_WRITER.broadcast_task(\n        ControlMessage(action=ControlAction.refresh_module_config, kwargs={\"module\": module.value})\n    )\n    return SuccessResponse(detail=\"Action sent.\")\n</code></pre>"},{"location":"reference/api/routers/control/#dp3.api.routers.control.execute_action","title":"execute_action  <code>async</code>","text":"<pre><code>execute_action(action: ControlAction) -&gt; SuccessResponse\n</code></pre> <p>Sends the given action into execution queue.</p> Source code in <code>dp3/api/routers/control.py</code> <pre><code>@router.get(\"/{action}\")\nasync def execute_action(action: ControlAction) -&gt; SuccessResponse:\n    \"\"\"Sends the given action into execution queue.\"\"\"\n    CONTROL_WRITER.put_task(ControlMessage(action=action))\n    return SuccessResponse(detail=\"Action sent.\")\n</code></pre>"},{"location":"reference/api/routers/entity/","title":"entity","text":""},{"location":"reference/api/routers/entity/#dp3.api.routers.entity","title":"dp3.api.routers.entity","text":""},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.check_etype","title":"check_etype  <code>async</code>","text":"<pre><code>check_etype(etype: str)\n</code></pre> <p>Middleware to check entity type existence</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>async def check_etype(etype: str):\n    \"\"\"Middleware to check entity type existence\"\"\"\n    if etype not in MODEL_SPEC.entities:\n        raise RequestValidationError([\"path\", \"etype\"], f\"Entity type '{etype}' doesn't exist\")\n    return etype\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.parse_eid","title":"parse_eid  <code>async</code>","text":"<pre><code>parse_eid(etype: str, eid: str)\n</code></pre> <p>Middleware to parse EID</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>async def parse_eid(etype: str, eid: str):\n    \"\"\"Middleware to parse EID\"\"\"\n    try:\n        return EntityIdAdapter.validate_python({\"etype\": etype, \"eid\": eid})\n    except ValidationError as e:\n        raise RequestValidationError([\"path\", \"eid\"], e.errors()[0][\"msg\"]) from e\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.get_eid_master_record_handler","title":"get_eid_master_record_handler","text":"<pre><code>get_eid_master_record_handler(e: EntityId, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None)\n</code></pre> <p>Handler for getting master record of EID</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>def get_eid_master_record_handler(\n    e: EntityId, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None\n):\n    \"\"\"Handler for getting master record of EID\"\"\"\n    # TODO: This is probably not the most efficient way. Maybe gather only\n    # plain data from master record and then call `get_timeseries_history`\n    # for timeseries.\n    master_record = DB.get_master_record(\n        e.type, e.id, projection={\"_id\": False, \"#hash\": False, \"#min_t2s\": False}\n    )\n\n    entity_attribs = MODEL_SPEC.attribs(e.type)\n\n    # Get filtered timeseries data\n    for attr in master_record:\n        # Check for no longer existing attributes\n        if attr in entity_attribs and entity_attribs[attr].t == AttrType.TIMESERIES:\n            master_record[attr] = DB.get_timeseries_history(\n                e.type, attr, e.id, t1=date_from, t2=date_to\n            )\n\n    return master_record\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.get_eid_snapshots_handler","title":"get_eid_snapshots_handler","text":"<pre><code>get_eid_snapshots_handler(e: EntityId, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None) -&gt; list[dict[str, Any]]\n</code></pre> <p>Handler for getting snapshots of EID</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>def get_eid_snapshots_handler(\n    e: EntityId, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Handler for getting snapshots of EID\"\"\"\n    snapshots = list(DB.snapshots.get_by_eid(e.type, e.id, t1=date_from, t2=date_to))\n\n    return snapshots\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.list_entity_type_eids","title":"list_entity_type_eids  <code>async</code>","text":"<pre><code>list_entity_type_eids(etype: str, fulltext_filters: Json = None, generic_filter: Json = None, skip: NonNegativeInt = 0, limit: NonNegativeInt = 20) -&gt; EntityEidList\n</code></pre> <p>List latest snapshots of all <code>id</code>s present in database under <code>etype</code>.</p> <p>Deprecated in favor of <code>/entity/{etype}/get</code> and <code>/entity/{etype}/count</code> endpoints, which provide more flexibility and better performance.</p> <p>See <code>/entity/{etype}/get</code> for more information.</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.get(\n    \"/{etype}\",\n    responses={400: {\"description\": \"Query can't be processed\", \"model\": ErrorResponse}},\n    deprecated=True,\n)\nasync def list_entity_type_eids(\n    etype: str,\n    fulltext_filters: Json = None,\n    generic_filter: Json = None,\n    skip: NonNegativeInt = 0,\n    limit: NonNegativeInt = 20,\n) -&gt; EntityEidList:\n    \"\"\"List latest snapshots of all `id`s present in database under `etype`.\n\n    Deprecated in favor of `/entity/{etype}/get` and `/entity/{etype}/count` endpoints,\n    which provide more flexibility and better performance.\n\n    See `/entity/{etype}/get` for more information.\n    \"\"\"\n    fulltext_filters, generic_filter = _validate_snapshot_filters(fulltext_filters, generic_filter)\n\n    try:\n        cursor, total_count = DB.snapshots.get_latest(etype, fulltext_filters, generic_filter)\n        cursor_page = cursor.skip(skip).limit(limit)\n    except DatabaseError as e:\n        raise HTTPException(status_code=400, detail=str(e)) from e\n\n    time_created = None\n\n    # Remove _id field\n    result = [r[\"last\"] for r in cursor_page]\n    for r in result:\n        time_created = r[\"_time_created\"]\n        del r[\"_time_created\"]\n\n    return EntityEidList(\n        time_created=time_created, count=len(result), total_count=total_count, data=result\n    )\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.get_entity_type_eids","title":"get_entity_type_eids  <code>async</code>","text":"<pre><code>get_entity_type_eids(etype: str, fulltext_filters: Json = None, generic_filter: Json = None, skip: NonNegativeInt = 0, limit: NonNegativeInt = 20) -&gt; EntityEidList\n</code></pre> <p>List latest snapshots of all <code>id</code>s present in database under <code>etype</code>.</p> <p>Contains only latest snapshot. The <code>total_count</code> returned is always 0, use <code>/entity/{etype}/count</code> to get total count.</p> <p>Uses pagination. Setting <code>limit</code> to 0 is interpreted as no limit (return all results).</p> <p>Returns only documents matching <code>generic_filter</code> and <code>fulltext_filters</code> (JSON object in format: attribute - fulltext filter). Fulltext filters are interpreted as regular expressions. Only string values may be filtered this way. There's no validation that queried attribute can be fulltext filtered. Only plain and observation attributes with string-based data types can be queried. Array and set data types are supported as well as long as they are not multi value at the same time. If you need to filter EIDs, use attribute fulltext_filters[\"eid\"].</p> <p>Generic filter allows filtering using generic MongoDB query (including <code>$and</code>, <code>$or</code>, <code>$lt</code>, etc.). For querying non-JSON-native types, you can use the following magic strings:</p> <ul> <li><code>\"$$IPv4{&lt;ip address&gt;}\"</code> - converts to IPv4Address object</li> <li><code>\"$$IPv6{&lt;ip address&gt;}\"</code> - converts to IPv6Address object</li> <li><code>\"$$int{&lt;value&gt;}\"</code> - may be necessary for filtering when <code>eid</code> data type is int</li> <li><code>\"$$Date{&lt;YYYY-mm-ddTHH:MM:ssZ&gt;}\"</code> - converts specified UTC date to UTC datetime object</li> <li><code>\"$$DateTs{&lt;POSIX timestamp&gt;}\"</code> - converts POSIX timestamp (int/float) to UTC datetime object</li> <li><code>\"$$MAC{&lt;mac address&gt;}\"</code> - converts to MACAddress object</li> </ul> <p>To query an IP prefix, use the following magic strings:</p> <ul> <li><code>\"$$IPv4Prefix{&lt;ip address&gt;/&lt;prefix length&gt;}\"</code> - matches prefix</li> <li><code>\"$$IPv6Prefix{&lt;ip address&gt;/&lt;prefix length&gt;}\"</code> - matches prefix</li> </ul> <p>To query a binary <code>_id</code>s of non-string snapshot buckets, use the following magic string:</p> <ul> <li> <p><code>\"$$Binary_ID{&lt;EID object | Valid magic string&gt;}\"</code></p> <ul> <li>converts to filter the exact EID object snapshots, only EID valid types are supported</li> </ul> </li> </ul> <p>There are no attribute name checks (may be added in the future).</p> <p>Generic filter examples:</p> <ul> <li><code>{\"attr1\": {\"$gt\": 10}, \"attr2\": {\"$lt\": 20}}</code></li> <li><code>{\"ip_attr\": \"$$IPv4{127.0.0.1}\"}</code> - converts to IPv4Address object, exact match</li> <li> <p><code>{\"ip_attr\": \"$$IPv4Prefix{127.0.0.0/24}\"}</code></p> <ul> <li>converts to <code>{\"ip_attr\": {\"$gte\": \"$$IPv4{127.0.0.0}\",   \"$lte\": \"$$IPv4{127.0.0.255}\"}}</code></li> </ul> </li> <li> <p><code>{\"ip_attr\": \"$$IPv6{::1}\"}</code> - converts to IPv6Address object, exact match</p> </li> <li> <p><code>{\"ip_attr\": \"$$IPv6Prefix{::1/64}\"}</code></p> <ul> <li>converts to <code>{\"ip_attr\": {\"$gte\": \"$$IPv6{::1}\",   \"$lte\": \"$$IPv6{::1:ffff:ffff:ffff:ffff}\"}}</code></li> </ul> </li> <li> <p><code>{\"_id\": \"$$Binary_ID{$$IPv4{127.0.0.1}}\"}</code></p> <ul> <li>converts to <code>{\"_id\": {\"$gte\": Binary(&lt;IP bytes + min timestamp&gt;),   \"$lt\": Binary(&lt;IP bytes + max timestamp&gt;)}}</code></li> </ul> </li> <li> <p><code>{\"id\": \"$$Binary_ID{$$IPv4Prefix{127.0.0.0/24}}\"}</code></p> <ul> <li>converts to <code>{\"_id\": {\"$gte\": Binary(&lt;IP 127.0.0.0 bytes + min timestamp&gt;),   \"$lte\": Binary(&lt;IP 127.0.0.255 bytes + max timestamp&gt;)}}</code></li> </ul> </li> <li> <p><code>{\"_time_created\": {\"$gte\": \"$$Date{2024-11-07T00:00:00Z}\"}}</code></p> <ul> <li>converts to <code>{\"_time_created\": datetime(2024, 11, 7, 0, 0, 0, tzinfo=timezone.utc)}</code></li> </ul> </li> <li> <p><code>{\"_time_created\": {\"$gte\": \"$$DateTs{1609459200}\"}}</code></p> <ul> <li>converts to <code>{\"_time_created\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc)}</code></li> </ul> </li> <li> <p><code>{\"attr\": \"$$MAC{00:11:22:33:44:55}\"}</code> - converts to MACAddress object, exact match</p> </li> <li><code>{\"_id\": \"$$Binary_ID{$$MAC{Ab-cD-Ef-12-34-56}}\"}</code><ul> <li>converts to <code>{\"_id\": {\"$gte\": Binary(&lt;MAC bytes + min timestamp&gt;),     \"$lt\": Binary(&lt;MAC bytes + max timestamp&gt;)}}</code></li> </ul> </li> </ul> <p>There are no attribute name checks (may be added in the future).</p> <p>Generic and fulltext filters are merged - fulltext overrides conflicting keys.</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.get(\n    \"/{etype}/get\",\n    responses={400: {\"description\": \"Query can't be processed\", \"model\": ErrorResponse}},\n)\nasync def get_entity_type_eids(\n    etype: str,\n    fulltext_filters: Json = None,\n    generic_filter: Json = None,\n    skip: NonNegativeInt = 0,\n    limit: NonNegativeInt = 20,\n) -&gt; EntityEidList:\n    \"\"\"List latest snapshots of all `id`s present in database under `etype`.\n\n    Contains only latest snapshot.\n    The `total_count` returned is always 0, use `/entity/{etype}/count` to get total count.\n\n    Uses pagination.\n    Setting `limit` to 0 is interpreted as no limit (return all results).\n\n    Returns only documents matching `generic_filter` and `fulltext_filters`\n    (JSON object in format: attribute - fulltext filter).\n    Fulltext filters are interpreted as regular expressions.\n    Only string values may be filtered this way. There's no validation that queried attribute\n    can be fulltext filtered.\n    Only plain and observation attributes with string-based data types can be queried.\n    Array and set data types are supported as well as long as they are not multi value\n    at the same time.\n    If you need to filter EIDs, use attribute fulltext_filters[\"eid\"].\n\n    Generic filter allows filtering using generic MongoDB query (including `$and`, `$or`,\n    `$lt`, etc.).\n    For querying non-JSON-native types, you can use the following magic strings:\n\n    - `\"$$IPv4{&lt;ip address&gt;}\"` - converts to IPv4Address object\n    - `\"$$IPv6{&lt;ip address&gt;}\"` - converts to IPv6Address object\n    - `\"$$int{&lt;value&gt;}\"` - may be necessary for filtering when `eid` data type is int\n    - `\"$$Date{&lt;YYYY-mm-ddTHH:MM:ssZ&gt;}\"` - converts specified UTC date to UTC datetime object\n    - `\"$$DateTs{&lt;POSIX timestamp&gt;}\"` - converts POSIX timestamp (int/float) to UTC datetime object\n    - `\"$$MAC{&lt;mac address&gt;}\"` - converts to MACAddress object\n\n    To query an IP prefix, use the following magic strings:\n\n    - `\"$$IPv4Prefix{&lt;ip address&gt;/&lt;prefix length&gt;}\"` - matches prefix\n    - `\"$$IPv6Prefix{&lt;ip address&gt;/&lt;prefix length&gt;}\"` - matches prefix\n\n    To query a binary `_id`s of non-string snapshot buckets,\n    use the following magic string:\n\n    - `\"$$Binary_ID{&lt;EID object | Valid magic string&gt;}\"`\n\n        - converts to filter the exact EID object snapshots, only EID valid types are supported\n\n    There are no attribute name checks (may be added in the future).\n\n    Generic filter examples:\n\n    - `{\"attr1\": {\"$gt\": 10}, \"attr2\": {\"$lt\": 20}}`\n    - `{\"ip_attr\": \"$$IPv4{127.0.0.1}\"}` - converts to IPv4Address object, exact match\n    - `{\"ip_attr\": \"$$IPv4Prefix{127.0.0.0/24}\"}`\n        - converts to `{\"ip_attr\": {\"$gte\": \"$$IPv4{127.0.0.0}\",\n          \"$lte\": \"$$IPv4{127.0.0.255}\"}}`\n\n    - `{\"ip_attr\": \"$$IPv6{::1}\"}` - converts to IPv6Address object, exact match\n    - `{\"ip_attr\": \"$$IPv6Prefix{::1/64}\"}`\n        - converts to `{\"ip_attr\": {\"$gte\": \"$$IPv6{::1}\",\n          \"$lte\": \"$$IPv6{::1:ffff:ffff:ffff:ffff}\"}}`\n\n    - `{\"_id\": \"$$Binary_ID{$$IPv4{127.0.0.1}}\"}`\n        - converts to `{\"_id\": {\"$gte\": Binary(&lt;IP bytes + min timestamp&gt;),\n          \"$lt\": Binary(&lt;IP bytes + max timestamp&gt;)}}`\n\n    - `{\"id\": \"$$Binary_ID{$$IPv4Prefix{127.0.0.0/24}}\"}`\n        - converts to `{\"_id\": {\"$gte\": Binary(&lt;IP 127.0.0.0 bytes + min timestamp&gt;),\n          \"$lte\": Binary(&lt;IP 127.0.0.255 bytes + max timestamp&gt;)}}`\n\n    - `{\"_time_created\": {\"$gte\": \"$$Date{2024-11-07T00:00:00Z}\"}}`\n        - converts to `{\"_time_created\": datetime(2024, 11, 7, 0, 0, 0, tzinfo=timezone.utc)}`\n\n    - `{\"_time_created\": {\"$gte\": \"$$DateTs{1609459200}\"}}`\n        - converts to `{\"_time_created\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc)}`\n\n    - `{\"attr\": \"$$MAC{00:11:22:33:44:55}\"}` - converts to MACAddress object, exact match\n    - `{\"_id\": \"$$Binary_ID{$$MAC{Ab-cD-Ef-12-34-56}}\"}`\n        - converts to `{\"_id\": {\"$gte\": Binary(&lt;MAC bytes + min timestamp&gt;),\n            \"$lt\": Binary(&lt;MAC bytes + max timestamp&gt;)}}`\n\n    There are no attribute name checks (may be added in the future).\n\n    Generic and fulltext filters are merged - fulltext overrides conflicting keys.\n    \"\"\"\n    fulltext_filters, generic_filter = _validate_snapshot_filters(fulltext_filters, generic_filter)\n\n    try:\n        cursor = DB.snapshots.find_latest(etype, fulltext_filters, generic_filter)\n        cursor_page = cursor.skip(skip).limit(limit)\n    except DatabaseError as e:\n        raise HTTPException(status_code=400, detail=str(e)) from e\n\n    time_created = None\n\n    # Remove _id field\n    result = [r[\"last\"] for r in cursor_page]\n    for r in result:\n        time_created = r[\"_time_created\"]\n        del r[\"_time_created\"]\n\n    return EntityEidList(time_created=time_created, count=len(result), total_count=0, data=result)\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.count_entity_type_eids","title":"count_entity_type_eids  <code>async</code>","text":"<pre><code>count_entity_type_eids(etype: str, fulltext_filters: Json = None, generic_filter: Json = None) -&gt; EntityEidCount\n</code></pre> <p>Count latest snapshots of all <code>id</code>s present in database under <code>etype</code>.</p> <p>Returns only count of documents matching <code>generic_filter</code> and <code>fulltext_filters</code>, see <code>/entity/{etype}/get</code> documentation for details.</p> <p>Note that responses from this endpoint may take much longer than <code>/entity/{etype}/get</code> for large datasets.</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.get(\n    \"/{etype}/count\",\n    responses={400: {\"description\": \"Query can't be processed\", \"model\": ErrorResponse}},\n)\nasync def count_entity_type_eids(\n    etype: str,\n    fulltext_filters: Json = None,\n    generic_filter: Json = None,\n) -&gt; EntityEidCount:\n    \"\"\"Count latest snapshots of all `id`s present in database under `etype`.\n\n    Returns only count of documents matching `generic_filter` and `fulltext_filters`,\n    see `/entity/{etype}/get` documentation for details.\n\n    Note that responses from this endpoint may take much longer than `/entity/{etype}/get`\n    for large datasets.\n    \"\"\"\n    fulltext_filters, generic_filter = _validate_snapshot_filters(fulltext_filters, generic_filter)\n\n    try:\n        count = DB.snapshots.count_latest(etype, fulltext_filters, generic_filter)\n    except DatabaseError as e:\n        raise HTTPException(status_code=400, detail=str(e)) from e\n\n    return EntityEidCount(total_count=count)\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.get_eid_data","title":"get_eid_data  <code>async</code>","text":"<pre><code>get_eid_data(e: ParsedEid, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None) -&gt; EntityEidData\n</code></pre> <p>Get data of <code>etype</code>'s <code>eid</code>.</p> <p>Contains all snapshots and master record. Snapshots are ordered by ascending creation time.</p> <p>Combines function of <code>/{etype}/{eid}/master</code> and <code>/{etype}/{eid}/snapshots</code>.</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.get(\"/{etype}/{eid}\")\nasync def get_eid_data(\n    e: ParsedEid, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None\n) -&gt; EntityEidData:\n    \"\"\"Get data of `etype`'s `eid`.\n\n    Contains all snapshots and master record.\n    Snapshots are ordered by ascending creation time.\n\n    Combines function of `/{etype}/{eid}/master` and `/{etype}/{eid}/snapshots`.\n    \"\"\"\n    master_record = get_eid_master_record_handler(e, date_from, date_to)\n    snapshots = get_eid_snapshots_handler(e, date_from, date_to)\n\n    # Whether this eid contains any data\n    empty = not master_record and len(snapshots) == 0\n\n    return EntityEidData(empty=empty, master_record=master_record, snapshots=snapshots)\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.get_eid_master_record","title":"get_eid_master_record  <code>async</code>","text":"<pre><code>get_eid_master_record(e: ParsedEid, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None) -&gt; EntityEidMasterRecord\n</code></pre> <p>Get master record of <code>etype</code>'s <code>eid</code>.</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.get(\"/{etype}/{eid}/master\")\nasync def get_eid_master_record(\n    e: ParsedEid, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None\n) -&gt; EntityEidMasterRecord:\n    \"\"\"Get master record of `etype`'s `eid`.\"\"\"\n    return get_eid_master_record_handler(e, date_from, date_to)\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.get_eid_snapshots","title":"get_eid_snapshots  <code>async</code>","text":"<pre><code>get_eid_snapshots(e: ParsedEid, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None) -&gt; EntityEidSnapshots\n</code></pre> <p>Get snapshots of <code>etype</code>'s <code>eid</code>.</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.get(\"/{etype}/{eid}/snapshots\")\nasync def get_eid_snapshots(\n    e: ParsedEid, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None\n) -&gt; EntityEidSnapshots:\n    \"\"\"Get snapshots of `etype`'s `eid`.\"\"\"\n    return get_eid_snapshots_handler(e, date_from, date_to)\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.get_eid_attr_value","title":"get_eid_attr_value  <code>async</code>","text":"<pre><code>get_eid_attr_value(e: ParsedEid, attr: str, date_from: Optional[datetime] = None, date_to: Optional[datetime] = None) -&gt; EntityEidAttrValueOrHistory\n</code></pre> <p>Get attribute value</p> <p>Value is either of: - current value: in case of plain attribute - current value and history: in case of observation attribute - history: in case of timeseries attribute</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.get(\"/{etype}/{eid}/get/{attr}\")\nasync def get_eid_attr_value(\n    e: ParsedEid,\n    attr: str,\n    date_from: Optional[datetime] = None,\n    date_to: Optional[datetime] = None,\n) -&gt; EntityEidAttrValueOrHistory:\n    \"\"\"Get attribute value\n\n    Value is either of:\n    - current value: in case of plain attribute\n    - current value and history: in case of observation attribute\n    - history: in case of timeseries attribute\n    \"\"\"\n    # Check if attribute exists\n    if attr not in MODEL_SPEC.attribs(e.type):\n        raise RequestValidationError([\"path\", \"attr\"], f\"Attribute '{attr}' doesn't exist\")\n\n    value_or_history = DB.get_value_or_history(e.type, attr, e.id, t1=date_from, t2=date_to)\n\n    return EntityEidAttrValueOrHistory(\n        attr_type=MODEL_SPEC.attr(e.type, attr).t, **value_or_history\n    )\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.set_eid_attr_value","title":"set_eid_attr_value  <code>async</code>","text":"<pre><code>set_eid_attr_value(etype: str, eid: str, attr: str, body: EntityEidAttrValue, request: Request) -&gt; SuccessResponse\n</code></pre> <p>Set current value of attribute</p> <p>Internally just creates datapoint for specified attribute and value.</p> <p>This endpoint is meant for <code>editable</code> plain attributes -- for direct user edit on DP3 web UI.</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.post(\"/{etype}/{eid}/set/{attr}\")\nasync def set_eid_attr_value(\n    etype: str, eid: str, attr: str, body: EntityEidAttrValue, request: Request\n) -&gt; SuccessResponse:\n    \"\"\"Set current value of attribute\n\n    Internally just creates datapoint for specified attribute and value.\n\n    This endpoint is meant for `editable` plain attributes -- for direct user edit on DP3 web UI.\n    \"\"\"\n    # Check if attribute exists\n    if attr not in MODEL_SPEC.attribs(etype):\n        raise RequestValidationError([\"path\", \"attr\"], f\"Attribute '{attr}' doesn't exist\")\n\n    # Construct datapoint\n    try:\n        dp = DataPoint(\n            type=etype,\n            id=eid,\n            attr=attr,\n            v=body.value,\n            t1=datetime.now(),\n            src=f\"{request.client.host} via API\",\n        )\n        dp3_dp = api_to_dp3_datapoint(dp.dict())\n    except ValidationError as e:\n        raise RequestValidationError([\"body\", \"value\"], e.errors()[0][\"msg\"]) from e\n\n    # This shouldn't fail\n    with task_context(MODEL_SPEC):\n        task = DataPointTask(etype=etype, eid=eid, data_points=[dp3_dp])\n\n    # Push tasks to task queue\n    TASK_WRITER.put_task(task, False)\n\n    # Datapoints from this endpoint are intentionally not logged using `DPLogger`.\n    # If for some reason, in the future, they need to be, just copy code from data ingestion\n    # endpoint.\n\n    return SuccessResponse()\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.get_distinct_attribute_values","title":"get_distinct_attribute_values  <code>async</code>","text":"<pre><code>get_distinct_attribute_values(etype: str, attr: str) -&gt; dict[JsonVal, int]\n</code></pre> <p>Gets distinct attribute values and their counts based on latest snapshots</p> <p>Useful for displaying <code>&lt;select&gt;</code> enumeration fields.</p> <p>Works for all plain and observation data types except <code>dict</code> and <code>json</code>.</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.get(\n    \"/{etype}/_/distinct/{attr}\",\n    responses={400: {\"description\": \"Query can't be processed\", \"model\": ErrorResponse}},\n)\nasync def get_distinct_attribute_values(etype: str, attr: str) -&gt; dict[JsonVal, int]:\n    \"\"\"Gets distinct attribute values and their counts based on latest snapshots\n\n    Useful for displaying `&lt;select&gt;` enumeration fields.\n\n    Works for all plain and observation data types except `dict` and `json`.\n    \"\"\"\n    try:\n        return DB.snapshots.get_distinct_val_count(etype, attr)\n    except DatabaseError as e:\n        raise HTTPException(status_code=400, detail=str(e)) from e\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.extend_eid_ttls","title":"extend_eid_ttls  <code>async</code>","text":"<pre><code>extend_eid_ttls(e: ParsedEid, body: dict[str, datetime]) -&gt; SuccessResponse\n</code></pre> <p>Extend TTLs of the specified entity</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.post(\"/{etype}/{eid}/ttl\")\nasync def extend_eid_ttls(e: ParsedEid, body: dict[str, datetime]) -&gt; SuccessResponse:\n    \"\"\"Extend TTLs of the specified entity\"\"\"\n    # Construct task\n    with task_context(MODEL_SPEC):\n        task = DataPointTask(etype=e.type, eid=e.id, ttl_tokens=body)\n\n    # Push tasks to task queue\n    TASK_WRITER.put_task(task, False)\n\n    return SuccessResponse()\n</code></pre>"},{"location":"reference/api/routers/entity/#dp3.api.routers.entity.delete_eid_record","title":"delete_eid_record  <code>async</code>","text":"<pre><code>delete_eid_record(e: ParsedEid) -&gt; SuccessResponse\n</code></pre> <p>Delete the master record and snapshots of the specified entity.</p> <p>Notice that this does not delete any raw datapoints, or block the re-creation of the entity if new datapoints are received.</p> Source code in <code>dp3/api/routers/entity.py</code> <pre><code>@router.delete(\"/{etype}/{eid}\")\nasync def delete_eid_record(e: ParsedEid) -&gt; SuccessResponse:\n    \"\"\"Delete the master record and snapshots of the specified entity.\n\n    Notice that this does not delete any raw datapoints,\n    or block the re-creation of the entity if new datapoints are received.\n    \"\"\"\n    # Create a \"delete\" task and push it to task queue\n    with task_context(MODEL_SPEC):\n        task = DataPointTask(etype=e.type, eid=e.id, delete=True)\n    TASK_WRITER.put_task(task, False)\n\n    return SuccessResponse()\n</code></pre>"},{"location":"reference/api/routers/root/","title":"root","text":""},{"location":"reference/api/routers/root/#dp3.api.routers.root","title":"dp3.api.routers.root","text":""},{"location":"reference/api/routers/root/#dp3.api.routers.root.health_check","title":"health_check  <code>async</code>","text":"<pre><code>health_check() -&gt; HealthCheckResponse\n</code></pre> <p>Health check</p> <p>Returns simple 'It works!' response.</p> Source code in <code>dp3/api/routers/root.py</code> <pre><code>@router.get(\"/\", tags=[\"Health\"])\nasync def health_check() -&gt; HealthCheckResponse:\n    \"\"\"Health check\n\n    Returns simple 'It works!' response.\n    \"\"\"\n    return HealthCheckResponse()\n</code></pre>"},{"location":"reference/api/routers/root/#dp3.api.routers.root.insert_datapoints","title":"insert_datapoints  <code>async</code>","text":"<pre><code>insert_datapoints(dps: list[DataPoint], request: Request) -&gt; SuccessResponse\n</code></pre> <p>Insert datapoints</p> <p>Validates and pushes datapoints into task queue, so they are processed by one of DP3 workers.</p> Source code in <code>dp3/api/routers/root.py</code> <pre><code>@router.post(DATAPOINTS_INGESTION_URL_PATH, tags=[\"Data ingestion\"])\nasync def insert_datapoints(dps: list[DataPoint], request: Request) -&gt; SuccessResponse:\n    \"\"\"Insert datapoints\n\n    Validates and pushes datapoints into task queue, so they are processed by one of DP3 workers.\n    \"\"\"\n    # Convert to DP3 datapoints\n    # This should not fail as all datapoints are already validated\n    dp3_dps = [api_to_dp3_datapoint(dp.model_dump()) for dp in dps]\n\n    # Group datapoints by etype-eid\n    tasks_dps = defaultdict(list)\n    for dp in dp3_dps:\n        key = (dp.etype, dp.eid)\n        tasks_dps[key].append(dp)\n\n    # Create tasks\n    tasks = []\n    with task_context(MODEL_SPEC):\n        for k in tasks_dps:\n            etype, eid = k\n\n            # This shouldn't fail either\n            tasks.append(DataPointTask(etype=etype, eid=eid, data_points=tasks_dps[k]))\n\n    # Push tasks to task queue\n    for task in tasks:\n        TASK_WRITER.put_task(task, False)\n\n    # Log datapoints\n    DP_LOGGER.log_good(dp3_dps, src=request.client.host)\n\n    return SuccessResponse()\n</code></pre>"},{"location":"reference/api/routers/root/#dp3.api.routers.root.list_entity_types","title":"list_entity_types  <code>async</code>","text":"<pre><code>list_entity_types() -&gt; dict[str, EntityState]\n</code></pre> <p>List entity types</p> <p>Returns dictionary containing all entity types configured -- their simplified configuration and current state information.</p> Source code in <code>dp3/api/routers/root.py</code> <pre><code>@router.get(\"/entities\", tags=[\"Entity\"])\nasync def list_entity_types() -&gt; dict[str, EntityState]:\n    \"\"\"List entity types\n\n    Returns dictionary containing all entity types configured -- their simplified configuration\n    and current state information.\n    \"\"\"\n    entities = {}\n\n    for etype in MODEL_SPEC.entities:\n        entity_spec = MODEL_SPEC.entity(etype)\n        entities[etype] = {\n            \"id\": etype,\n            \"id_data_type\": entity_spec.id_data_type.root,\n            \"name\": entity_spec.name,\n            \"attribs\": MODEL_SPEC.attribs(etype),\n            \"eid_estimate_count\": DB.estimate_count_eids(etype),\n        }\n\n    return entities\n</code></pre>"},{"location":"reference/api/routers/telemetry/","title":"telemetry","text":""},{"location":"reference/api/routers/telemetry/#dp3.api.routers.telemetry","title":"dp3.api.routers.telemetry","text":""},{"location":"reference/api/routers/telemetry/#dp3.api.routers.telemetry.get_sources_validity","title":"get_sources_validity  <code>async</code>","text":"<pre><code>get_sources_validity() -&gt; dict[str, datetime]\n</code></pre> <p>Get validity of all data sources</p> <p>Returns timestamps (datetimes) of current validity of all sources. This should be latest <code>t2</code> of incoming datapoints for given data source.</p> Source code in <code>dp3/api/routers/telemetry.py</code> <pre><code>@router.get(\"/sources_validity\")\nasync def get_sources_validity() -&gt; dict[str, datetime]:\n    \"\"\"Get validity of all data sources\n\n    Returns timestamps (datetimes) of current validity of all sources.\n    This should be latest `t2` of incoming datapoints for given data source.\n    \"\"\"\n    return TELEMETRY_READER.get_sources_validity()\n</code></pre>"},{"location":"reference/bin/","title":"bin","text":""},{"location":"reference/bin/#dp3.bin","title":"dp3.bin","text":"<p>Executables for running the platform.</p>"},{"location":"reference/bin/api/","title":"api","text":""},{"location":"reference/bin/api/#dp3.bin.api","title":"dp3.bin.api","text":"<p>Run the DP3 API using uvicorn.</p>"},{"location":"reference/bin/check/","title":"check","text":""},{"location":"reference/bin/check/#dp3.bin.check","title":"dp3.bin.check","text":"<p>Load and check configuration from given directory, print any errors, and exit.</p> TODO <ul> <li>refactor to simplify the code, some error path matching must be done to counteract   the AttrSpec function magic where Pydantic fails, but otherwise it is not required</li> </ul>"},{"location":"reference/bin/check/#dp3.bin.check.ConfigEncoder","title":"ConfigEncoder","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>JSONEncoder to encode parsed configuration.</p>"},{"location":"reference/bin/check/#dp3.bin.check.locate_attribs_error","title":"locate_attribs_error","text":"<pre><code>locate_attribs_error(data: dict, sought_err: str) -&gt; tuple[list[tuple], list[dict]]\n</code></pre> <p>Locate source of an error in a dict of AttrSpecs. Returns all sources of the same kind of error.</p> Source code in <code>dp3/bin/check.py</code> <pre><code>def locate_attribs_error(data: dict, sought_err: str) -&gt; tuple[list[tuple], list[dict]]:\n    \"\"\"\n    Locate source of an error in a dict of AttrSpecs.\n    Returns all sources of the same kind of error.\n    \"\"\"\n    paths = []\n    sources = []\n\n    for attr, attr_spec in data.items():\n        try:\n            AttrSpec(attr, attr_spec)\n        except ValidationError as exception:\n            for err_dict in exception.errors():\n                if err_dict[\"msg\"] == sought_err:\n                    paths.append((attr, *err_dict[\"loc\"]))\n                    sources.append(attr_spec)\n        except (ValueError, AssertionError) as exception:\n            if sought_err.endswith(exception.args[0]):\n                paths.append((attr,))\n                sources.append(attr_spec)\n\n    return paths, sources\n</code></pre>"},{"location":"reference/bin/check/#dp3.bin.check.get_error_sources","title":"get_error_sources","text":"<pre><code>get_error_sources(data: dict, error: dict) -&gt; tuple[list, list]\n</code></pre> <p>Locate source of an error in validated data using Pydantic error path.</p> Source code in <code>dp3/bin/check.py</code> <pre><code>def get_error_sources(data: dict, error: dict) -&gt; tuple[list, list]:\n    \"\"\"Locate source of an error in validated data using Pydantic error path.\"\"\"\n    err_path = error[\"loc\"]\n\n    # Kickstart the model exploration\n    curr_model_dict = get_type_hints(ModelSpec)[\"config\"]\n    curr_model_origin = get_origin(curr_model_dict)\n    curr_model = get_args(curr_model_dict)[1]\n\n    for key in err_path[1:]:\n        if curr_model_origin is not dict and curr_model_origin is not None:\n            return [err_path], [data]\n\n        if key in data:\n            prev_data = data\n            data = data[key]\n\n            if (curr_model, key) in special_model_cases:\n                curr_model = special_model_cases[curr_model, key]\n                curr_model_dict = get_type_hints(curr_model)\n                curr_model_origin = get_origin(curr_model_dict)\n\n                if curr_model == AttrSpec:\n                    return get_all_attribs_errors(data, error)\n                continue\n\n            if isinstance(curr_model_dict, dict):\n                if key in curr_model_dict:\n                    curr_model = curr_model_dict[key]\n                else:\n                    return [err_path], [prev_data]\n\n            curr_model_dict = get_type_hints(curr_model)\n            curr_model_origin = get_origin(curr_model_dict)\n            if curr_model_origin is dict:\n                curr_model = get_args(curr_model_dict)[1]\n        else:\n            return [err_path], [data]\n\n    if curr_model == AttrSpec:\n        return get_all_attribs_errors(data, error)\n\n    return [], locate_basemodel_error(data, curr_model)\n</code></pre>"},{"location":"reference/bin/check/#dp3.bin.check.locate_errors","title":"locate_errors","text":"<pre><code>locate_errors(exc: ValidationError, data: dict)\n</code></pre> <p>Locate errors (i.e.: get the paths and sources) in a ValidationError object.</p> Source code in <code>dp3/bin/check.py</code> <pre><code>def locate_errors(exc: ValidationError, data: dict):\n    \"\"\"Locate errors (i.e.: get the paths and sources) in a ValidationError object.\"\"\"\n    paths = []\n    sources = []\n    errors = []\n\n    for error in exc.errors():\n        if error[\"loc\"] == ():\n            paths.append(())\n            sources.append(None)\n            errors.append(error[\"msg\"])\n            continue\n\n        message = f'{error[\"msg\"]} (type={error[\"type\"]})'\n        e_paths, e_sources = get_error_sources(data, error)\n\n        paths.extend(e_paths)\n        sources.extend(e_sources)\n        errors.extend(message for _ in range(len(e_paths)))\n\n    return paths, sources, errors\n</code></pre>"},{"location":"reference/bin/cli/","title":"cli","text":""},{"location":"reference/bin/cli/#dp3.bin.cli","title":"dp3.bin.cli","text":"<p>A utility for running DP3 commands using a CLI.</p>"},{"location":"reference/bin/config/","title":"config","text":""},{"location":"reference/bin/config/#dp3.bin.config","title":"dp3.bin.config","text":"<p>DP3 Setup Config Script for creating a DP3 application.</p>"},{"location":"reference/bin/config/#dp3.bin.config.init_parser","title":"init_parser","text":"<pre><code>init_parser(parser)\n</code></pre> <p>There are two desired use-cases for this command:</p> <pre><code>dp3 config nginx --app-name &lt;APP_NAME&gt; --hostname &lt;SERVER_HOSTNAME&gt; --www-root &lt;DIRECTORY&gt;\n</code></pre> <pre><code>dp3 config supervisor --app-name &lt;APP_NAME&gt; --config &lt;CONFIG_DIR&gt;\n</code></pre> Source code in <code>dp3/bin/config.py</code> <pre><code>def init_parser(parser):\n    \"\"\"\n    There are two desired use-cases for this command:\n\n    ```\n    dp3 config nginx --app-name &lt;APP_NAME&gt; --hostname &lt;SERVER_HOSTNAME&gt; --www-root &lt;DIRECTORY&gt;\n    ```\n\n    ```\n    dp3 config supervisor --app-name &lt;APP_NAME&gt; --config &lt;CONFIG_DIR&gt;\n    ```\n    \"\"\"\n    parser.add_argument(\n        \"subcommand\",\n        help=\"What configuration to setup. \"\n        \"'nginx' requires appname, hostname and www_root, \"\n        \"'supervisor' requires appname and config.\",\n        choices=[\"nginx\", \"supervisor\"],\n    )\n    parser.add_argument(\n        \"--app-name\", dest=\"app_name\", help=\"The name of the application.\", type=str\n    )\n    parser.add_argument(\n        \"--hostname\", dest=\"server_hostname\", help=\"The hostname of the server.\", type=str\n    )\n    parser.add_argument(\n        \"--www-root\",\n        dest=\"www_root\",\n        help=\"The directory where served HTML content will be placed.\",\n        type=str,\n    )\n    parser.add_argument(\n        \"--config\",\n        dest=\"config_dir\",\n        help=\"The directory where the DP3 config of your application is stored.\",\n        type=str,\n    )\n</code></pre>"},{"location":"reference/bin/config/#dp3.bin.config.config_nginx","title":"config_nginx","text":"<pre><code>config_nginx(app_name, server_hostname, www_root)\n</code></pre> <p>Configure nginx to serve the application.</p> Source code in <code>dp3/bin/config.py</code> <pre><code>def config_nginx(app_name, server_hostname, www_root):\n    \"\"\"Configure nginx to serve the application.\"\"\"\n    # Get the current package location.\n    package_dir = Path(__file__).parent.parent\n\n    nginx_dir = Path(\"/etc/nginx/\")\n\n    # Copy the template files to the project directory.\n    shutil.copytree(package_dir / \"template\" / \"nginx\", nginx_dir, dirs_exist_ok=True)\n\n    replace_template(nginx_dir, \"{{DP3_APP}}\", app_name)\n    replace_template(nginx_dir, \"__SERVER_NAME__\", server_hostname)\n    replace_template(nginx_dir, \"__WWW_ROOT__\", www_root)\n\n    # Create the www root directory.\n    www_root_dir = Path(www_root)\n    www_root_dir.mkdir(exist_ok=True, parents=True)\n    shutil.copytree(package_dir / \"template\" / \"html\", www_root_dir, dirs_exist_ok=True)\n\n    replace_template(www_root_dir, \"{{DP3_APP}}\", app_name)\n    replace_template(www_root_dir, \"{{HOSTNAME}}\", server_hostname)\n\n    shutil.chown(www_root_dir, user=app_name, group=app_name)\n    www_root_dir.chmod(0o775)\n</code></pre>"},{"location":"reference/bin/config/#dp3.bin.config.get_python_directories","title":"get_python_directories","text":"<pre><code>get_python_directories() -&gt; tuple[Path, Path]\n</code></pre> <p>Get the directory where the DP3 executable is located and where the DP3 library is installed.</p> <p>There are two cases we need to consider:</p> <ul> <li>The package is installed in a normal way, be it in virtual environment,   in <code>~/.local</code> or in <code>/usr/local</code>. The path will allways look something like   <code>/path/to/python/lib/python3.X/{site|dist}-packages/dp3</code>.   The path to the executable will be <code>/path/to/python/bin/dp3</code>.</li> <li>The package is installed in editable mode, in which case the path of the   package depends on the location of the repo. The path to the executable relates   in no way to the path of the package, we cannot be sure.   We therefore give it our best guess, which is the path of the python executable.   This will be correct only in the case of a virtual environment, but it is the best   we can do.</li> </ul> Source code in <code>dp3/bin/config.py</code> <pre><code>def get_python_directories() -&gt; tuple[Path, Path]:\n    \"\"\"\n    Get the directory where the DP3 executable is located and where the DP3 library is installed.\n\n    There are two cases we need to consider:\n\n    - The package is installed in a normal way, be it in virtual environment,\n      in `~/.local` or in `/usr/local`. The path will allways look something like\n      `/path/to/python/lib/python3.X/{site|dist}-packages/dp3`.\n      The path to the executable will be `/path/to/python/bin/dp3`.\n    - The package is installed in editable mode, in which case the path of the\n      package depends on the location of the repo. The path to the executable relates\n      in no way to the path of the package, we cannot be sure.\n      We therefore give it our best guess, which is the path of the python executable.\n      This will be correct only in the case of a virtual environment, but it is the best\n      we can do.\n    \"\"\"\n    package_path = Path(__file__).parent.parent.absolute()\n    packages_path = package_path.parent\n    lib_python_path = packages_path.parent\n\n    if \"-packages\" in packages_path.name and \"python3\" in lib_python_path.name:\n        binary_path = package_path.parent.parent.parent.parent / \"bin\"\n    else:  # This is a development install.\n        binary_path = Path(sys.executable).parent.absolute()\n    return binary_path, package_path\n</code></pre>"},{"location":"reference/bin/config/#dp3.bin.config.config_supervisor","title":"config_supervisor","text":"<pre><code>config_supervisor(app_name, config_dir)\n</code></pre> <p>Configure supervisor for process management. Replaces templates: DP3_EXE, DP3_APP, CONFIG_DIR, DP3_BIN, DP3_LIB</p> Source code in <code>dp3/bin/config.py</code> <pre><code>def config_supervisor(app_name, config_dir):\n    \"\"\"\n    Configure supervisor for process management.\n    Replaces templates: DP3_EXE, DP3_APP, CONFIG_DIR, DP3_BIN, DP3_LIB\n    \"\"\"\n    # Get the current package location and other relative directories.\n    python_bin_dir, package_dir = get_python_directories()\n    dp3_executable_path = str(python_bin_dir / \"dp3\")\n    abs_config_dir = Path(config_dir).absolute()\n\n    # Load configured worker count from dp3 config.\n    config = read_config_dir(config_dir, recursive=True)\n    worker_count = int(config.get(\"processing_core.worker_processes\"))\n\n    # Ensure the config directory exists.\n    supervisor_dir = Path(f\"/etc/{app_name}/\")\n    supervisor_dir.mkdir(exist_ok=True, parents=True)\n    shutil.chown(supervisor_dir, user=app_name, group=app_name)\n    supervisor_dir.chmod(0o775)\n\n    # Copy the template files to the project directory.\n    shutil.copytree(package_dir / \"template\" / \"supervisor\", supervisor_dir, dirs_exist_ok=True)\n\n    replace_template(supervisor_dir, \"{{DP3_APP}}\", app_name)\n    replace_template(supervisor_dir, \"{{CONFIG_DIR}}\", str(abs_config_dir))\n    replace_template(supervisor_dir, \"{{DP3_BIN}}\", str(python_bin_dir))\n    replace_template(supervisor_dir, \"{{DP3_EXE}}\", dp3_executable_path)\n    replace_template(supervisor_dir, \"{{DP3_PACKAGE_DIR}}\", str(package_dir))\n    replace_template(supervisor_dir, \"{{WORKER_COUNT}}\", str(worker_count))\n\n    # Set up the systemd service to start supervisor.\n    service_path = Path(f\"/etc/systemd/system/{app_name}.service\")\n    shutil.copy(\n        package_dir / \"template\" / \"systemd\" / \"app.service\",\n        service_path,\n    )\n    replace_template_file(service_path, \"{{DP3_APP}}\", app_name)\n\n    # Set up the appctl script.\n    appctl_path = Path(f\"/usr/bin/{app_name}ctl\")\n    shutil.copy(\n        package_dir / \"template\" / \"appctl\",\n        appctl_path,\n    )\n    replace_template_file(appctl_path, \"{{DP3_APP}}\", app_name)\n    appctl_path.chmod(0o755)\n</code></pre>"},{"location":"reference/bin/schema_update/","title":"schema_update","text":""},{"location":"reference/bin/schema_update/#dp3.bin.schema_update","title":"dp3.bin.schema_update","text":"<p>Update the database schema after making conflicting changes to the model.</p> <p>Authors: Ond\u0159ej Sedl\u00e1\u010dek xsedla1o@stud.fit.vutbr.cz</p>"},{"location":"reference/bin/setup/","title":"setup","text":""},{"location":"reference/bin/setup/#dp3.bin.setup","title":"dp3.bin.setup","text":"<p>DP3 Setup Script for creating a DP3 application.</p>"},{"location":"reference/bin/setup/#dp3.bin.setup.replace_template","title":"replace_template","text":"<pre><code>replace_template(directory: Path, template: str, replace_with: str)\n</code></pre> <p>Replace all occurrences of <code>template</code> with the given text.</p> Source code in <code>dp3/bin/setup.py</code> <pre><code>def replace_template(directory: Path, template: str, replace_with: str):\n    \"\"\"Replace all occurrences of `template` with the given text.\"\"\"\n    for file in directory.rglob(\"*\"):\n        if file.is_file():\n            replace_template_file(file, template, replace_with)\n</code></pre>"},{"location":"reference/bin/setup/#dp3.bin.setup.init_parser","title":"init_parser","text":"<pre><code>init_parser(parser)\n</code></pre> <p>Initialize an argparse object to parse the project directory and the app name.</p> Source code in <code>dp3/bin/setup.py</code> <pre><code>def init_parser(parser):\n    \"\"\"\n    Initialize an argparse object to parse the project directory and the app name.\n    \"\"\"\n    parser.add_argument(\"project_dir\", help=\"The project directory.\", type=str)\n    parser.add_argument(\"app_name\", help=\"The name of the application.\", type=str)\n</code></pre>"},{"location":"reference/bin/worker/","title":"worker","text":""},{"location":"reference/bin/worker/#dp3.bin.worker","title":"dp3.bin.worker","text":""},{"location":"reference/common/","title":"common","text":""},{"location":"reference/common/#dp3.common","title":"dp3.common","text":"<p>Common modules which are used throughout the platform.</p> <ul> <li>Config, EntitySpec and AttrSpec - Models for reading, validation and representing platform configuration of entities and their attributes. datatype is also used within this context.</li> <li>Scheduler - Allows modules to run callbacks at specified times</li> <li>Task - Model for a single task processed by the platform</li> <li>Utils - Auxiliary utility functions</li> </ul>"},{"location":"reference/common/attrspec/","title":"attrspec","text":""},{"location":"reference/common/attrspec/#dp3.common.attrspec","title":"dp3.common.attrspec","text":""},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrType","title":"AttrType","text":"<p>               Bases: <code>Flag</code></p> <p>Enum of attribute types</p> <p><code>PLAIN</code> = 1 <code>OBSERVATIONS</code> = 2 <code>TIMESERIES</code> = 4</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrType.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(type_str: str)\n</code></pre> <p>Convert string representation like \"plain\" to AttrType.</p> Source code in <code>dp3/common/attrspec.py</code> <pre><code>@classmethod\ndef from_str(cls, type_str: str):\n    \"\"\"\n    Convert string representation like \"plain\" to AttrType.\n    \"\"\"\n    try:\n        return cls(cls[type_str.upper()])\n    except Exception as e:\n        raise ValueError(f\"Invalid `type` of attribute '{type_str}'\") from e\n</code></pre>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.ObservationsHistoryParams","title":"ObservationsHistoryParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>History parameters field of observations attribute</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.TimeseriesTSParams","title":"TimeseriesTSParams","text":"<p>               Bases: <code>BaseModel</code></p> <p>Timeseries parameters field of timeseries attribute</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.TimeseriesSeries","title":"TimeseriesSeries","text":"<p>               Bases: <code>BaseModel</code></p> <p>Series of timeseries attribute</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecGeneric","title":"AttrSpecGeneric","text":"<p>               Bases: <code>SpecModel</code></p> <p>Base of attribute specification</p> <p>Parent of other <code>AttrSpec</code> classes.</p> <p>Attributes:</p> Name Type Description <code>ttl</code> <code>Optional[ParsedTimedelta]</code> <p>Optional extension of TTL of the entity - will be ignored if lifetime setting does not match.</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecClassic","title":"AttrSpecClassic","text":"<p>               Bases: <code>AttrSpecGeneric</code></p> <p>Parent of non-timeseries <code>AttrSpec</code> classes.</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecClassic.is_iterable","title":"is_iterable  <code>property</code>","text":"<pre><code>is_iterable: bool\n</code></pre> <p>Returns whether specified attribute is iterable.</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecClassic.element_type","title":"element_type  <code>property</code>","text":"<pre><code>element_type: DataType\n</code></pre> <p>Returns the element type for iterable data types.</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecClassic.is_relation","title":"is_relation  <code>property</code>","text":"<pre><code>is_relation: bool\n</code></pre> <p>Returns whether specified attribute is a link.</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecClassic.relation_to","title":"relation_to  <code>property</code>","text":"<pre><code>relation_to: str\n</code></pre> <p>Returns linked entity id. Raises ValueError if attribute is not a link.</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecClassic.is_mirrored","title":"is_mirrored  <code>property</code>","text":"<pre><code>is_mirrored: bool\n</code></pre> <p>Returns whether specified attribute is a mirrored link.</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecClassic.mirror_as","title":"mirror_as  <code>property</code>","text":"<pre><code>mirror_as: str\n</code></pre> <p>Returns:</p> Type Description <code>str</code> <p>name of the mirrored attribute.</p> <p>Raises:     ValueError: If attribute is not a mirrored link.</p>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecPlain","title":"AttrSpecPlain","text":"<pre><code>AttrSpecPlain(**data)\n</code></pre> <p>               Bases: <code>AttrSpecClassic</code></p> <p>Plain attribute specification</p> Source code in <code>dp3/common/attrspec.py</code> <pre><code>def __init__(self, **data):\n    super().__init__(**data)\n\n    entity_spec = get_entity_context()[\"self\"]\n    self._dp_model = create_model(\n        f\"DataPointPlain_{self.id}\",\n        __base__=DataPointPlainBase,\n        eid=(entity_spec.id_data_type.data_type, ...),\n        v=(self.data_type.data_type, ...),\n    )\n</code></pre>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecReadOnly","title":"AttrSpecReadOnly","text":"<pre><code>AttrSpecReadOnly(**data)\n</code></pre> <p>               Bases: <code>AttrSpecPlain</code></p> <p>Read-only plain attribute specification. Used for internal attributes.</p> Source code in <code>dp3/common/attrspec.py</code> <pre><code>def __init__(self, **data):\n    super().__init__(**data)\n\n    entity_spec = get_entity_context()[\"self\"]\n    self._dp_model = create_model(\n        f\"DataPointReadOnly_{self.id}\",\n        __base__=DataPointPlainBase,\n        eid=(entity_spec.id_data_type.data_type, ...),\n        v=(ReadOnly, ...),\n    )\n</code></pre>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecObservations","title":"AttrSpecObservations","text":"<pre><code>AttrSpecObservations(**data)\n</code></pre> <p>               Bases: <code>AttrSpecClassic</code></p> <p>Observations attribute specification</p> Source code in <code>dp3/common/attrspec.py</code> <pre><code>def __init__(self, **data):\n    super().__init__(**data)\n\n    value_validator = self.data_type.data_type\n    entity_spec = get_entity_context()[\"self\"]\n    self._dp_model = create_model(\n        f\"DataPointObservations_{self.id}\",\n        __base__=DataPointObservationsBase,\n        eid=(entity_spec.id_data_type.data_type, ...),\n        v=(value_validator, ...),\n    )\n</code></pre>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpecTimeseries","title":"AttrSpecTimeseries","text":"<pre><code>AttrSpecTimeseries(**data)\n</code></pre> <p>               Bases: <code>AttrSpecGeneric</code></p> <p>Timeseries attribute specification</p> Source code in <code>dp3/common/attrspec.py</code> <pre><code>def __init__(self, **data):\n    super().__init__(**data)\n\n    entity_spec = get_entity_context()[\"self\"]\n\n    # Typing of `v` field\n    dp_value_typing = {}\n    for s in self.series:\n        data_type = self.series[s].data_type.data_type\n        dp_value_typing[s] = ((list[data_type]), ...)\n\n    # Add root validator\n    if self.timeseries_type == \"regular\":\n        root_validator = dp_ts_root_validator_regular_wrapper(self.timeseries_params.time_step)\n    elif self.timeseries_type == \"irregular\":\n        root_validator = dp_ts_root_validator_irregular\n    elif self.timeseries_type == \"irregular_intervals\":\n        root_validator = dp_ts_root_validator_irregular_intervals\n    else:\n        raise ValueError(f\"Unknown timeseries type '{self.timeseries_type}'\")\n\n    # Validators\n    dp_validators = {\n        \"v_validator\": field_validator(\"v\")(dp_ts_v_validator),\n        \"root_validator\": model_validator(mode=\"after\")(root_validator),\n    }\n\n    self._dp_model = create_model(\n        f\"DataPointTimeseries_{self.id}\",\n        __base__=DataPointTimeseriesBase,\n        __validators__=dp_validators,\n        eid=(entity_spec.id_data_type.data_type, ...),\n        v=(create_model(f\"DataPointTimeseriesValue_{self.id}\", **dp_value_typing), ...),\n    )\n</code></pre>"},{"location":"reference/common/attrspec/#dp3.common.attrspec.AttrSpec","title":"AttrSpec","text":"<pre><code>AttrSpec(id: str, spec: dict[str, Any]) -&gt; AttrSpecType\n</code></pre> <p>Factory for <code>AttrSpec</code> classes</p> Source code in <code>dp3/common/attrspec.py</code> <pre><code>def AttrSpec(id: str, spec: dict[str, Any]) -&gt; AttrSpecType:\n    \"\"\"Factory for `AttrSpec` classes\"\"\"\n\n    assert isinstance(spec, dict), \"Attribute specification must be a dict\"\n    if \"type\" not in spec:\n        raise ValueError(\"Missing mandatory attribute `type`\")\n    attr_type = AttrType.from_str(spec.get(\"type\"))\n    subclasses = {\n        AttrType.PLAIN: AttrSpecPlain,\n        AttrType.OBSERVATIONS: AttrSpecObservations,\n        AttrType.TIMESERIES: AttrSpecTimeseries,\n    }\n\n    spec[\"id\"] = id  # Fill in the id if not present, else overwrite\n    return subclasses[attr_type](**spec)\n</code></pre>"},{"location":"reference/common/base_module/","title":"base_module","text":""},{"location":"reference/common/base_module/#dp3.common.base_module","title":"dp3.common.base_module","text":""},{"location":"reference/common/base_module/#dp3.common.base_module.BaseModule","title":"BaseModule","text":"<pre><code>BaseModule(platform_config: PlatformConfig, module_config: dict, registrar: CallbackRegistrar)\n</code></pre> <p>Base class for platform modules. Every module must inherit this abstract class for automatic loading of module!</p> <p>Attributes:</p> Name Type Description <code>refresh</code> <code>SharedFlag</code> <p>SharedFlag that is set to True when module should refresh its attributes values</p> <code>log</code> <code>Logger</code> <p>Logger for the module</p> <p>Initialize the module and register callbacks.</p> <p><code>self.load_config()</code> is called in the base class.</p> <p>Parameters:</p> Name Type Description Default <code>platform_config</code> <code>PlatformConfig</code> <p>Platform configuration class</p> required <code>module_config</code> <code>dict</code> <p>Configuration of the module, equivalent of <code>platform_config.config.get(\"modules.&lt;module_name&gt;\")</code></p> required <code>registrar</code> <code>CallbackRegistrar</code> <p>A callback / hook registration interface</p> required Source code in <code>dp3/common/base_module.py</code> <pre><code>def __init__(\n    self, platform_config: PlatformConfig, module_config: dict, registrar: CallbackRegistrar\n):\n    \"\"\"Initialize the module and register callbacks.\n\n    `self.load_config()` is called in the base class.\n\n    Args:\n        platform_config: Platform configuration class\n        module_config: Configuration of the module,\n            equivalent of `platform_config.config.get(\"modules.&lt;module_name&gt;\")`\n        registrar: A callback / hook registration interface\n    \"\"\"\n    self.refresh: SharedFlag = SharedFlag(False, banner=f\"Refresh {self.__class__.__name__}\")\n    self.log: logging.Logger = logging.getLogger(self.__class__.__name__)\n\n    self.load_config(platform_config, module_config)\n</code></pre>"},{"location":"reference/common/base_module/#dp3.common.base_module.BaseModule.load_config","title":"load_config","text":"<pre><code>load_config(config: PlatformConfig, module_config: dict) -&gt; None\n</code></pre> <p>Load module configuration.</p> <p>Is called on module initialization and on refresh request via <code>/control</code> API.</p> Source code in <code>dp3/common/base_module.py</code> <pre><code>def load_config(self, config: PlatformConfig, module_config: dict) -&gt; None:\n    \"\"\"Load module configuration.\n\n    Is called on module initialization and on refresh request via `/control` API.\n    \"\"\"\n</code></pre>"},{"location":"reference/common/base_module/#dp3.common.base_module.BaseModule.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Run the module - used to run own thread if needed.</p> <p>Called after initialization, may be used to create and run a separate thread if needed by the module. Do nothing unless overridden.</p> Source code in <code>dp3/common/base_module.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Run the module - used to run own thread if needed.\n\n    Called after initialization, may be used to create and run a separate\n    thread if needed by the module. Do nothing unless overridden.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/common/base_module/#dp3.common.base_module.BaseModule.stop","title":"stop","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stop the module - used to stop own thread.</p> <p>Called before program exit, may be used to finalize and stop the separate thread if it is used. Do nothing unless overridden.</p> Source code in <code>dp3/common/base_module.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"\n    Stop the module - used to stop own thread.\n\n    Called before program exit, may be used to finalize and stop the\n    separate thread if it is used. Do nothing unless overridden.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/common/callback_registrar/","title":"callback_registrar","text":""},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar","title":"dp3.common.callback_registrar","text":""},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar","title":"CallbackRegistrar","text":"<pre><code>CallbackRegistrar(scheduler: Scheduler, task_executor: TaskExecutor, snap_shooter: SnapShooter, updater: Updater)\n</code></pre> <p>Interface for callback registration.</p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def __init__(\n    self,\n    scheduler: Scheduler,\n    task_executor: TaskExecutor,\n    snap_shooter: SnapShooter,\n    updater: Updater,\n):\n    self._scheduler = scheduler\n    self._task_executor = task_executor\n    self._snap_shooter = snap_shooter\n    self._updater = updater\n\n    self.log = logging.getLogger(self.__class__.__name__)\n    self.model_spec = task_executor.model_spec\n    self.attr_spec_t_to_on_attr = {\n        AttrType.PLAIN: \"on_new_plain\",\n        AttrType.OBSERVATIONS: \"on_new_observation\",\n        AttrType.TIMESERIES: \"on_new_ts_chunk\",\n    }\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.scheduler_register","title":"scheduler_register","text":"<pre><code>scheduler_register(func: Callable, *, func_args: Union[list, tuple] = None, func_kwargs: dict = None, year: Union[int, str] = None, month: Union[int, str] = None, day: Union[int, str] = None, week: Union[int, str] = None, day_of_week: Union[int, str] = None, hour: Union[int, str] = None, minute: Union[int, str] = None, second: Union[int, str] = None, timezone: str = 'UTC', misfire_grace_time: int = 1) -&gt; int\n</code></pre> <p>Register a function to be run at specified times.</p> <p>Pass cron-like specification of when the function should be called, see docs of apscheduler.triggers.cron for details.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>function or method to be called</p> required <code>func_args</code> <code>Union[list, tuple]</code> <p>list of positional arguments to call func with</p> <code>None</code> <code>func_kwargs</code> <code>dict</code> <p>dict of keyword arguments to call func with</p> <code>None</code> <code>year</code> <code>Union[int, str]</code> <p>4-digit year</p> <code>None</code> <code>month</code> <code>Union[int, str]</code> <p>month (1-12)</p> <code>None</code> <code>day</code> <code>Union[int, str]</code> <p>day of month (1-31)</p> <code>None</code> <code>week</code> <code>Union[int, str]</code> <p>ISO week (1-53)</p> <code>None</code> <code>day_of_week</code> <code>Union[int, str]</code> <p>number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun)</p> <code>None</code> <code>hour</code> <code>Union[int, str]</code> <p>hour (0-23)</p> <code>None</code> <code>minute</code> <code>Union[int, str]</code> <p>minute (0-59)</p> <code>None</code> <code>second</code> <code>Union[int, str]</code> <p>second (0-59)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for time specification (default is UTC).</p> <code>'UTC'</code> <code>misfire_grace_time</code> <code>int</code> <p>seconds after the designated run time that the job is still allowed to be run (default is 1)</p> <code>1</code> <p>Returns:      job ID</p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def scheduler_register(\n    self,\n    func: Callable,\n    *,\n    func_args: Union[list, tuple] = None,\n    func_kwargs: dict = None,\n    year: Union[int, str] = None,\n    month: Union[int, str] = None,\n    day: Union[int, str] = None,\n    week: Union[int, str] = None,\n    day_of_week: Union[int, str] = None,\n    hour: Union[int, str] = None,\n    minute: Union[int, str] = None,\n    second: Union[int, str] = None,\n    timezone: str = \"UTC\",\n    misfire_grace_time: int = 1,\n) -&gt; int:\n    \"\"\"\n    Register a function to be run at specified times.\n\n    Pass cron-like specification of when the function should be called,\n    see [docs](https://apscheduler.readthedocs.io/en/latest/modules/triggers/cron.html)\n    of apscheduler.triggers.cron for details.\n\n    Args:\n        func: function or method to be called\n        func_args: list of positional arguments to call func with\n        func_kwargs: dict of keyword arguments to call func with\n        year: 4-digit year\n        month: month (1-12)\n        day: day of month (1-31)\n        week: ISO week (1-53)\n        day_of_week: number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun)\n        hour: hour (0-23)\n        minute: minute (0-59)\n        second: second (0-59)\n        timezone: Timezone for time specification (default is UTC).\n        misfire_grace_time: seconds after the designated run time\n            that the job is still allowed to be run (default is 1)\n    Returns:\n         job ID\n    \"\"\"\n    return self._scheduler.register(\n        func,\n        func_args=func_args,\n        func_kwargs=func_kwargs,\n        year=year,\n        month=month,\n        day=day,\n        week=week,\n        day_of_week=day_of_week,\n        hour=hour,\n        minute=minute,\n        second=second,\n        timezone=timezone,\n        misfire_grace_time=misfire_grace_time,\n    )\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_task_hook","title":"register_task_hook","text":"<pre><code>register_task_hook(hook_type: str, hook: Callable)\n</code></pre> <p>Registers one of available task hooks</p> <p>See: <code>TaskGenericHooksContainer</code> in <code>task_hooks.py</code></p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_task_hook(self, hook_type: str, hook: Callable):\n    \"\"\"Registers one of available task hooks\n\n    See: [`TaskGenericHooksContainer`][dp3.task_processing.task_hooks.TaskGenericHooksContainer]\n    in `task_hooks.py`\n    \"\"\"\n    self._task_executor.register_task_hook(hook_type, hook)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_allow_entity_creation_hook","title":"register_allow_entity_creation_hook","text":"<pre><code>register_allow_entity_creation_hook(hook: Callable[[AnyEidT, DataPointTask], bool], entity: str)\n</code></pre> <p>Registers passed hook to allow entity creation.</p> <p>Binds hook to specified entity (though same hook can be bound multiple times).</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[AnyEidT, DataPointTask], bool]</code> <p><code>hook</code> callable should expect eid and Task as arguments and return a bool.</p> required <code>entity</code> <code>str</code> <p>specifies entity type</p> required Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_allow_entity_creation_hook(\n    self,\n    hook: Callable[[AnyEidT, DataPointTask], bool],\n    entity: str,\n):\n    \"\"\"\n    Registers passed hook to allow entity creation.\n\n    Binds hook to specified entity (though same hook can be bound multiple times).\n\n    Args:\n        hook: `hook` callable should expect eid and Task as arguments and return a bool.\n        entity: specifies entity type\n    \"\"\"\n    self._task_executor.register_entity_hook(\"allow_entity_creation\", hook, entity)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_on_entity_creation_hook","title":"register_on_entity_creation_hook","text":"<pre><code>register_on_entity_creation_hook(hook: Callable[[AnyEidT, DataPointTask], list[DataPointTask]], entity: str, refresh: SharedFlag = None, may_change: list[list[str]] = None)\n</code></pre> <p>Registers passed hook to be called on entity creation.</p> <p>Binds hook to specified entity (though same hook can be bound multiple times).</p> <p>Allows registration of refreshing on configuration changes, if <code>refresh</code> is specified. In that case, <code>may_change</code> must be specified.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[AnyEidT, DataPointTask], list[DataPointTask]]</code> <p><code>hook</code> callable should expect eid and Task as arguments and return a list of DataPointTask objects to perform.</p> required <code>entity</code> <code>str</code> <p>specifies entity type</p> required <code>refresh</code> <code>SharedFlag</code> <p>If specified, registered hook will be called on configuration changes. Pass <code>self.refresh</code> from <code>BaseModule</code> subclasses.</p> <code>None</code> <code>may_change</code> <code>list[list[str]]</code> <p>each item should specify an attribute that <code>hook</code> may change, for specification format see <code>register_correlation_hook</code></p> <code>None</code> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_on_entity_creation_hook(\n    self,\n    hook: Callable[[AnyEidT, DataPointTask], list[DataPointTask]],\n    entity: str,\n    refresh: SharedFlag = None,\n    may_change: list[list[str]] = None,\n):\n    \"\"\"\n    Registers passed hook to be called on entity creation.\n\n    Binds hook to specified entity (though same hook can be bound multiple times).\n\n    Allows registration of refreshing on configuration changes, if `refresh` is specified.\n    In that case, `may_change` must be specified.\n\n    Args:\n        hook: `hook` callable should expect eid and Task as arguments and\n            return a list of DataPointTask objects to perform.\n        entity: specifies entity type\n        refresh: If specified, registered hook will be called on configuration changes.\n            Pass `self.refresh` from `BaseModule` subclasses.\n        may_change: each item should specify an attribute that `hook` may change,\n            for specification format see `register_correlation_hook`\n    \"\"\"\n    self._task_executor.register_entity_hook(\"on_entity_creation\", hook, entity)\n    if refresh is not None:\n        if may_change is None:\n            raise ValueError(\"'may_change' must be specified if 'refresh' is specified\")\n        self._snap_shooter.register_correlation_hook(\n            partial(on_entity_creation_in_snapshots, self.model_spec, refresh, hook),\n            entity,\n            [],\n            may_change,\n        )\n        self._snap_shooter.register_run_finalize_hook(partial(unset_flag, refresh))\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_entity_hook","title":"register_entity_hook","text":"<pre><code>register_entity_hook(hook_type: str, hook: Callable, entity: str)\n</code></pre> <p>Registers one of available task entity hooks</p> <p>Deprecated</p> <p>This method is deprecated, use <code>register_on_entity_creation_hook</code> or <code>register_allow_entity_creation_hook</code> instead.</p> <p>See: <code>TaskEntityHooksContainer</code> in <code>task_hooks.py</code></p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_entity_hook(self, hook_type: str, hook: Callable, entity: str):\n    \"\"\"Registers one of available task entity hooks\n\n    !!! warning \"Deprecated\"\n\n        This method is deprecated, use `register_on_entity_creation_hook`\n        or `register_allow_entity_creation_hook` instead.\n\n    See: [`TaskEntityHooksContainer`][dp3.task_processing.task_hooks.TaskEntityHooksContainer]\n    in `task_hooks.py`\n    \"\"\"\n    self._task_executor.register_entity_hook(hook_type, hook, entity)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_on_new_attr_hook","title":"register_on_new_attr_hook","text":"<pre><code>register_on_new_attr_hook(hook: Callable[[AnyEidT, DataPointType], Union[None, list[DataPointTask]]], entity: str, attr: str, refresh: SharedFlag = None, may_change: list[list[str]] = None)\n</code></pre> <p>Registers passed hook to be called on new attribute datapoint.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[AnyEidT, DataPointType], Union[None, list[DataPointTask]]]</code> <p><code>hook</code> callable should expect eid and a datapoint as arguments. Can optionally return a list of DataPointTasks to perform.</p> required <code>entity</code> <code>str</code> <p>specifies entity type</p> required <code>attr</code> <code>str</code> <p>specifies attribute name</p> required <code>refresh</code> <code>SharedFlag</code> <p>If specified, registered hook will be called on configuration changes. Pass <code>self.refresh</code> from <code>BaseModule</code> subclasses.</p> <code>None</code> <code>may_change</code> <code>list[list[str]]</code> <p>each item should specify an attribute that <code>hook</code> may change, for specification format see <code>register_correlation_hook</code></p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If entity and attr do not specify a valid attribute, a ValueError is raised.</p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_on_new_attr_hook(\n    self,\n    hook: Callable[[AnyEidT, DataPointType], Union[None, list[DataPointTask]]],\n    entity: str,\n    attr: str,\n    refresh: SharedFlag = None,\n    may_change: list[list[str]] = None,\n):\n    \"\"\"\n    Registers passed hook to be called on new attribute datapoint.\n\n    Args:\n        hook: `hook` callable should expect eid and a datapoint as arguments.\n            Can optionally return a list of DataPointTasks to perform.\n        entity: specifies entity type\n        attr: specifies attribute name\n        refresh: If specified, registered hook will be called on configuration changes.\n            Pass `self.refresh` from `BaseModule` subclasses.\n        may_change: each item should specify an attribute that `hook` may change,\n            for specification format see `register_correlation_hook`\n\n    Raises:\n        ValueError: If entity and attr do not specify a valid attribute, a ValueError is raised.\n    \"\"\"\n    try:\n        hook_type = self.attr_spec_t_to_on_attr[self.model_spec.attributes[entity, attr].t]\n    except KeyError as e:\n        raise ValueError(\n            f\"Cannot register hook for attribute {entity}/{attr}, are you sure it exists?\"\n        ) from e\n    self._task_executor.register_attr_hook(hook_type, hook, entity, attr)\n\n    if refresh is None:\n        return\n    if may_change is None:\n        raise ValueError(\"'may_change' must be specified if 'refresh' is specified\")\n\n    self._snap_shooter.register_correlation_hook(\n        partial(on_attr_change_in_snapshots, self.model_spec, refresh, hook),\n        entity,\n        [[attr]],\n        may_change,\n    )\n    self._snap_shooter.register_run_finalize_hook(partial(unset_flag, refresh))\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_attr_hook","title":"register_attr_hook","text":"<pre><code>register_attr_hook(hook_type: str, hook: Callable, entity: str, attr: str)\n</code></pre> <p>Registers one of available task attribute hooks</p> <p>Deprecated</p> <p>This method is deprecated, use <code>register_on_new_attr_hook</code> instead.</p> <p>See: <code>TaskAttrHooksContainer</code> in <code>task_hooks.py</code></p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_attr_hook(self, hook_type: str, hook: Callable, entity: str, attr: str):\n    \"\"\"\n    Registers one of available task attribute hooks\n\n    !!! warning \"Deprecated\"\n\n        This method is deprecated, use `register_on_new_attr_hook` instead.\n\n    See: [`TaskAttrHooksContainer`][dp3.task_processing.task_hooks.TaskAttrHooksContainer]\n    in `task_hooks.py`\n    \"\"\"\n    self._task_executor.register_attr_hook(hook_type, hook, entity, attr)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_timeseries_hook","title":"register_timeseries_hook","text":"<pre><code>register_timeseries_hook(hook: Callable[[str, str, list[dict]], list[DataPointTask]], entity_type: str, attr_type: str)\n</code></pre> <p>Registers passed timeseries hook to be called during snapshot creation.</p> <p>Binds hook to specified <code>entity_type</code> and <code>attr_type</code> (though same hook can be bound multiple times).</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[str, str, list[dict]], list[DataPointTask]]</code> <p><code>hook</code> callable should expect entity_type, attr_type and attribute history as arguments and return a list of <code>DataPointTask</code> objects.</p> required <code>entity_type</code> <code>str</code> <p>specifies entity type</p> required <code>attr_type</code> <code>str</code> <p>specifies attribute type</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If entity_type and attr_type do not specify a valid timeseries attribute, a ValueError is raised.</p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_timeseries_hook(\n    self,\n    hook: Callable[[str, str, list[dict]], list[DataPointTask]],\n    entity_type: str,\n    attr_type: str,\n):\n    \"\"\"\n    Registers passed timeseries hook to be called during snapshot creation.\n\n    Binds hook to specified `entity_type` and `attr_type` (though same hook can be bound\n    multiple times).\n\n    Args:\n        hook: `hook` callable should expect entity_type, attr_type and attribute\n            history as arguments and return a list of `DataPointTask` objects.\n        entity_type: specifies entity type\n        attr_type: specifies attribute type\n\n    Raises:\n        ValueError: If entity_type and attr_type do not specify a valid timeseries attribute,\n            a ValueError is raised.\n    \"\"\"\n    self._snap_shooter.register_timeseries_hook(hook, entity_type, attr_type)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_correlation_hook","title":"register_correlation_hook","text":"<pre><code>register_correlation_hook(hook: Callable[[str, dict], Union[None, list[DataPointTask]]], entity_type: str, depends_on: list[list[str]], may_change: list[list[str]])\n</code></pre> <p>Registers passed hook to be called during snapshot creation.</p> <p>Binds hook to specified entity_type (though same hook can be bound multiple times).</p> <p><code>entity_type</code> and attribute specifications are validated, <code>ValueError</code> is raised on failure.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[str, dict], Union[None, list[DataPointTask]]]</code> <p><code>hook</code> callable should expect entity type as str and its current values, including linked entities, as dict. Can optionally return a list of DataPointTask objects to perform.</p> required <code>entity_type</code> <code>str</code> <p>specifies entity type</p> required <code>depends_on</code> <code>list[list[str]]</code> <p>each item should specify an attribute that is depended on in the form of a path from the specified entity_type to individual attributes (even on linked entities).</p> required <code>may_change</code> <code>list[list[str]]</code> <p>each item should specify an attribute that <code>hook</code> may change. specification format is identical to <code>depends_on</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>On failure of specification validation.</p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_correlation_hook(\n    self,\n    hook: Callable[[str, dict], Union[None, list[DataPointTask]]],\n    entity_type: str,\n    depends_on: list[list[str]],\n    may_change: list[list[str]],\n):\n    \"\"\"\n    Registers passed hook to be called during snapshot creation.\n\n    Binds hook to specified entity_type (though same hook can be bound multiple times).\n\n    `entity_type` and attribute specifications are validated, `ValueError` is raised on failure.\n\n    Args:\n        hook: `hook` callable should expect entity type as str\n            and its current values, including linked entities, as dict.\n            Can optionally return a list of DataPointTask objects to perform.\n        entity_type: specifies entity type\n        depends_on: each item should specify an attribute that is depended on\n            in the form of a path from the specified entity_type to individual attributes\n            (even on linked entities).\n        may_change: each item should specify an attribute that `hook` may change.\n            specification format is identical to `depends_on`.\n\n    Raises:\n        ValueError: On failure of specification validation.\n    \"\"\"\n    self._snap_shooter.register_correlation_hook(hook, entity_type, depends_on, may_change)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_snapshot_init_hook","title":"register_snapshot_init_hook","text":"<pre><code>register_snapshot_init_hook(hook: Callable[[], list[DataPointTask]])\n</code></pre> <p>Registers passed hook to be called before a run of snapshot creation begins.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[], list[DataPointTask]]</code> <p><code>hook</code> callable should expect no arguments and return a list of DataPointTask objects to perform.</p> required Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_snapshot_init_hook(self, hook: Callable[[], list[DataPointTask]]):\n    \"\"\"\n    Registers passed hook to be called before a run of snapshot creation begins.\n\n    Args:\n        hook: `hook` callable should expect no arguments and\n            return a list of DataPointTask objects to perform.\n    \"\"\"\n    self._snap_shooter.register_run_init_hook(hook)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_snapshot_finalize_hook","title":"register_snapshot_finalize_hook","text":"<pre><code>register_snapshot_finalize_hook(hook: Callable[[], list[DataPointTask]])\n</code></pre> <p>Registers passed hook to be called after a run of snapshot creation ends.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[], list[DataPointTask]]</code> <p><code>hook</code> callable should expect no arguments and return a list of DataPointTask objects to perform.</p> required Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_snapshot_finalize_hook(self, hook: Callable[[], list[DataPointTask]]):\n    \"\"\"\n    Registers passed hook to be called after a run of snapshot creation ends.\n\n    Args:\n        hook: `hook` callable should expect no arguments and\n            return a list of DataPointTask objects to perform.\n    \"\"\"\n    self._snap_shooter.register_run_finalize_hook(hook)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_periodic_update_hook","title":"register_periodic_update_hook","text":"<pre><code>register_periodic_update_hook(hook: Callable[[str, AnyEidT, dict], list[DataPointTask]], hook_id: str, entity_type: str, period: ParsedTimedelta)\n</code></pre> <p>Registers a callback for periodic update of entities of the specified type.</p> <p>The callback receives the entity type, the entity ID and the master record.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[str, AnyEidT, dict], list[DataPointTask]]</code> <p><code>hook</code> callable should expect entity type, entity ID and master record as arguments and return a list of DataPointTask objects to perform.</p> required <code>hook_id</code> <code>str</code> <p>specifies hook ID</p> required <code>entity_type</code> <code>str</code> <p>specifies entity type</p> required <code>period</code> <code>ParsedTimedelta</code> <p>specifies period of the callback</p> required Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_periodic_update_hook(\n    self,\n    hook: Callable[[str, AnyEidT, dict], list[DataPointTask]],\n    hook_id: str,\n    entity_type: str,\n    period: ParsedTimedelta,\n):\n    \"\"\"\n    Registers a callback for periodic update of entities of the specified type.\n\n    The callback receives the entity type, the entity ID and the master record.\n\n    Args:\n        hook: `hook` callable should expect entity type, entity ID and master record\n            as arguments and return a list of DataPointTask objects to perform.\n        hook_id: specifies hook ID\n        entity_type: specifies entity type\n        period: specifies period of the callback\n    \"\"\"\n    self._updater.register_record_update_hook(hook, hook_id, entity_type, period)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.CallbackRegistrar.register_periodic_eid_update_hook","title":"register_periodic_eid_update_hook","text":"<pre><code>register_periodic_eid_update_hook(hook: Callable[[str, AnyEidT], list[DataPointTask]], hook_id: str, entity_type: str, period: ParsedTimedelta)\n</code></pre> <p>Registers a callback for periodic update of entities of the specified type.</p> <p>The callback receives the entity type and the entity ID.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[str, AnyEidT], list[DataPointTask]]</code> <p><code>hook</code> callable should expect entity type and entity ID as arguments and return a list of DataPointTask objects to perform.</p> required <code>hook_id</code> <code>str</code> <p>specifies hook ID</p> required <code>entity_type</code> <code>str</code> <p>specifies entity type</p> required <code>period</code> <code>ParsedTimedelta</code> <p>specifies period of the callback</p> required Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def register_periodic_eid_update_hook(\n    self,\n    hook: Callable[[str, AnyEidT], list[DataPointTask]],\n    hook_id: str,\n    entity_type: str,\n    period: ParsedTimedelta,\n):\n    \"\"\"\n    Registers a callback for periodic update of entities of the specified type.\n\n    The callback receives the entity type and the entity ID.\n\n    Args:\n        hook: `hook` callable should expect entity type and entity ID as arguments\n            and return a list of DataPointTask objects to perform.\n        hook_id: specifies hook ID\n        entity_type: specifies entity type\n        period: specifies period of the callback\n    \"\"\"\n    self._updater.register_eid_update_hook(hook, hook_id, entity_type, period)\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.on_entity_creation_in_snapshots","title":"on_entity_creation_in_snapshots","text":"<pre><code>on_entity_creation_in_snapshots(model_spec: ModelSpec, run_flag: SharedFlag, original_hook: Callable[[AnyEidT, DataPointTask], list[DataPointTask]], etype: str, record: dict) -&gt; list[DataPointTask]\n</code></pre> <p>Wrapper for on_entity_creation hooks to enable running as a snapshot callback.</p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def on_entity_creation_in_snapshots(\n    model_spec: ModelSpec,\n    run_flag: SharedFlag,\n    original_hook: Callable[[AnyEidT, DataPointTask], list[DataPointTask]],\n    etype: str,\n    record: dict,\n) -&gt; list[DataPointTask]:\n    \"\"\"Wrapper for on_entity_creation hooks to enable running as a snapshot callback.\"\"\"\n    if not run_flag.isset():\n        return []\n    eid = record[\"eid\"]\n    mock_task = DataPointTask(etype=etype, eid=eid, data_points=[])\n    tasks = original_hook(eid, mock_task)\n    write_datapoints_into_record(model_spec, tasks, record)\n    return tasks\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.on_attr_change_in_snapshots","title":"on_attr_change_in_snapshots","text":"<pre><code>on_attr_change_in_snapshots(model_spec: ModelSpec, run_flag: SharedFlag, original_hook: Callable[[AnyEidT, DataPointTask], Union[list[DataPointTask], None]], etype: str, record: dict) -&gt; list[DataPointTask]\n</code></pre> <p>Wrapper for on_entity_creation hooks to enable running as a snapshot callback.</p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def on_attr_change_in_snapshots(\n    model_spec: ModelSpec,\n    run_flag: SharedFlag,\n    original_hook: Callable[[AnyEidT, DataPointTask], Union[list[DataPointTask], None]],\n    etype: str,\n    record: dict,\n) -&gt; list[DataPointTask]:\n    \"\"\"Wrapper for on_entity_creation hooks to enable running as a snapshot callback.\"\"\"\n    if not run_flag.isset():\n        return []\n    eid = record[\"eid\"]\n    mock_task = DataPointTask(etype=etype, eid=eid, data_points=[])\n    tasks = original_hook(eid, mock_task)\n    if isinstance(tasks, list):\n        write_datapoints_into_record(model_spec, tasks, record)\n    return tasks\n</code></pre>"},{"location":"reference/common/callback_registrar/#dp3.common.callback_registrar.reload_module_config","title":"reload_module_config","text":"<pre><code>reload_module_config(log: Logger, platform_config: PlatformConfig, modules: dict, module: str) -&gt; None\n</code></pre> <p>Reloads configuration of a module.</p> <p>Parameters:</p> Name Type Description Default <code>log</code> <code>Logger</code> <p>log to write messages to</p> required <code>platform_config</code> <code>PlatformConfig</code> <p>Platform configuration</p> required <code>modules</code> <code>dict</code> <p>Dictionary of loaded modules by their names</p> required <code>module</code> <code>str</code> <p>Name of the module to reload</p> required <p>Returns:</p> Type Description <code>None</code> <p>Module's configuration</p> Source code in <code>dp3/common/callback_registrar.py</code> <pre><code>def reload_module_config(\n    log: Logger, platform_config: PlatformConfig, modules: dict, module: str\n) -&gt; None:\n    \"\"\"\n    Reloads configuration of a module.\n\n    Args:\n        log: log to write messages to\n        platform_config: Platform configuration\n        modules: Dictionary of loaded modules by their names\n        module: Name of the module to reload\n\n    Returns:\n        Module's configuration\n    \"\"\"\n    log.debug(f\"Reloading config of module '{module}'\")\n\n    if module not in modules:\n        log.warning(f\"Could not find module '{module}', cannot reload config. Is it loaded?\")\n        return\n\n    config = read_config_dir(platform_config.config_base_path, recursive=True)\n    module_config = config.get(f\"modules.{module}\", {})\n    modules[module].load_config(platform_config, module_config)\n    modules[module].refresh.set()\n    log.info(f\"Config of module '{module}' reloaded successfully.\")\n</code></pre>"},{"location":"reference/common/config/","title":"config","text":""},{"location":"reference/common/config/#dp3.common.config","title":"dp3.common.config","text":"<p>Platform config file reader and config model.</p>"},{"location":"reference/common/config/#dp3.common.config.HierarchicalDict","title":"HierarchicalDict","text":"<p>               Bases: <code>dict</code></p> <p>Extension of built-in <code>dict</code> that simplifies working with a nested hierarchy of dicts.</p>"},{"location":"reference/common/config/#dp3.common.config.HierarchicalDict.get","title":"get","text":"<pre><code>get(key, default=NoDefault)\n</code></pre> <p>Key may be a path (in dot notation) into a hierarchy of dicts. For example   <code>dictionary.get('abc.x.y')</code> is equivalent to   <code>dictionary['abc']['x']['y']</code>.</p> <p>:returns: <code>self[key]</code> or <code>default</code> if key is not found.</p> Source code in <code>dp3/common/config.py</code> <pre><code>def get(self, key, default=NoDefault):\n    \"\"\"\n    Key may be a path (in dot notation) into a hierarchy of dicts. For example\n      `dictionary.get('abc.x.y')`\n    is equivalent to\n      `dictionary['abc']['x']['y']`.\n\n    :returns: `self[key]` or `default` if key is not found.\n    \"\"\"\n    d = self\n    try:\n        while \".\" in key:\n            first_key, key = key.split(\".\", 1)\n            d = d[first_key]\n        return d[key]\n    except (KeyError, TypeError):\n        pass  # not found - continue below\n    if default is NoDefault:\n        raise MissingConfigError(\"Mandatory configuration element is missing: \" + key)\n    else:\n        return default\n</code></pre>"},{"location":"reference/common/config/#dp3.common.config.HierarchicalDict.update","title":"update","text":"<pre><code>update(other, **kwargs)\n</code></pre> <p>Update <code>HierarchicalDict</code> with other dictionary and merge common keys.</p> <p>If there is a key in both current and the other dictionary and values of both keys are dictionaries, they are merged together.</p> <p>Example: <pre><code>HierarchicalDict({'a': {'b': 1, 'c': 2}}).update({'a': {'b': 10, 'd': 3}})\n-&gt;\nHierarchicalDict({'a': {'b': 10, 'c': 2, 'd': 3}})\n</code></pre> Changes the dictionary directly, returns <code>None</code>.</p> Source code in <code>dp3/common/config.py</code> <pre><code>def update(self, other, **kwargs):\n    \"\"\"\n    Update `HierarchicalDict` with other dictionary and merge common keys.\n\n    If there is a key in both current and the other dictionary and values of\n    both keys are dictionaries, they are merged together.\n\n    Example:\n    ```\n    HierarchicalDict({'a': {'b': 1, 'c': 2}}).update({'a': {'b': 10, 'd': 3}})\n    -&gt;\n    HierarchicalDict({'a': {'b': 10, 'c': 2, 'd': 3}})\n    ```\n    Changes the dictionary directly, returns `None`.\n    \"\"\"\n    other = dict(other)\n    for key in other:\n        if key in self:\n            if isinstance(self[key], dict) and isinstance(other[key], dict):\n                # The key is present in both dicts and both key values are dicts -&gt; merge them\n                HierarchicalDict.update(self[key], other[key])\n            else:\n                # One of the key values is not a dict -&gt; overwrite the value\n                # in self by the one from other (like normal \"update\" does)\n                self[key] = other[key]\n        else:\n            # key is not present in self -&gt; set it to value from other\n            self[key] = other[key]\n</code></pre>"},{"location":"reference/common/config/#dp3.common.config.CronExpression","title":"CronExpression","text":"<p>               Bases: <code>BaseModel</code></p> <p>Cron expression used for scheduling. Also support standard cron expressions, such as</p> <ul> <li>\"*/15\" (every 15 units)</li> <li>\"1,2,3\" (1, 2 and 3)</li> <li>\"1-3\" (1, 2 and 3)</li> </ul> <p>Attributes:</p> Name Type Description <code>year</code> <code>Optional[str]</code> <p>4-digit year</p> <code>month</code> <code>Optional[int]</code> <p>month (1-12)</p> <code>day</code> <code>Optional[Union[Annotated[int, Field(ge=1, le=31)], CronStr]]</code> <p>day of month (1-31)</p> <code>week</code> <code>Optional[int]</code> <p>ISO week (1-53)</p> <code>day_of_week</code> <code>Optional[Union[Annotated[int, Field(ge=0, le=6)], CronStr]]</code> <p>number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun)</p> <code>hour</code> <code>Optional[Union[TimeInt, CronStr]]</code> <p>hour (0-23)</p> <code>minute</code> <code>Optional[Union[TimeInt, CronStr]]</code> <p>minute (0-59)</p> <code>second</code> <code>Optional[Union[TimeInt, CronStr]]</code> <p>second (0-59)</p> <code>timezone</code> <code>str</code> <p>Timezone for time specification (default is UTC).</p>"},{"location":"reference/common/config/#dp3.common.config.EntitySpecDict","title":"EntitySpecDict","text":"<p>               Bases: <code>BaseModel</code></p> <p>Class representing full specification of an entity.</p> <p>Attributes:</p> Name Type Description <code>entity</code> <code>EntitySpec</code> <p>Specification and settings of entity itself.</p> <code>attribs</code> <code>dict[str, AttrSpecType]</code> <p>A mapping of attribute id -&gt; AttrSpec</p>"},{"location":"reference/common/config/#dp3.common.config.ModelSpec","title":"ModelSpec","text":"<pre><code>ModelSpec(config: HierarchicalDict)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Class representing the platform's current entity and attribute specification.</p> <p>Attributes:</p> Name Type Description <code>config</code> <code>dict[str, EntitySpecDict]</code> <p>Legacy config format, exactly mirrors the config files.</p> <code>entities</code> <code>dict[str, EntitySpec]</code> <p>Mapping of entity id -&gt; EntitySpec</p> <code>attributes</code> <code>dict[tuple[str, str], AttrSpecType]</code> <p>Mapping of (entity id, attribute id) -&gt; AttrSpec</p> <code>entity_attributes</code> <code>dict[str, dict[str, AttrSpecType]]</code> <p>Mapping of entity id -&gt; attribute id -&gt; AttrSpec</p> <code>relations</code> <code>dict[tuple[str, str], AttrSpecType]</code> <p>Mapping of (entity id, attribute id) -&gt; AttrSpec only contains attributes which are relations.</p> <p>Provided configuration must be a dict of following structure: <pre><code>{\n    &lt;entity type&gt;: {\n        'entity': {\n            entity specification\n        },\n        'attribs': {\n            &lt;attr id&gt;: {\n                attribute specification\n            },\n            other attributes\n        }\n    },\n    other entity types\n}\n</code></pre> Raises:     ValueError: if the specification is invalid.</p> Source code in <code>dp3/common/config.py</code> <pre><code>def __init__(self, config: HierarchicalDict):\n    \"\"\"\n    Provided configuration must be a dict of following structure:\n    ```\n    {\n        &lt;entity type&gt;: {\n            'entity': {\n                entity specification\n            },\n            'attribs': {\n                &lt;attr id&gt;: {\n                    attribute specification\n                },\n                other attributes\n            }\n        },\n        other entity types\n    }\n    ```\n    Raises:\n        ValueError: if the specification is invalid.\n    \"\"\"\n    super().__init__(\n        config=config, entities={}, attributes={}, entity_attributes={}, relations={}\n    )\n</code></pre>"},{"location":"reference/common/config/#dp3.common.config.PlatformConfig","title":"PlatformConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>An aggregation of configuration available to modules.</p> <p>Attributes:</p> Name Type Description <code>app_name</code> <code>str</code> <p>Name of the application, used when naming various structures of the platform</p> <code>config_base_path</code> <code>str</code> <p>Path to directory containing platform config</p> <code>config</code> <code>HierarchicalDict</code> <p>A dictionary that contains the platform config</p> <code>model_spec</code> <code>ModelSpec</code> <p>Specification of the platform's model (entities and attributes)</p> <code>num_processes</code> <code>PositiveInt</code> <p>Number of worker processes</p> <code>process_index</code> <code>NonNegativeInt</code> <p>Index of current process</p>"},{"location":"reference/common/config/#dp3.common.config.read_config","title":"read_config","text":"<pre><code>read_config(filepath: str) -&gt; HierarchicalDict\n</code></pre> <p>Read configuration file and return config as a dict-like object.</p> <p>The configuration file should contain a valid YAML - Comments may be included as lines starting with <code>#</code> (optionally preceded   by whitespaces).</p> <p>This function reads the file and converts it to a <code>HierarchicalDict</code>. The only difference from built-in <code>dict</code> is its <code>get</code> method, which allows hierarchical keys (e.g. <code>abc.x.y</code>). See doc of get method for more information.</p> Source code in <code>dp3/common/config.py</code> <pre><code>def read_config(filepath: str) -&gt; HierarchicalDict:\n    \"\"\"\n    Read configuration file and return config as a dict-like object.\n\n    The configuration file should contain a valid YAML\n    - Comments may be included as lines starting with `#` (optionally preceded\n      by whitespaces).\n\n    This function reads the file and converts it to a `HierarchicalDict`.\n    The only difference from built-in `dict` is its `get` method, which allows\n    hierarchical keys (e.g. `abc.x.y`).\n    See [doc of get method][dp3.common.config.HierarchicalDict.get] for more information.\n    \"\"\"\n    with open(filepath) as file_content:\n        return HierarchicalDict(yaml.safe_load(file_content))\n</code></pre>"},{"location":"reference/common/config/#dp3.common.config.read_config_dir","title":"read_config_dir","text":"<pre><code>read_config_dir(dir_path: str, recursive: bool = False) -&gt; HierarchicalDict\n</code></pre> <p>Same as read_config, but it loads whole configuration directory of YAML files, so only files ending with \".yml\" are loaded. Each loaded configuration is located under key named after configuration filename.</p> <p>Parameters:</p> Name Type Description Default <code>dir_path</code> <code>str</code> <p>Path to read config from.</p> required <code>recursive</code> <code>bool</code> <p>If <code>recursive</code> is set, then the configuration directory will be read recursively (including configuration files inside directories).</p> <code>False</code> Source code in <code>dp3/common/config.py</code> <pre><code>def read_config_dir(dir_path: str, recursive: bool = False) -&gt; HierarchicalDict:\n    \"\"\"\n    Same as [read_config][dp3.common.config.read_config],\n    but it loads whole configuration directory of YAML files,\n    so only files ending with \".yml\" are loaded.\n    Each loaded configuration is located under key named after configuration filename.\n\n    Args:\n        dir_path: Path to read config from.\n        recursive: If `recursive` is set, then the configuration directory will be read\n            recursively (including configuration files inside directories).\n    \"\"\"\n    all_files_paths = os.listdir(dir_path)\n    config = HierarchicalDict()\n    for config_filename in all_files_paths:\n        config_full_path = os.path.join(dir_path, config_filename)\n        if os.path.isdir(config_full_path) and recursive:\n            loaded_config = read_config_dir(config_full_path, recursive)\n        elif os.path.isfile(config_full_path) and config_filename.endswith(\".yml\"):\n            try:\n                loaded_config = read_config(config_full_path)\n            except TypeError:\n                # configuration file is empty\n                continue\n            # remove '.yml' suffix of filename\n            config_filename = config_filename[:-4]\n        else:\n            continue\n        # place configuration files into another dictionary level named by config dictionary name\n        config[config_filename] = loaded_config\n    return config\n</code></pre>"},{"location":"reference/common/config/#dp3.common.config.entity_type_context","title":"entity_type_context","text":"<pre><code>entity_type_context(model_spec: ModelSpec) -&gt; Iterator[None]\n</code></pre> <p>Context manager for AttrSpec initialization.</p> Source code in <code>dp3/common/config.py</code> <pre><code>@contextmanager\ndef entity_type_context(model_spec: ModelSpec) -&gt; Iterator[None]:\n    \"\"\"Context manager for AttrSpec initialization.\"\"\"\n    token = _init_entity_type_context_var.set(\n        {entity: spec.id_data_type.root for entity, spec in model_spec.entities.items()}\n    )\n    try:\n        yield\n    finally:\n        _init_entity_type_context_var.reset(token)\n</code></pre>"},{"location":"reference/common/config/#dp3.common.config.get_entity_type_context","title":"get_entity_type_context","text":"<pre><code>get_entity_type_context() -&gt; dict\n</code></pre> <p>Get entity spec context.</p> Source code in <code>dp3/common/config.py</code> <pre><code>def get_entity_type_context() -&gt; dict:\n    \"\"\"Get entity spec context.\"\"\"\n    cxt = _init_entity_type_context_var.get()\n    if cxt is None or not isinstance(cxt, dict):\n        raise ValueError(\"Entity type context is not set\")\n    return cxt\n</code></pre>"},{"location":"reference/common/context/","title":"context","text":""},{"location":"reference/common/context/#dp3.common.context","title":"dp3.common.context","text":""},{"location":"reference/common/context/#dp3.common.context.entity_context","title":"entity_context","text":"<pre><code>entity_context(self_spec, entities: dict) -&gt; Iterator[None]\n</code></pre> <p>Context manager for AttrSpec initialization.</p> Source code in <code>dp3/common/context.py</code> <pre><code>@contextmanager\ndef entity_context(self_spec, entities: dict) -&gt; Iterator[None]:\n    \"\"\"Context manager for AttrSpec initialization.\"\"\"\n    token = _init_attr_spec_context_var.set({\"self\": self_spec, \"entities\": entities})\n    try:\n        yield\n    finally:\n        _init_attr_spec_context_var.reset(token)\n</code></pre>"},{"location":"reference/common/context/#dp3.common.context.get_entity_context","title":"get_entity_context","text":"<pre><code>get_entity_context() -&gt; dict\n</code></pre> <p>Get entity spec context.</p> Source code in <code>dp3/common/context.py</code> <pre><code>def get_entity_context() -&gt; dict:\n    \"\"\"Get entity spec context.\"\"\"\n    cxt = _init_attr_spec_context_var.get()\n    if cxt is None or not isinstance(cxt, dict):\n        raise ValueError(\"Entity spec context is not set\")\n    return cxt\n</code></pre>"},{"location":"reference/common/control/","title":"control","text":""},{"location":"reference/common/control/#dp3.common.control","title":"dp3.common.control","text":"<p>Module enabling remote control of the platform's internal events.</p>"},{"location":"reference/common/control/#dp3.common.control.Control","title":"Control","text":"<pre><code>Control(platform_config: PlatformConfig)\n</code></pre> <p>Class enabling remote control of the platform's internal events.</p> Source code in <code>dp3/common/control.py</code> <pre><code>def __init__(\n    self,\n    platform_config: PlatformConfig,\n) -&gt; None:\n    self.log = logging.getLogger(\"Control\")\n    self.action_handlers: dict[ControlAction, Callable] = {}\n\n    self.config = ControlConfig.model_validate(platform_config.config.get(\"control\"))\n    self.allowed_actions = set(self.config.allowed_actions)\n    self.log.debug(\"Allowed actions: %s\", self.allowed_actions)\n\n    queue = f\"{platform_config.app_name}-worker-{platform_config.process_index}-control\"\n    self.control_queue = TaskQueueReader(\n        callback=self.process_control_task,\n        parse_task=ControlMessage.model_validate_json,\n        app_name=platform_config.app_name,\n        worker_index=platform_config.process_index,\n        rabbit_config=platform_config.config.get(\"processing_core.msg_broker\", {}),\n        queue=queue,\n        priority_queue=False,\n        parent_logger=self.log,\n    )\n</code></pre>"},{"location":"reference/common/control/#dp3.common.control.Control.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Connect to RabbitMQ and start consuming from TaskQueue.</p> Source code in <code>dp3/common/control.py</code> <pre><code>def start(self):\n    \"\"\"Connect to RabbitMQ and start consuming from TaskQueue.\"\"\"\n    unconfigured_handlers = self.allowed_actions - set(self.action_handlers)\n    if unconfigured_handlers:\n        raise ValueError(\n            f\"The following configured actions are missing handlers: {unconfigured_handlers}\"\n        )\n\n    self.log.info(\"Connecting to RabbitMQ\")\n    self.control_queue.connect()\n    self.control_queue.check()  # check presence of needed queues\n    self.control_queue.start()\n\n    self.log.debug(\n        \"Configured handlers: %s\", \", \".join(get_func_name(f) for f in self.action_handlers)\n    )\n</code></pre>"},{"location":"reference/common/control/#dp3.common.control.Control.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop consuming from TaskQueue, disconnect from RabbitMQ.</p> Source code in <code>dp3/common/control.py</code> <pre><code>def stop(self):\n    \"\"\"Stop consuming from TaskQueue, disconnect from RabbitMQ.\"\"\"\n    self.control_queue.stop()\n    self.control_queue.disconnect()\n</code></pre>"},{"location":"reference/common/control/#dp3.common.control.Control.set_action_handler","title":"set_action_handler","text":"<pre><code>set_action_handler(action: ControlAction, handler: Callable)\n</code></pre> <p>Sets the handler for the given action</p> Source code in <code>dp3/common/control.py</code> <pre><code>def set_action_handler(self, action: ControlAction, handler: Callable):\n    \"\"\"Sets the handler for the given action\"\"\"\n    self.log.debug(\"Setting handler for action %s: %s\", action, get_func_name(handler))\n    self.action_handlers[action] = handler\n</code></pre>"},{"location":"reference/common/control/#dp3.common.control.Control.process_control_task","title":"process_control_task","text":"<pre><code>process_control_task(msg_id, task: ControlMessage)\n</code></pre> <p>Acknowledges the received message and executes an action according to the <code>task</code>.</p> <p>This function should not be called directly, but set as callback for TaskQueueReader.</p> Source code in <code>dp3/common/control.py</code> <pre><code>def process_control_task(self, msg_id, task: ControlMessage):\n    \"\"\"\n    Acknowledges the received message and executes an action according to the `task`.\n\n    This function should not be called directly, but set as callback for TaskQueueReader.\n    \"\"\"\n    if not self.control_queue.ack(msg_id):\n        return\n    if task.action in self.allowed_actions:\n        self.log.info(\"Executing action: %s\", task.action)\n        self.action_handlers[task.action](**task.kwargs)\n        self.log.info(\"Action finished: %s\", task.action)\n    else:\n        self.log.error(\"Action not allowed: %s\", task.action)\n</code></pre>"},{"location":"reference/common/control/#dp3.common.control.refresh_on_entity_creation","title":"refresh_on_entity_creation","text":"<pre><code>refresh_on_entity_creation(task_distributor: TaskDistributor, task_executor: TaskExecutor, etype: str)\n</code></pre> <p>Refreshes hooks called on new entity creation for all entities in DB.</p> Source code in <code>dp3/common/control.py</code> <pre><code>def refresh_on_entity_creation(\n    task_distributor: TaskDistributor, task_executor: TaskExecutor, etype: str\n):\n    \"\"\"Refreshes hooks called on new entity creation for all entities in DB.\"\"\"\n    tasks = task_executor.refresh_on_entity_creation(\n        etype=etype,\n        worker_id=task_distributor.process_index,\n        worker_cnt=task_distributor.num_processes,\n    )\n    task_distributor.push_new_tasks(tasks)\n</code></pre>"},{"location":"reference/common/datapoint/","title":"datapoint","text":""},{"location":"reference/common/datapoint/#dp3.common.datapoint","title":"dp3.common.datapoint","text":""},{"location":"reference/common/datapoint/#dp3.common.datapoint.DataPointBase","title":"DataPointBase","text":"<p>               Bases: <code>BaseModel</code></p> <p>Data-point</p> <p>Contains single raw data value received on API. This is just base class - plain, observation or timeseries datapoints inherit from this class (see below).</p> <p>Provides front line of validation for this data value.</p> <p>Internal usage: inside Task, created by TaskExecutor</p>"},{"location":"reference/common/datapoint/#dp3.common.datapoint.DataPointPlainBase","title":"DataPointPlainBase","text":"<p>               Bases: <code>DataPointBase</code></p> <p>Plain attribute data-point</p> <p>Contains single raw data value received on API for plain attribute.</p> <p>In case of plain data-point, it's not really a data-point, but we use the same naming for simplicity.</p>"},{"location":"reference/common/datapoint/#dp3.common.datapoint.DataPointObservationsBase","title":"DataPointObservationsBase","text":"<p>               Bases: <code>DataPointBase</code></p> <p>Observations attribute data-point</p> <p>Contains single raw data value received on API for observations attribute.</p>"},{"location":"reference/common/datapoint/#dp3.common.datapoint.DataPointTimeseriesBase","title":"DataPointTimeseriesBase","text":"<p>               Bases: <code>DataPointBase</code></p> <p>Timeseries attribute data-point</p> <p>Contains single raw data value received on API for observations attribute.</p>"},{"location":"reference/common/datapoint/#dp3.common.datapoint.ignore_value","title":"ignore_value","text":"<pre><code>ignore_value(_v)\n</code></pre> <p>Ignore the passed value and return None.</p> Source code in <code>dp3/common/datapoint.py</code> <pre><code>def ignore_value(_v):\n    \"\"\"Ignore the passed value and return None.\"\"\"\n    return None\n</code></pre>"},{"location":"reference/common/datapoint/#dp3.common.datapoint.is_list_ordered","title":"is_list_ordered","text":"<pre><code>is_list_ordered(to_check: list)\n</code></pre> <p>Checks if list is ordered (not decreasing anywhere)</p> Source code in <code>dp3/common/datapoint.py</code> <pre><code>def is_list_ordered(to_check: list):\n    \"\"\"Checks if list is ordered (not decreasing anywhere)\"\"\"\n    return all(to_check[i] &lt;= to_check[i + 1] for i in range(len(to_check) - 1))\n</code></pre>"},{"location":"reference/common/datapoint/#dp3.common.datapoint.dp_ts_v_validator","title":"dp_ts_v_validator","text":"<pre><code>dp_ts_v_validator(v)\n</code></pre> <p>Check if all value arrays are the same length.</p> Source code in <code>dp3/common/datapoint.py</code> <pre><code>def dp_ts_v_validator(v):\n    \"\"\"Check if all value arrays are the same length.\"\"\"\n    values_len = {len(v_i) for _, v_i in v.model_dump().items()}\n    assert len(values_len) == 1, f\"Series values have different lengths: {values_len}\"\n\n    return v\n</code></pre>"},{"location":"reference/common/datapoint/#dp3.common.datapoint.dp_ts_root_validator_irregular","title":"dp_ts_root_validator_irregular","text":"<pre><code>dp_ts_root_validator_irregular(self)\n</code></pre> <p>Validates or sets t2 of irregular timeseries datapoint</p> Source code in <code>dp3/common/datapoint.py</code> <pre><code>def dp_ts_root_validator_irregular(self):\n    \"\"\"Validates or sets t2 of irregular timeseries datapoint\"\"\"\n    first_time = self.v.time[0]\n    last_time = self.v.time[-1]\n\n    # Check t1 &lt;= first_time\n    assert self.t1 &lt;= first_time, f\"'t1' is above first item in 'time' series ({first_time})\"\n\n    # Check last_time &lt;= t2\n    if self.t2:\n        assert self.t2 &gt;= last_time, f\"'t2' is below last item in 'time' series ({last_time})\"\n    else:\n        self.t2 = last_time\n\n    # time must be ordered\n    assert is_list_ordered(self.v.time), \"'time' series is not ordered\"\n\n    return self\n</code></pre>"},{"location":"reference/common/datapoint/#dp3.common.datapoint.dp_ts_root_validator_irregular_intervals","title":"dp_ts_root_validator_irregular_intervals","text":"<pre><code>dp_ts_root_validator_irregular_intervals(self)\n</code></pre> <p>Validates or sets t2 of irregular intervals timeseries datapoint</p> Source code in <code>dp3/common/datapoint.py</code> <pre><code>def dp_ts_root_validator_irregular_intervals(self):\n    \"\"\"Validates or sets t2 of irregular intervals timeseries datapoint\"\"\"\n    first_time = self.v.time_first[0]\n    last_time = self.v.time_last[-1]\n\n    # Check t1 &lt;= first_time\n    assert self.t1 &lt;= first_time, f\"'t1' is above first item in 'time_first' series ({first_time})\"\n\n    # Check last_time &lt;= t2\n    if self.t2:\n        assert self.t2 &gt;= last_time, f\"'t2' is below last item in 'time_last' series ({last_time})\"\n    else:\n        self.t2 = last_time\n\n    # Check time_first[i] &lt;= time_last[i]\n    assert all(\n        t[0] &lt;= t[1] for t in zip(self.v.time_first, self.v.time_last)\n    ), \"'time_first[i] &lt;= time_last[i]' isn't true for all 'i'\"\n\n    return self\n</code></pre>"},{"location":"reference/common/datatype/","title":"datatype","text":""},{"location":"reference/common/datatype/#dp3.common.datatype","title":"dp3.common.datatype","text":""},{"location":"reference/common/datatype/#dp3.common.datatype.AnyEidT","title":"AnyEidT  <code>module-attribute</code>","text":"<pre><code>AnyEidT = Union[str, int, IPv4Address, IPv6Address, MACAddress]\n</code></pre> <p>Type alias for any of possible entity ID data types.</p> <p>Note that the type is determined based on the loaded entity configuration and in most cases is only one of the options, based on what entity is being processed.</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.ReadOnly","title":"ReadOnly","text":"<p>               Bases: <code>BaseModel</code></p> <p>The ReadOnly data_type is used to avoid datapoint insertion for an attribute.</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType","title":"DataType","text":"<p>               Bases: <code>RootModel</code></p> <p>Data type container</p> <p>Represents one of primitive data types:</p> <ul> <li>tag</li> <li>binary</li> <li>string</li> <li>int</li> <li>int64</li> <li>float</li> <li>ipv4</li> <li>ipv6</li> <li>mac</li> <li>time</li> <li>special</li> <li>json</li> </ul> <p>or composite data type:</p> <ul> <li>link <li>array <li>set <li>dict <li>category"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.data_type","title":"data_type  <code>property</code>","text":"<pre><code>data_type: Union[type, BaseModel]\n</code></pre> <p>Type for incoming value validation</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.type_info","title":"type_info  <code>property</code>","text":"<pre><code>type_info: str\n</code></pre> <p>String representation of the data type, immune to whitespace changes</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.hashable","title":"hashable  <code>property</code>","text":"<pre><code>hashable: bool\n</code></pre> <p>Whether contained data is hashable</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.iterable","title":"iterable  <code>property</code>","text":"<pre><code>iterable: bool\n</code></pre> <p>Whether the data type is iterable</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.elem_type","title":"elem_type  <code>property</code>","text":"<pre><code>elem_type: DataType\n</code></pre> <p>if <code>iterable</code>, the element data type</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.is_link","title":"is_link  <code>property</code>","text":"<pre><code>is_link: bool\n</code></pre> <p>Whether the data type is a link between entities</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.mirror_link","title":"mirror_link  <code>property</code>","text":"<pre><code>mirror_link: bool\n</code></pre> <p>If <code>is_link</code>, whether the link is mirrored</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.mirror_as","title":"mirror_as  <code>property</code>","text":"<pre><code>mirror_as: Union[str, None]\n</code></pre> <p>If <code>mirror_link</code>, what is the name of the mirrored attribute</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.link_to","title":"link_to  <code>property</code>","text":"<pre><code>link_to: str\n</code></pre> <p>If <code>is_link</code>, the target linked entity</p>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.determine_value_validator","title":"determine_value_validator","text":"<pre><code>determine_value_validator()\n</code></pre> <p>Determines value validator (inner <code>data_type</code>).</p> Source code in <code>dp3/common/datatype.py</code> <pre><code>@model_validator(mode=\"after\")\ndef determine_value_validator(self):\n    \"\"\"Determines value validator (inner `data_type`).\"\"\"\n    return self._determine_value_validator()\n</code></pre>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.get_linked_entity","title":"get_linked_entity","text":"<pre><code>get_linked_entity() -&gt; str\n</code></pre> <p>Returns linked entity id. Raises ValueError if DataType is not a link.</p> Source code in <code>dp3/common/datatype.py</code> <pre><code>def get_linked_entity(self) -&gt; str:\n    \"\"\"Returns linked entity id. Raises ValueError if DataType is not a link.\"\"\"\n    try:\n        return self._link_to\n    except AttributeError:\n        raise ValueError(f\"DataType '{self}' is not a link.\") from None\n</code></pre>"},{"location":"reference/common/datatype/#dp3.common.datatype.DataType.link_has_data","title":"link_has_data","text":"<pre><code>link_has_data() -&gt; bool\n</code></pre> <p>Whether link has data. Raises ValueError if DataType is not a link.</p> Source code in <code>dp3/common/datatype.py</code> <pre><code>def link_has_data(self) -&gt; bool:\n    \"\"\"Whether link has data. Raises ValueError if DataType is not a link.\"\"\"\n    try:\n        return self._link_data\n    except AttributeError:\n        raise ValueError(f\"DataType '{self}' is not a link.\") from None\n</code></pre>"},{"location":"reference/common/datatype/#dp3.common.datatype.EidDataType","title":"EidDataType","text":"<p>               Bases: <code>DataType</code></p> <p>Data type container for entity id</p> <p>Represents one of primitive data types: - string - int - ipv4 - ipv6 - mac</p>"},{"location":"reference/common/entityspec/","title":"entityspec","text":""},{"location":"reference/common/entityspec/#dp3.common.entityspec","title":"dp3.common.entityspec","text":""},{"location":"reference/common/entityspec/#dp3.common.entityspec.ImmortalLifetime","title":"ImmortalLifetime","text":"<p>               Bases: <code>SpecModel</code></p> <p>Immortal lifetime specification.</p> <p>The entity is never deleted.</p>"},{"location":"reference/common/entityspec/#dp3.common.entityspec.TimeToLiveLifetime","title":"TimeToLiveLifetime","text":"<p>               Bases: <code>SpecModel</code></p> <p>TTL lifetime specification.</p> <p>The entity is deleted after all of its TTL tokens expire. TTL tokens can be attached to new data datapoints on per-attribute basis (see <code>AttrSpecGeneric</code>), set to mirror the lifetime of the data (<code>mirror_data</code>), or sent explicitly using the API (see <code>/entity/{etype}/{eid}/ttl</code>).</p> <p>Attributes:</p> Name Type Description <code>on_create</code> <code>ParsedTimedelta</code> <p>The base lifetime of an entity.</p> <code>mirror_data</code> <code>bool</code> <p>If <code>True</code> (default), the lifetime of the entity is extended by the <code>max_age</code> of the incoming observations and timeseries data-points.</p>"},{"location":"reference/common/entityspec/#dp3.common.entityspec.WeakLifetime","title":"WeakLifetime","text":"<p>               Bases: <code>SpecModel</code></p> <p>Weak entity lifetime specification</p>"},{"location":"reference/common/entityspec/#dp3.common.entityspec.EntitySpec","title":"EntitySpec","text":"<p>               Bases: <code>SpecModel</code></p> <p>Entity specification</p> <p>This class represents specification of an entity type (e.g. ip, asn, ...)</p> <p>Attributes:</p> Name Type Description <code>id</code> <code>str</code> <p>Entity type identifier</p> <code>id_data_type</code> <code>EidDataType</code> <p>Entity type identifier data type</p> <code>name</code> <code>str</code> <p>User-friendly entity type name for display</p> <code>snapshot</code> <code>bool</code> <p>If <code>True</code>, the entity type supports snapshots</p> <code>lifetime</code> <code>Union[ImmortalLifetime, TimeToLiveLifetime, WeakLifetime]</code> <p>Entity lifetime specification</p> <code>description</code> <code>str</code> <p>Entity type description</p>"},{"location":"reference/common/entityspec/#dp3.common.entityspec.EntitySpec.fill_validator","title":"fill_validator","text":"<pre><code>fill_validator()\n</code></pre> <p>Fill the <code>eid_validator</code> attribute.</p> Source code in <code>dp3/common/entityspec.py</code> <pre><code>@model_validator(mode=\"after\")\ndef fill_validator(self):\n    \"\"\"Fill the `eid_validator` attribute.\"\"\"\n    self._eid_validator = self.id_data_type.data_type\n    return self\n</code></pre>"},{"location":"reference/common/mac_address/","title":"mac_address","text":""},{"location":"reference/common/mac_address/#dp3.common.mac_address","title":"dp3.common.mac_address","text":""},{"location":"reference/common/mac_address/#dp3.common.mac_address.MACAddress","title":"MACAddress","text":"<pre><code>MACAddress(mac: Union[bytes, str, MACAddress])\n</code></pre> <p>Represents a MAC Address.</p> <p>Can be initialized from colon or comma separated string, or from raw bytes.</p> Source code in <code>dp3/common/mac_address.py</code> <pre><code>def __init__(self, mac: Union[bytes, str, \"MACAddress\"]):\n    if isinstance(mac, self.__class__):\n        mac = mac.mac  # type: ignore\n    if not isinstance(mac, bytes) or len(mac) != 6:\n        mac = self._parse_mac(mac)\n\n    self.mac: bytes = mac\n</code></pre>"},{"location":"reference/common/scheduler/","title":"scheduler","text":""},{"location":"reference/common/scheduler/#dp3.common.scheduler","title":"dp3.common.scheduler","text":"<p>Allows modules to register functions (callables) to be run at specified times or intervals (like cron does).</p> <p>Based on APScheduler package</p>"},{"location":"reference/common/scheduler/#dp3.common.scheduler.Scheduler","title":"Scheduler","text":"<pre><code>Scheduler()\n</code></pre> <p>Allows modules to register functions (callables) to be run at specified times or intervals (like cron does).</p> Source code in <code>dp3/common/scheduler.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.log = logging.getLogger(\"Scheduler\")\n    # self.log.setLevel(\"DEBUG\")\n    logging.getLogger(\"apscheduler.scheduler\").setLevel(\"WARNING\")\n    logging.getLogger(\"apscheduler.executors.default\").setLevel(\"WARNING\")\n    self.sched = BackgroundScheduler(timezone=\"UTC\")\n    self.last_job_id = 0\n</code></pre>"},{"location":"reference/common/scheduler/#dp3.common.scheduler.Scheduler.register","title":"register","text":"<pre><code>register(func: Callable, func_args: Union[list, tuple] = None, func_kwargs: dict = None, year: Union[int, str] = None, month: Union[int, str] = None, day: Union[int, str] = None, week: Union[int, str] = None, day_of_week: Union[int, str] = None, hour: Union[int, str] = None, minute: Union[int, str] = None, second: Union[int, str] = None, timezone: str = 'UTC', misfire_grace_time: int = 1) -&gt; int\n</code></pre> <p>Register a function to be run at specified times.</p> <p>Pass cron-like specification of when the function should be called, see docs of apscheduler.triggers.cron for details.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>function or method to be called</p> required <code>func_args</code> <code>Union[list, tuple]</code> <p>list of positional arguments to call func with</p> <code>None</code> <code>func_kwargs</code> <code>dict</code> <p>dict of keyword arguments to call func with</p> <code>None</code> <code>year</code> <code>Union[int, str]</code> <p>4-digit year</p> <code>None</code> <code>month</code> <code>Union[int, str]</code> <p>month (1-12)</p> <code>None</code> <code>day</code> <code>Union[int, str]</code> <p>day of month (1-31)</p> <code>None</code> <code>week</code> <code>Union[int, str]</code> <p>ISO week (1-53)</p> <code>None</code> <code>day_of_week</code> <code>Union[int, str]</code> <p>number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun)</p> <code>None</code> <code>hour</code> <code>Union[int, str]</code> <p>hour (0-23)</p> <code>None</code> <code>minute</code> <code>Union[int, str]</code> <p>minute (0-59)</p> <code>None</code> <code>second</code> <code>Union[int, str]</code> <p>second (0-59)</p> <code>None</code> <code>timezone</code> <code>str</code> <p>Timezone for time specification (default is UTC).</p> <code>'UTC'</code> <code>misfire_grace_time</code> <code>int</code> <p>seconds after the designated run time that the job is still allowed to be run (default is 1)</p> <code>1</code> <p>Returns:      job ID</p> Source code in <code>dp3/common/scheduler.py</code> <pre><code>def register(\n    self,\n    func: Callable,\n    func_args: Union[list, tuple] = None,\n    func_kwargs: dict = None,\n    year: Union[int, str] = None,\n    month: Union[int, str] = None,\n    day: Union[int, str] = None,\n    week: Union[int, str] = None,\n    day_of_week: Union[int, str] = None,\n    hour: Union[int, str] = None,\n    minute: Union[int, str] = None,\n    second: Union[int, str] = None,\n    timezone: str = \"UTC\",\n    misfire_grace_time: int = 1,\n) -&gt; int:\n    \"\"\"\n    Register a function to be run at specified times.\n\n    Pass cron-like specification of when the function should be called,\n    see [docs](https://apscheduler.readthedocs.io/en/latest/modules/triggers/cron.html)\n    of apscheduler.triggers.cron for details.\n\n    Args:\n        func: function or method to be called\n        func_args: list of positional arguments to call func with\n        func_kwargs: dict of keyword arguments to call func with\n        year: 4-digit year\n        month: month (1-12)\n        day: day of month (1-31)\n        week: ISO week (1-53)\n        day_of_week: number or name of weekday (0-6 or mon,tue,wed,thu,fri,sat,sun)\n        hour: hour (0-23)\n        minute: minute (0-59)\n        second: second (0-59)\n        timezone: Timezone for time specification (default is UTC).\n        misfire_grace_time: seconds after the designated run time\n            that the job is still allowed to be run (default is 1)\n    Returns:\n         job ID\n    \"\"\"\n    self.last_job_id += 1\n    trigger = CronTrigger(\n        year, month, day, week, day_of_week, hour, minute, second, timezone=timezone\n    )\n    self.sched.add_job(\n        func,\n        trigger,\n        func_args,\n        func_kwargs,\n        coalesce=True,\n        max_instances=1,\n        misfire_grace_time=misfire_grace_time,\n        id=str(self.last_job_id),\n    )\n    self.log.debug(f\"Registered function {get_func_name(func)} to be called at {trigger}\")\n    return self.last_job_id\n</code></pre>"},{"location":"reference/common/scheduler/#dp3.common.scheduler.Scheduler.pause_job","title":"pause_job","text":"<pre><code>pause_job(id)\n</code></pre> <p>Pause job with given ID</p> Source code in <code>dp3/common/scheduler.py</code> <pre><code>def pause_job(self, id):\n    \"\"\"Pause job with given ID\"\"\"\n    self.sched.pause_job(str(id))\n</code></pre>"},{"location":"reference/common/scheduler/#dp3.common.scheduler.Scheduler.resume_job","title":"resume_job","text":"<pre><code>resume_job(id)\n</code></pre> <p>Resume previously paused job with given ID</p> Source code in <code>dp3/common/scheduler.py</code> <pre><code>def resume_job(self, id):\n    \"\"\"Resume previously paused job with given ID\"\"\"\n    self.sched.resume_job(str(id))\n</code></pre>"},{"location":"reference/common/state/","title":"state","text":""},{"location":"reference/common/state/#dp3.common.state","title":"dp3.common.state","text":""},{"location":"reference/common/state/#dp3.common.state.SharedFlag","title":"SharedFlag","text":"<pre><code>SharedFlag(flag: bool = False, banner: str = '')\n</code></pre> <p>A bool wrapper that can be shared between multiple objects</p> Source code in <code>dp3/common/state.py</code> <pre><code>def __init__(self, flag: bool = False, banner: str = \"\"):\n    self._flag = flag\n    self._banner = banner\n</code></pre>"},{"location":"reference/common/state/#dp3.common.state.SharedFlag.set","title":"set","text":"<pre><code>set(flag: bool = True)\n</code></pre> <p>Set the flag to <code>flag</code>. True by default.</p> Source code in <code>dp3/common/state.py</code> <pre><code>def set(self, flag: bool = True):\n    \"\"\"Set the flag to `flag`. True by default.\"\"\"\n    self._flag = flag\n</code></pre>"},{"location":"reference/common/state/#dp3.common.state.SharedFlag.unset","title":"unset","text":"<pre><code>unset()\n</code></pre> <p>Unset the flag.</p> Source code in <code>dp3/common/state.py</code> <pre><code>def unset(self):\n    \"\"\"Unset the flag.\"\"\"\n    self._flag = False\n</code></pre>"},{"location":"reference/common/state/#dp3.common.state.SharedFlag.isset","title":"isset","text":"<pre><code>isset()\n</code></pre> <p>Check if the flag is set.</p> Source code in <code>dp3/common/state.py</code> <pre><code>def isset(self):\n    \"\"\"Check if the flag is set.\"\"\"\n    return self._flag\n</code></pre>"},{"location":"reference/common/task/","title":"task","text":""},{"location":"reference/common/task/#dp3.common.task","title":"dp3.common.task","text":""},{"location":"reference/common/task/#dp3.common.task.Task","title":"Task","text":"<p>               Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>A generic task type class.</p> <p>An abstraction for the task queue classes to depend upon.</p>"},{"location":"reference/common/task/#dp3.common.task.Task.routing_key","title":"routing_key  <code>abstractmethod</code>","text":"<pre><code>routing_key() -&gt; str\n</code></pre> <p>Returns:</p> Type Description <code>str</code> <p>A string to be used as a routing key between workers.</p> Source code in <code>dp3/common/task.py</code> <pre><code>@abstractmethod\ndef routing_key(self) -&gt; str:\n    \"\"\"\n    Returns:\n        A string to be used as a routing key between workers.\n    \"\"\"\n</code></pre>"},{"location":"reference/common/task/#dp3.common.task.Task.hashed_routing_key","title":"hashed_routing_key","text":"<pre><code>hashed_routing_key() -&gt; int\n</code></pre> <p>Returns:</p> Type Description <code>int</code> <p>An integer to be used as a hashed routing key between workers.</p> Source code in <code>dp3/common/task.py</code> <pre><code>def hashed_routing_key(self) -&gt; int:\n    \"\"\"\n    Returns:\n        An integer to be used as a hashed routing key between workers.\n    \"\"\"\n    return HASH(self.routing_key())\n</code></pre>"},{"location":"reference/common/task/#dp3.common.task.Task.as_message","title":"as_message  <code>abstractmethod</code>","text":"<pre><code>as_message() -&gt; str\n</code></pre> <p>Returns:</p> Type Description <code>str</code> <p>A string representation of the object.</p> Source code in <code>dp3/common/task.py</code> <pre><code>@abstractmethod\ndef as_message(self) -&gt; str:\n    \"\"\"\n    Returns:\n        A string representation of the object.\n    \"\"\"\n</code></pre>"},{"location":"reference/common/task/#dp3.common.task.DataPointTask","title":"DataPointTask","text":"<pre><code>DataPointTask(__pydantic_self__, **data: Any)\n</code></pre> <p>               Bases: <code>Task</code></p> <p>DataPointTask</p> <p>Contains single task to be pushed to TaskQueue and processed. Attributes:     etype: Entity type     eid: Entity id / key     data_points: List of DataPoints to process     tags: List of tags     ttl_tokens: Dictionary of TTL tokens.     delete: If True, delete entity</p> <p>Set the <code>model_spec</code> context variable and initialize the Task.</p> <p>See https://docs.pydantic.dev/latest/concepts/validators/#validation-context</p> Source code in <code>dp3/common/task.py</code> <pre><code>def __init__(__pydantic_self__, **data: Any) -&gt; None:\n    \"\"\"Set the `model_spec` context variable and initialize the Task.\n\n    See https://docs.pydantic.dev/latest/concepts/validators/#validation-context\n    \"\"\"\n    __pydantic_self__.__pydantic_validator__.validate_python(\n        data,\n        self_instance=__pydantic_self__,\n        context=_init_context_var.get(),\n    )\n</code></pre>"},{"location":"reference/common/task/#dp3.common.task.Snapshot","title":"Snapshot","text":"<p>               Bases: <code>Task</code></p> <p>Snapshot</p> <p>Contains a list of entities, the meaning of which depends on the <code>type</code>. If <code>type</code> is \"task\", then the list contains linked entities for which a snapshot should be created. Otherwise <code>type</code> is \"linked_entities\", indicating which entities must be skipped in a parallelized creation of unlinked entities.</p> <p>Attributes:</p> Name Type Description <code>entities</code> <code>list[EntityTuple]</code> <p>List of (entity_type, entity_id)</p> <code>time</code> <code>datetime</code> <p>timestamp for snapshot creation</p> <code>final</code> <code>bool</code> <p>If True, this is the last linked snapshot for the given time</p>"},{"location":"reference/common/task/#dp3.common.task.HASH","title":"HASH","text":"<pre><code>HASH(key: str) -&gt; int\n</code></pre> <p>Hash function used to distribute tasks to worker processes. Args:     key: to be hashed Returns:     last 4 bytes of MD5</p> Source code in <code>dp3/common/task.py</code> <pre><code>def HASH(key: str) -&gt; int:\n    \"\"\"Hash function used to distribute tasks to worker processes.\n    Args:\n        key: to be hashed\n    Returns:\n        last 4 bytes of MD5\n    \"\"\"\n    return int(hashlib.md5(key.encode(\"utf8\")).hexdigest()[-4:], 16)\n</code></pre>"},{"location":"reference/common/task/#dp3.common.task.task_context","title":"task_context","text":"<pre><code>task_context(model_spec: ModelSpec) -&gt; Iterator[None]\n</code></pre> <p>Context manager for setting the <code>model_spec</code> context variable.</p> Source code in <code>dp3/common/task.py</code> <pre><code>@contextmanager\ndef task_context(model_spec: ModelSpec) -&gt; Iterator[None]:\n    \"\"\"Context manager for setting the `model_spec` context variable.\"\"\"\n    token = _init_context_var.set({\"model_spec\": model_spec})\n    try:\n        yield\n    finally:\n        _init_context_var.reset(token)\n</code></pre>"},{"location":"reference/common/task/#dp3.common.task.parse_eids_from_cache","title":"parse_eids_from_cache","text":"<pre><code>parse_eids_from_cache(model_spec: ModelSpec, link_entity_entries: list[str]) -&gt; list[AnyEidT]\n</code></pre> <p>Parses entity IDs from the \"Link\" cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_spec</code> <code>ModelSpec</code> <p>Model specification.</p> required <code>link_entity_entries</code> <code>list[str]</code> <p>List of entity entries from the cache.</p> required <p>Returns:</p> Type Description <code>list[AnyEidT]</code> <p>List of entity IDs.</p> Source code in <code>dp3/common/task.py</code> <pre><code>def parse_eids_from_cache(model_spec: ModelSpec, link_entity_entries: list[str]) -&gt; list[AnyEidT]:\n    \"\"\"Parses entity IDs from the \"Link\" cache.\n\n    Args:\n        model_spec: Model specification.\n        link_entity_entries: List of entity entries from the cache.\n\n    Returns:\n        List of entity IDs.\n    \"\"\"\n    raw = [entry.split(\"#\", maxsplit=1) for entry in link_entity_entries]\n    return [eid for etype, eid in validate_entities(model_spec, raw)]\n</code></pre>"},{"location":"reference/common/task/#dp3.common.task.parse_eid_tuples_from_cache","title":"parse_eid_tuples_from_cache","text":"<pre><code>parse_eid_tuples_from_cache(model_spec: ModelSpec, link_entity_entries: list[str]) -&gt; list[tuple[str, AnyEidT]]\n</code></pre> <p>Parses entity IDs from the \"Link\" cache.</p> <p>Parameters:</p> Name Type Description Default <code>model_spec</code> <code>ModelSpec</code> <p>Model specification.</p> required <code>link_entity_entries</code> <code>list[str]</code> <p>List of entity entries from the cache.</p> required <p>Returns:</p> Type Description <code>list[tuple[str, AnyEidT]]</code> <p>List of tuples (entity type, entity ID).</p> Source code in <code>dp3/common/task.py</code> <pre><code>def parse_eid_tuples_from_cache(\n    model_spec: ModelSpec, link_entity_entries: list[str]\n) -&gt; list[tuple[str, AnyEidT]]:\n    \"\"\"Parses entity IDs from the \"Link\" cache.\n\n    Args:\n        model_spec: Model specification.\n        link_entity_entries: List of entity entries from the cache.\n\n    Returns:\n        List of tuples (entity type, entity ID).\n    \"\"\"\n    raw = [entry.split(\"#\", maxsplit=1) for entry in link_entity_entries]\n    return validate_entities(model_spec, raw)\n</code></pre>"},{"location":"reference/common/types/","title":"types","text":""},{"location":"reference/common/types/#dp3.common.types","title":"dp3.common.types","text":""},{"location":"reference/common/types/#dp3.common.types.DP3Encoder","title":"DP3Encoder","text":"<p>               Bases: <code>JSONEncoder</code></p> <p>JSONEncoder to encode python types using DP3 conventions.</p>"},{"location":"reference/common/types/#dp3.common.types.parse_timedelta_or_passthrough","title":"parse_timedelta_or_passthrough","text":"<pre><code>parse_timedelta_or_passthrough(v)\n</code></pre> <p>We pass the value to the native pydantic validator if the value does not match our pattern.</p> Source code in <code>dp3/common/types.py</code> <pre><code>def parse_timedelta_or_passthrough(v):\n    \"\"\"\n    We pass the value to the native pydantic validator if the value does not match our pattern.\n    \"\"\"\n    if v and isinstance(v, str) and time_duration_pattern.match(v):\n        return parse_time_duration(v)\n    return v\n</code></pre>"},{"location":"reference/common/types/#dp3.common.types.t2_implicity_t1","title":"t2_implicity_t1","text":"<pre><code>t2_implicity_t1(v, info: FieldValidationInfo)\n</code></pre> <p>If t2 is not specified, it is set to t1.</p> Source code in <code>dp3/common/types.py</code> <pre><code>def t2_implicity_t1(v, info: FieldValidationInfo):\n    \"\"\"If t2 is not specified, it is set to t1.\"\"\"\n    v = v or info.data.get(\"t1\")\n    return v\n</code></pre>"},{"location":"reference/common/types/#dp3.common.types.t2_after_t1","title":"t2_after_t1","text":"<pre><code>t2_after_t1(v, info: FieldValidationInfo)\n</code></pre> <p>t2 must be after t1</p> Source code in <code>dp3/common/types.py</code> <pre><code>def t2_after_t1(v, info: FieldValidationInfo):\n    \"\"\"t2 must be after t1\"\"\"\n    if info.data.get(\"t1\"):\n        assert info.data[\"t1\"] &lt;= v, \"'t2' is before 't1'\"\n    return v\n</code></pre>"},{"location":"reference/common/utils/","title":"utils","text":""},{"location":"reference/common/utils/#dp3.common.utils","title":"dp3.common.utils","text":"<p>auxiliary/utility functions and classes</p>"},{"location":"reference/common/utils/#dp3.common.utils.entity_expired","title":"entity_expired","text":"<pre><code>entity_expired(utcnow: datetime, master_document: dict)\n</code></pre> <p>Check if entity is expired (all TTLs are in the past)</p> Source code in <code>dp3/common/utils.py</code> <pre><code>def entity_expired(utcnow: datetime, master_document: dict):\n    \"\"\"Check if entity is expired (all TTLs are in the past)\"\"\"\n    if \"#ttl\" not in master_document:\n        return False\n    return all(ttl &lt; utcnow for ttl in master_document[\"#ttl\"].values())\n</code></pre>"},{"location":"reference/common/utils/#dp3.common.utils.int2bytes","title":"int2bytes","text":"<pre><code>int2bytes(num: int) -&gt; bytes\n</code></pre> <p>Convert signed int to however many bytes necessary in big-endian order</p> Source code in <code>dp3/common/utils.py</code> <pre><code>def int2bytes(num: int) -&gt; bytes:\n    \"\"\"Convert signed int to however many bytes necessary in big-endian order\"\"\"\n    return num.to_bytes(\n        length=(8 + (num + (num &lt; 0)).bit_length()) // 8, byteorder=\"big\", signed=True\n    )\n</code></pre>"},{"location":"reference/common/utils/#dp3.common.utils.bytes2int","title":"bytes2int","text":"<pre><code>bytes2int(b: bytes) -&gt; int\n</code></pre> <p>Convert bytes to signed int in big-endian order</p> Source code in <code>dp3/common/utils.py</code> <pre><code>def bytes2int(b: bytes) -&gt; int:\n    \"\"\"Convert bytes to signed int in big-endian order\"\"\"\n    return int.from_bytes(b, \"big\", signed=True)\n</code></pre>"},{"location":"reference/common/utils/#dp3.common.utils.parse_rfc_time","title":"parse_rfc_time","text":"<pre><code>parse_rfc_time(time_str)\n</code></pre> <p>Parse time in RFC 3339 format and return it as naive datetime in UTC.</p> <p>Timezone specification is optional (UTC is assumed when none is specified).</p> Source code in <code>dp3/common/utils.py</code> <pre><code>def parse_rfc_time(time_str):\n    \"\"\"\n    Parse time in RFC 3339 format and return it as naive datetime in UTC.\n\n    Timezone specification is optional (UTC is assumed when none is specified).\n    \"\"\"\n    res = timestamp_re.match(time_str)\n    if res is not None:\n        year, month, day, hour, minute, second = (int(n or 0) for n in res.group(*range(1, 7)))\n        us_str = (res.group(7) or \"0\")[:6].ljust(6, \"0\")\n        us = int(us_str)\n        zonestr = res.group(8)\n        zoneoffset = 0 if zonestr in (None, \"z\", \"Z\") else int(zonestr[:3]) * 60 + int(zonestr[4:6])\n        zonediff = datetime.timedelta(minutes=zoneoffset)\n        return datetime.datetime(year, month, day, hour, minute, second, us) - zonediff\n    else:\n        raise ValueError(\"Wrong timestamp format\")\n</code></pre>"},{"location":"reference/common/utils/#dp3.common.utils.parse_time_duration","title":"parse_time_duration","text":"<pre><code>parse_time_duration(duration_string: Union[str, int, timedelta]) -&gt; datetime.timedelta\n</code></pre> <p>Parse duration in format  (or just \"0\"). <p>Return datetime.timedelta</p> Source code in <code>dp3/common/utils.py</code> <pre><code>def parse_time_duration(duration_string: Union[str, int, datetime.timedelta]) -&gt; datetime.timedelta:\n    \"\"\"\n    Parse duration in format &lt;num&gt;&lt;s/m/h/d&gt; (or just \"0\").\n\n    Return datetime.timedelta\n    \"\"\"\n    # if it's already timedelta, just return it unchanged\n    if isinstance(duration_string, datetime.timedelta):\n        return duration_string\n    # if number is passed, consider it number of seconds\n    if isinstance(duration_string, (int, float)):\n        return datetime.timedelta(seconds=duration_string)\n\n    d = 0\n    h = 0\n    m = 0\n    s = 0\n\n    if duration_string == \"0\":\n        pass\n    elif duration_string[-1] == \"d\":\n        d = int(duration_string[:-1])\n    elif duration_string[-1] == \"h\":\n        h = int(duration_string[:-1])\n    elif duration_string[-1] == \"m\":\n        m = int(duration_string[:-1])\n    elif duration_string[-1] == \"s\":\n        s = int(duration_string[:-1])\n    else:\n        raise ValueError(\"Invalid time duration string\")\n\n    return datetime.timedelta(days=d, hours=h, minutes=m, seconds=s)\n</code></pre>"},{"location":"reference/common/utils/#dp3.common.utils.conv_to_json","title":"conv_to_json","text":"<pre><code>conv_to_json(obj)\n</code></pre> <p>Convert special types to JSON (use as \"default\" param of json.dumps)</p> <p>Supported types/objects: - datetime - timedelta</p> Source code in <code>dp3/common/utils.py</code> <pre><code>def conv_to_json(obj):\n    \"\"\"Convert special types to JSON (use as \"default\" param of json.dumps)\n\n    Supported types/objects:\n    - datetime\n    - timedelta\n    \"\"\"\n    if isinstance(obj, datetime.datetime):\n        if obj.tzinfo:\n            raise NotImplementedError(\n                \"Can't serialize timezone-aware datetime object \"\n                \"(DP3 policy is to use naive datetimes in UTC everywhere)\"\n            )\n        return {\"$datetime\": obj.strftime(\"%Y-%m-%dT%H:%M:%S.%f\")}\n    if isinstance(obj, datetime.timedelta):\n        return {\"$timedelta\": f\"{obj.days},{obj.seconds},{obj.microseconds}\"}\n    raise TypeError(f\"{repr(obj)}%r is not JSON serializable\")\n</code></pre>"},{"location":"reference/common/utils/#dp3.common.utils.conv_from_json","title":"conv_from_json","text":"<pre><code>conv_from_json(dct)\n</code></pre> <p>Convert special JSON keys created by conv_to_json back to Python objects (use as \"object_hook\" param of json.loads)</p> <p>Supported types/objects: - datetime - timedelta</p> Source code in <code>dp3/common/utils.py</code> <pre><code>def conv_from_json(dct):\n    \"\"\"Convert special JSON keys created by conv_to_json back to Python objects\n    (use as \"object_hook\" param of json.loads)\n\n    Supported types/objects:\n    - datetime\n    - timedelta\n    \"\"\"\n    if \"$datetime\" in dct:\n        val = dct[\"$datetime\"]\n        return datetime.datetime.strptime(val, \"%Y-%m-%dT%H:%M:%S.%f\")\n    if \"$timedelta\" in dct:\n        days, seconds, microseconds = dct[\"$timedelta\"].split(\",\")\n        return datetime.timedelta(int(days), int(seconds), int(microseconds))\n    return dct\n</code></pre>"},{"location":"reference/common/utils/#dp3.common.utils.batched","title":"batched","text":"<pre><code>batched(iterable: Iterable, n: int) -&gt; Iterator[list]\n</code></pre> <p>Batch data into tuples of length n. The last batch may be shorter.</p> Source code in <code>dp3/common/utils.py</code> <pre><code>def batched(iterable: Iterable, n: int) -&gt; Iterator[list]:\n    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n    # batched('ABCDEFG', 3) --&gt; ABC DEF G\n    if n &lt; 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while True:\n        batch = list(islice(it, n))\n        if not batch:\n            break\n        yield batch\n</code></pre>"},{"location":"reference/common/utils/#dp3.common.utils.get_func_name","title":"get_func_name","text":"<pre><code>get_func_name(func_or_method)\n</code></pre> <p>Get name of function or method as pretty string.</p> Source code in <code>dp3/common/utils.py</code> <pre><code>def get_func_name(func_or_method):\n    \"\"\"Get name of function or method as pretty string.\"\"\"\n    if isinstance(func_or_method, partial):\n        wrapper = \"partial({name}, {args})\"\n        args = [str(a) for a in func_or_method.args]\n        args.extend(f\"{k}={v}\" for k, v in func_or_method.keywords.items())\n        args = \", \".join(args)\n        func_or_method = func_or_method.func\n    else:\n        wrapper = \"{name}{args}\"\n        args = \"\"\n\n    try:\n        fname = func_or_method.__func__.__qualname__\n    except AttributeError:\n        try:\n            fname = func_or_method.__name__\n        except AttributeError:\n            fname = str(func_or_method)\n\n    try:\n        module = func_or_method.__module__\n    except AttributeError:\n        return fname\n    return wrapper.format(name=f\"{module}.{fname}\", args=args)\n</code></pre>"},{"location":"reference/core/","title":"core","text":""},{"location":"reference/core/#dp3.core","title":"dp3.core","text":""},{"location":"reference/core/collector/","title":"collector","text":""},{"location":"reference/core/collector/#dp3.core.collector","title":"dp3.core.collector","text":"<p>Core module performing deletion of entities based on specified policy.</p>"},{"location":"reference/core/collector/#dp3.core.collector.GarbageCollectorConfig","title":"GarbageCollectorConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>The configuration of the Collector module.</p> <p>Attributes:</p> Name Type Description <code>collection_rate</code> <code>CronExpression</code> <p>The rate at which the collector module runs. Default is 3:00 AM every day.</p>"},{"location":"reference/core/collector/#dp3.core.collector.GarbageCollector","title":"GarbageCollector","text":"<pre><code>GarbageCollector(db: EntityDatabase, platform_config: PlatformConfig, registrar: CallbackRegistrar)\n</code></pre> <p>Collector module manages the lifetimes of entities based on specified policy.</p> Source code in <code>dp3/core/collector.py</code> <pre><code>def __init__(\n    self,\n    db: EntityDatabase,\n    platform_config: PlatformConfig,\n    registrar: CallbackRegistrar,\n):\n    self.log = logging.getLogger(\"GarbageCollector\")\n    self.model_spec = platform_config.model_spec\n    self.config = GarbageCollectorConfig.model_validate(\n        platform_config.config.get(\"garbage_collector\", {})\n    )\n    self.db = db\n\n    self.worker_index = platform_config.process_index\n    self.num_workers = platform_config.num_processes\n\n    # Get link cache\n    self.cache = self.db.get_module_cache(\"Link\")\n    self.inverse_relations = self._get_inverse_relations()\n\n    for entity, entity_config in self.model_spec.entities.items():\n        lifetime = entity_config.lifetime\n        if lifetime.type == \"immortal\":\n            pass\n        elif lifetime.type == \"ttl\":\n            registrar.register_entity_hook(\n                \"on_entity_creation\",\n                partial(self.extend_ttl_on_create, base_ttl=lifetime.on_create),\n                entity,\n            )\n\n            self._register_ttl_extensions(entity, registrar, lifetime.mirror_data)\n\n            registrar.scheduler_register(\n                self.collect_ttl,\n                func_args=[entity],\n                **self.config.collection_rate.model_dump(),\n            )\n        elif lifetime.type == \"weak\":\n            if self.inverse_relations[entity]:\n                registrar.scheduler_register(\n                    self.collect_weak,\n                    func_args=[entity],\n                    **self.config.collection_rate.model_dump(),\n                )\n            else:\n                raise ValueError(\n                    f\"Entity {entity} has weak lifetime \"\n                    f\"but is not referenced by any other entities.\"\n                )\n\n        else:\n            raise ValueError(f\"Unknown lifetime type: {lifetime.type}\")\n</code></pre>"},{"location":"reference/core/collector/#dp3.core.collector.GarbageCollector.extend_ttl_on_create","title":"extend_ttl_on_create","text":"<pre><code>extend_ttl_on_create(eid: AnyEidT, task: DataPointTask, base_ttl: timedelta) -&gt; list[DataPointTask]\n</code></pre> <p>Extends the TTL of the entity by the specified timedelta.</p> Source code in <code>dp3/core/collector.py</code> <pre><code>def extend_ttl_on_create(\n    self, eid: AnyEidT, task: DataPointTask, base_ttl: timedelta\n) -&gt; list[DataPointTask]:\n    \"\"\"Extends the TTL of the entity by the specified timedelta.\"\"\"\n    task = DataPointTask(\n        etype=task.etype,\n        eid=eid,\n        ttl_tokens={\"base\": datetime.utcnow() + base_ttl},\n    )\n    return [task]\n</code></pre>"},{"location":"reference/core/collector/#dp3.core.collector.GarbageCollector.collect_weak","title":"collect_weak","text":"<pre><code>collect_weak(etype: str)\n</code></pre> <p>Deletes weak entities when their last reference has expired.</p> Source code in <code>dp3/core/collector.py</code> <pre><code>def collect_weak(self, etype: str):\n    \"\"\"Deletes weak entities when their last reference has expired.\"\"\"\n    self.log.debug(\"Starting removal of '%s' weak entities\", etype)\n    start = datetime.now()\n    entities = 0\n    deleted = 0\n\n    self.db.save_metadata(\n        start,\n        {\"entities\": 0, \"deleted\": 0, \"weak_collect_start\": start, \"entity\": etype},\n    )\n\n    # Aggregate the cache entities by their \"to\" field, which contains the entity\n    aggregated = self.cache.aggregate(\n        [\n            {\"$match\": {\"to\": {\"$regex\": f\"^{etype}#\"}}},\n            {\"$group\": {\"_id\": \"$to\"}},\n        ]\n    )\n    have_references = parse_eids_from_cache(self.model_spec, [doc[\"_id\"] for doc in aggregated])\n    entities += len(have_references)\n\n    to_delete = []\n    records_cursor = self.db.get_worker_master_records(\n        self.worker_index,\n        self.num_workers,\n        etype,\n        query_filter={\"_id\": {\"$nin\": have_references}},\n    )\n    try:\n        for master_document in records_cursor:\n            entities += 1\n            deleted += 1\n            to_delete.append(master_document[\"_id\"])\n\n            if len(to_delete) &gt;= DB_SEND_CHUNK:\n                self.db.delete_eids(etype, to_delete)\n                to_delete.clear()\n\n        if to_delete:\n            self.db.delete_eids(etype, to_delete)\n            to_delete.clear()\n\n    finally:\n        records_cursor.close()\n\n    self.db.update_metadata(\n        start,\n        metadata={\"weak_collect_end\": datetime.now()},\n        increase={\"entities\": entities, \"deleted\": deleted},\n    )\n    self.log.info(\n        \"Removal of '%s' weak entities done - %s tracked, %s processed &amp; deleted\",\n        etype,\n        entities,\n        deleted,\n    )\n</code></pre>"},{"location":"reference/core/collector/#dp3.core.collector.GarbageCollector.collect_ttl","title":"collect_ttl","text":"<pre><code>collect_ttl(etype: str)\n</code></pre> <p>Deletes entities after their TTL lifetime has expired.</p> Source code in <code>dp3/core/collector.py</code> <pre><code>def collect_ttl(self, etype: str):\n    \"\"\"Deletes entities after their TTL lifetime has expired.\"\"\"\n    self.log.debug(\"Starting removal of '%s' entities by TTL\", etype)\n    start = datetime.now()\n    utc_now = datetime.utcnow()\n    entities = 0\n    deleted = 0\n\n    to_delete = []\n    expired_ttls = {}\n\n    self.db.save_metadata(\n        start, {\"entities\": 0, \"deleted\": 0, \"ttl_collect_start\": start, \"entity\": etype}\n    )\n\n    records_cursor = self.db.get_worker_master_records(\n        self.worker_index, self.num_workers, etype, no_cursor_timeout=True\n    )\n    try:\n        for master_document in records_cursor:\n            entities += 1\n            if \"#ttl\" not in master_document:\n                continue  # TTL not set, ignore for now\n\n            if all(ttl &lt; utc_now for ttl in master_document[\"#ttl\"].values()):\n                deleted += 1\n                to_delete.append(master_document[\"_id\"])\n            else:\n                eid_expired_ttls = [\n                    name for name, ttl in master_document[\"#ttl\"].items() if ttl &lt; start\n                ]\n                if eid_expired_ttls:\n                    expired_ttls[master_document[\"_id\"]] = eid_expired_ttls\n\n            if len(to_delete) &gt;= DB_SEND_CHUNK:\n                self.db.delete_eids(etype, to_delete)\n                to_delete.clear()\n            if len(expired_ttls) &gt;= DB_SEND_CHUNK:\n                self.db.remove_expired_ttls(etype, expired_ttls)\n                expired_ttls.clear()\n\n        if to_delete:\n            self.db.delete_eids(etype, to_delete)\n            to_delete.clear()\n        if expired_ttls:\n            self.db.remove_expired_ttls(etype, expired_ttls)\n            expired_ttls.clear()\n\n    finally:\n        records_cursor.close()\n\n    self.db.update_metadata(\n        start,\n        metadata={\"ttl_collect_end\": datetime.now()},\n        increase={\"entities\": entities, \"deleted\": deleted},\n    )\n    self.log.info(\n        \"Removal of '%s' entities by TTL done - %s processed, %s deleted\",\n        etype,\n        entities,\n        deleted,\n    )\n</code></pre>"},{"location":"reference/core/collector/#dp3.core.collector.GarbageCollector.extend_plain_ttl","title":"extend_plain_ttl","text":"<pre><code>extend_plain_ttl(eid: AnyEidT, dp: DataPointBase, extend_by: timedelta) -&gt; list[DataPointTask]\n</code></pre> <p>Extends the TTL of the entity by the specified timedelta.</p> Source code in <code>dp3/core/collector.py</code> <pre><code>def extend_plain_ttl(\n    self, eid: AnyEidT, dp: DataPointBase, extend_by: timedelta\n) -&gt; list[DataPointTask]:\n    \"\"\"Extends the TTL of the entity by the specified timedelta.\"\"\"\n    now = datetime.utcnow()\n    task = DataPointTask(\n        etype=dp.etype,\n        eid=eid,\n        ttl_tokens={\"data\": now + extend_by},\n    )\n    return [task]\n</code></pre>"},{"location":"reference/core/collector/#dp3.core.collector.GarbageCollector.extend_observations_ttl","title":"extend_observations_ttl","text":"<pre><code>extend_observations_ttl(eid: AnyEidT, dp: DataPointObservationsBase, extend_by: timedelta) -&gt; list[DataPointTask]\n</code></pre> <p>Extends the TTL of the entity by the specified timedelta.</p> Source code in <code>dp3/core/collector.py</code> <pre><code>def extend_observations_ttl(\n    self, eid: AnyEidT, dp: DataPointObservationsBase, extend_by: timedelta\n) -&gt; list[DataPointTask]:\n    \"\"\"Extends the TTL of the entity by the specified timedelta.\"\"\"\n    task = DataPointTask(\n        etype=dp.etype,\n        eid=eid,\n        ttl_tokens={\"data\": dp.t2 + extend_by},\n    )\n    return [task]\n</code></pre>"},{"location":"reference/core/collector/#dp3.core.collector.GarbageCollector.extend_timeseries_ttl","title":"extend_timeseries_ttl","text":"<pre><code>extend_timeseries_ttl(eid: AnyEidT, dp: DataPointTimeseriesBase, extend_by: timedelta) -&gt; list[DataPointTask]\n</code></pre> <p>Extends the TTL of the entity by the specified timedelta.</p> Source code in <code>dp3/core/collector.py</code> <pre><code>def extend_timeseries_ttl(\n    self, eid: AnyEidT, dp: DataPointTimeseriesBase, extend_by: timedelta\n) -&gt; list[DataPointTask]:\n    \"\"\"Extends the TTL of the entity by the specified timedelta.\"\"\"\n    task = DataPointTask(\n        etype=dp.etype,\n        eid=eid,\n        ttl_tokens={\"data\": dp.t2 + extend_by},\n    )\n    return [task]\n</code></pre>"},{"location":"reference/core/link_manager/","title":"link_manager","text":""},{"location":"reference/core/link_manager/#dp3.core.link_manager","title":"dp3.core.link_manager","text":"<p>Core module managing links between entities.</p>"},{"location":"reference/core/link_manager/#dp3.core.link_manager.LinkManager","title":"LinkManager","text":"<pre><code>LinkManager(db: EntityDatabase, platform_config: PlatformConfig, registrar: CallbackRegistrar)\n</code></pre> <p>Manages the shared Link cache and updates links after entity deletion.</p> Source code in <code>dp3/core/link_manager.py</code> <pre><code>def __init__(\n    self,\n    db: EntityDatabase,\n    platform_config: PlatformConfig,\n    registrar: CallbackRegistrar,\n):\n    self.log = logging.getLogger(\"LinkManager\")\n    self.model_spec = platform_config.model_spec\n    self.db = db\n\n    self.cache = self.db.get_module_cache(\"Link\")\n    self._setup_cache_indexes()\n    self.db.register_on_entity_delete(\n        self.remove_link_cache_of_deleted, self.remove_link_cache_of_many_deleted\n    )\n    self.max_date = datetime.max.replace(tzinfo=None)\n    for (entity, attr), spec in self.model_spec.relations.items():\n        if spec.t == AttrType.PLAIN:\n            if spec.is_iterable:\n                func = self.add_iterable_plain_to_link_cache\n            else:\n                func = self.add_plain_to_link_cache\n            registrar.register_attr_hook(\n                \"on_new_plain\", partial(func, spec.relation_to), entity, attr\n            )\n        elif spec.t == AttrType.OBSERVATIONS:\n            if spec.is_iterable:\n                func = self.add_iterable_observation_to_link_cache\n            else:\n                func = self.add_observation_to_link_cache\n            registrar.register_attr_hook(\n                \"on_new_observation\",\n                partial(func, spec.relation_to, spec.history_params.post_validity),\n                entity,\n                attr,\n            )\n</code></pre>"},{"location":"reference/core/updater/","title":"updater","text":""},{"location":"reference/core/updater/#dp3.core.updater","title":"dp3.core.updater","text":"<p>Core module that executes periodic update callbacks.</p>"},{"location":"reference/core/updater/#dp3.core.updater.UpdaterConfig","title":"UpdaterConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>The configuration of the Updater module.</p> <p>The periodic update is executed in smaller batches for better robustness. The batch size is dynamically adjusted based the total number of entities and the estimated growth rate.</p> <p>Attributes:</p> Name Type Description <code>update_batch_cron</code> <code>CronExpression</code> <p>A CRON expression for the periodic update.</p> <code>update_batch_period</code> <code>ParsedTimedelta</code> <p>The period of the periodic update. Should equal to the period of <code>update_batch_cron</code>.</p> <code>cache_management_cron</code> <code>CronExpression</code> <p>A CRON expression for the cache management.</p> <code>cache_max_entries</code> <code>int</code> <p>The maximum number of finished cache entries per thread_id.</p>"},{"location":"reference/core/updater/#dp3.core.updater.UpdateThreadState","title":"UpdateThreadState","text":"<p>               Bases: <code>BaseModel</code></p> <p>A cache item describing a state of one configured update thread.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>Literal['state']</code> <p>\"state\"</p> <code>t_created</code> <code>datetime</code> <p>Time of creation.</p> <code>t_last_update</code> <code>datetime</code> <p>Time of last update.</p> <code>t_end</code> <code>datetime</code> <p>Time of predicted period end.</p> <code>processed</code> <code>int</code> <p>The number of currently processed entities.</p> <code>total</code> <code>int</code> <p>The total number of entities.</p> <code>iteration</code> <code>int</code> <p>The current iteration.</p> <code>total_iterations</code> <code>int</code> <p>Total number of iterations.</p> <code>etype</code> <code>str</code> <p>Entity type.</p> <code>period</code> <code>float</code> <p>Period length in seconds.</p> <code>eid_only</code> <code>bool</code> <p>Whether only eids are passed to hooks.</p> <code>hook_ids</code> <code>list[str]</code> <p>Hook ids.</p> <code>runtime_secs</code> <code>float</code> <p>Total hook runtime in seconds.</p>"},{"location":"reference/core/updater/#dp3.core.updater.UpdateThreadState.thread_id","title":"thread_id  <code>property</code>","text":"<pre><code>thread_id: tuple[float, str, bool]\n</code></pre> <p>A tuple of (period, entity_type, eid_only).</p>"},{"location":"reference/core/updater/#dp3.core.updater.UpdateThreadState.new","title":"new  <code>classmethod</code>","text":"<pre><code>new(hooks: dict, period: float, entity_type: str, eid_only: bool = False)\n</code></pre> <p>Create a new instance initialized with hooks and thread_id components.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>@classmethod\ndef new(cls, hooks: dict, period: float, entity_type: str, eid_only: bool = False):\n    \"\"\"Create a new instance initialized with hooks and thread_id components.\"\"\"\n    now = datetime.now()\n    return cls(\n        t_created=now,\n        t_last_update=now,\n        t_end=now + timedelta(seconds=period),\n        period=period,\n        etype=entity_type,\n        eid_only=eid_only,\n        hook_ids=hooks.keys(),\n    )\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.UpdateThreadState.id_attributes","title":"id_attributes  <code>staticmethod</code>","text":"<pre><code>id_attributes()\n</code></pre> <p>A list of attributes which identify the state in cache.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>@staticmethod\ndef id_attributes():\n    \"\"\"A list of attributes which identify the state in cache.\"\"\"\n    return [\"type\", \"period\", \"etype\", \"eid_only\", \"t_created\"]\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.UpdateThreadState.reset","title":"reset","text":"<pre><code>reset()\n</code></pre> <p>Resets counters and timestamps.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>def reset(self):\n    \"\"\"Resets counters and timestamps.\"\"\"\n    now = datetime.now()\n    self.t_created = now\n    self.t_last_update = now\n    self.t_end = now + timedelta(seconds=self.period)\n    self.iteration = 0\n    self.processed = 0\n    self.runtime_secs = 0.0\n    self.finished = False\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.UpdaterCache","title":"UpdaterCache","text":"<pre><code>UpdaterCache(cache_collection)\n</code></pre> <p>The cache collection contains the metadata documents with the state of the update process for each entity type.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>def __init__(self, cache_collection):\n    self._cache = cache_collection\n    self._setup_cache_indexes()\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.UpdaterCache.get_unfinished","title":"get_unfinished","text":"<pre><code>get_unfinished() -&gt; Iterator[UpdateThreadState]\n</code></pre> <p>Yields all unfinished cache entries from DB.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>def get_unfinished(self) -&gt; Iterator[UpdateThreadState]:\n    \"\"\"Yields all unfinished cache entries from DB.\"\"\"\n    for state in self._cache.find({\"type\": \"state\", \"finished\": False}):\n        yield UpdateThreadState.model_validate(state)\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.UpdaterCache.upsert","title":"upsert","text":"<pre><code>upsert(state: UpdateThreadState) -&gt; UpdateResult\n</code></pre> <p>Update or insert a state entry into DB.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>def upsert(self, state: UpdateThreadState) -&gt; UpdateResult:\n    \"\"\"Update or insert a state entry into DB.\"\"\"\n    state_dict = state.model_dump()\n    filter_dict = {k: v for k, v in state_dict.items() if k in state.id_attributes()}\n    update_dict = {\n        \"$set\": {k: v for k, v in state_dict.items() if k not in state.id_attributes()}\n    }\n    return self._cache.update_one(filter_dict, update=update_dict, upsert=True)\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.UpdaterCache.register_management","title":"register_management","text":"<pre><code>register_management(scheduler: Scheduler, trigger: CronExpression, max_entries: int) -&gt; int\n</code></pre> <p>Registers the cache management task with the scheduler.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>def register_management(\n    self, scheduler: Scheduler, trigger: CronExpression, max_entries: int\n) -&gt; int:\n    \"\"\"Registers the cache management task with the scheduler.\"\"\"\n    return scheduler.register(\n        self._manage_cache, func_args=[max_entries], **trigger.model_dump()\n    )\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.Updater","title":"Updater","text":"<pre><code>Updater(db: EntityDatabase, task_queue_writer: TaskQueueWriter, platform_config: PlatformConfig, scheduler: Scheduler, elog: EventGroupType)\n</code></pre> <p>Executes periodic update callbacks.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>def __init__(\n    self,\n    db: EntityDatabase,\n    task_queue_writer: TaskQueueWriter,\n    platform_config: PlatformConfig,\n    scheduler: Scheduler,\n    elog: EventGroupType,\n):\n    self.log = logging.getLogger(\"Updater\")\n    self.elog = elog\n\n    self.model_spec = platform_config.model_spec\n    self.config = UpdaterConfig.model_validate(platform_config.config.get(\"updater\", {}))\n    self.db = db\n    self.task_queue_writer = task_queue_writer\n    self.scheduler = scheduler\n\n    self.enabled = platform_config.process_index == 0\n\n    if not self.enabled:\n        return\n\n    # Get state cache\n    self.cache = UpdaterCache(self.db.get_module_cache(\"Updater\"))\n    self.cache.register_management(\n        scheduler, self.config.cache_management_cron, self.config.cache_max_entries\n    )\n\n    self.update_thread_hooks = defaultdict(dict)\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.Updater.register_record_update_hook","title":"register_record_update_hook","text":"<pre><code>register_record_update_hook(hook: Callable[[str, str, dict], list[DataPointTask]], hook_id: str, entity_type: str, period: ParsedTimedelta)\n</code></pre> <p>Registers a hook for periodic update of entities of the specified type.</p> <p>The hook receives the entity type, the entity ID and the master record.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>@validate_call\ndef register_record_update_hook(\n    self,\n    hook: Callable[[str, str, dict], list[DataPointTask]],\n    hook_id: str,\n    entity_type: str,\n    period: ParsedTimedelta,\n):\n    \"\"\"Registers a hook for periodic update of entities of the specified type.\n\n    The hook receives the entity type, the entity ID and the master record.\n    \"\"\"\n    self._register_hook(hook, hook_id, entity_type, period.total_seconds(), eid_only=False)\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.Updater.register_eid_update_hook","title":"register_eid_update_hook","text":"<pre><code>register_eid_update_hook(hook: Callable[[str, str], list[DataPointTask]], hook_id: str, entity_type: str, period: ParsedTimedelta)\n</code></pre> <p>Registers a hook for periodic update of entities of the specified type.</p> <p>The hook receives the entity type and the entity ID.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>@validate_call\ndef register_eid_update_hook(\n    self,\n    hook: Callable[[str, str], list[DataPointTask]],\n    hook_id: str,\n    entity_type: str,\n    period: ParsedTimedelta,\n):\n    \"\"\"Registers a hook for periodic update of entities of the specified type.\n\n    The hook receives the entity type and the entity ID.\n    \"\"\"\n    self._register_hook(hook, hook_id, entity_type, period.total_seconds(), eid_only=True)\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.Updater.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Starts the updater.</p> <p>Will fetch the state of the updater from the cache and schedule the update threads.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>def start(self):\n    \"\"\"\n    Starts the updater.\n\n    Will fetch the state of the updater from the cache and schedule the update threads.\n    \"\"\"\n    if not self.enabled:\n        return\n\n    # Get all unfinished progress states\n    saved_states = {}\n    for state in self.cache.get_unfinished():\n        saved_states[state.thread_id] = state\n\n    # Confirm all saved states have configured hooks, terminate if not\n    for thread_id, state in saved_states.items():\n        if thread_id not in self.update_thread_hooks:\n            self.log.warning(\n                \"Previously configured hooks %s for '%s' entity with period: %ss \"\n                \"match no current configuration, aborting update thread.\",\n                state.hook_ids,\n                state.etype,\n                state.period,\n            )\n            state.finished = True\n            self.cache.upsert(state)\n            continue\n\n        # Find if any new hooks are added\n        configured_hooks = self.update_thread_hooks[thread_id]\n        saved_hook_ids = set(state.hook_ids)\n        configured_hook_ids = set(configured_hooks.keys())\n        new_hook_ids = configured_hook_ids - saved_hook_ids\n        deleted_hook_ids = saved_hook_ids - configured_hook_ids\n\n        # Update the state with new hooks\n        if deleted_hook_ids or new_hook_ids:\n            if deleted_hook_ids:\n                self.log.warning(\n                    \"Previously configured hooks %s were deleted for entity '%s', period: %ss\",\n                    deleted_hook_ids,\n                    state.etype,\n                    state.period,\n                )\n            state.hook_ids = list(configured_hook_ids)\n            self.cache.upsert(state)\n\n    # Add newly configured hooks that are not in the saved states\n    for thread_id, hooks in self.update_thread_hooks.items():\n        if thread_id not in saved_states:\n            state = UpdateThreadState.new(hooks, *thread_id)\n            saved_states[thread_id] = state\n\n    # Schedule the update threads\n    for (period, entity_type, eid_only), state in saved_states.items():\n        if state.finished:\n            continue\n        hooks = self.update_thread_hooks[(period, entity_type, eid_only)]\n        if eid_only:\n            processing_func = self._process_eid_update_batch\n        else:\n            processing_func = self._process_update_batch\n\n        state.total = self.db.get_estimated_entity_count(entity_type)\n        try:\n            total_iterations = self._calculate_iteration_count(state.period)\n            if state.iteration != 0 and total_iterations != state.total_iterations:\n                self.log.info(\"The update period was changed, resetting iteration number\")\n                state.iteration = 0\n            state.total_iterations = total_iterations\n        except ValueError:\n            self.log.error(\n                \"Invalid period configuration for thread: %s, \"\n                \"the update batch period must be smaller than the total period \",\n                state,\n            )\n            raise\n\n        self.scheduler.register(\n            processing_func,\n            func_args=[entity_type, hooks, state],\n            **self.config.update_batch_cron.model_dump(),\n        )\n\n    # Connect the queue writer\n    self.task_queue_writer.connect()\n    self.task_queue_writer.check()  # check presence of needed exchanges\n</code></pre>"},{"location":"reference/core/updater/#dp3.core.updater.Updater.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stops the updater.</p> Source code in <code>dp3/core/updater.py</code> <pre><code>def stop(self):\n    \"\"\"Stops the updater.\"\"\"\n    if not self.enabled:\n        return\n\n    self.task_queue_writer.disconnect()\n</code></pre>"},{"location":"reference/database/","title":"database","text":""},{"location":"reference/database/#dp3.database","title":"dp3.database","text":"<p>A wrapper responsible for communication with the database server.</p>"},{"location":"reference/database/config/","title":"config","text":""},{"location":"reference/database/config/#dp3.database.config","title":"dp3.database.config","text":""},{"location":"reference/database/config/#dp3.database.config.MongoHostConfig","title":"MongoHostConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MongoDB host.</p>"},{"location":"reference/database/config/#dp3.database.config.MongoStandaloneConfig","title":"MongoStandaloneConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MongoDB standalone configuration.</p>"},{"location":"reference/database/config/#dp3.database.config.MongoReplicaConfig","title":"MongoReplicaConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MongoDB replica set configuration.</p>"},{"location":"reference/database/config/#dp3.database.config.StorageConfig","title":"StorageConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Storage configuration.</p>"},{"location":"reference/database/config/#dp3.database.config.MongoConfig","title":"MongoConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Database configuration.</p>"},{"location":"reference/database/database/","title":"database","text":""},{"location":"reference/database/database/#dp3.database.database","title":"dp3.database.database","text":""},{"location":"reference/database/database/#dp3.database.database.EntityDatabase","title":"EntityDatabase","text":"<pre><code>EntityDatabase(config: HierarchicalDict, model_spec: ModelSpec, num_processes: int, process_index: int = 0, elog: Optional[EventGroupType] = None)\n</code></pre> <p>MongoDB database wrapper responsible for whole communication with database server. Initializes database schema based on database configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>HierarchicalDict</code> <p>configuration of database connection (content of database.yml)</p> required <code>model_spec</code> <code>ModelSpec</code> <p>ModelSpec object, configuration of data model (entities and attributes)</p> required <code>process_index</code> <code>int</code> <p>index of worker process - used for sharding metadata</p> <code>0</code> <code>num_processes</code> <code>int</code> <p>number of worker processes</p> required Source code in <code>dp3/database/database.py</code> <pre><code>def __init__(\n    self,\n    config: HierarchicalDict,\n    model_spec: ModelSpec,\n    num_processes: int,\n    process_index: int = 0,\n    elog: Optional[EventGroupType] = None,\n) -&gt; None:\n    self.log = logging.getLogger(\"EntityDatabase\")\n    self.elog = elog or DummyEventGroup()\n\n    db_config = MongoConfig.model_validate(config.get(\"database\", {}))\n\n    self.log.info(\"Connecting to database...\")\n    for attempt, delay in enumerate(RECONNECT_DELAYS):\n        try:\n            self._db = self.connect(db_config)\n            # Check if connected\n            self._db.admin.command(\"ping\")\n        except pymongo.errors.ConnectionFailure as e:\n            if attempt + 1 == len(RECONNECT_DELAYS):\n                raise DatabaseError(\n                    \"Cannot connect to database with specified connection arguments.\"\n                ) from e\n            else:\n                self.log.error(\n                    \"Cannot connect to database (attempt %d, retrying in %ds).\",\n                    attempt + 1,\n                    delay,\n                )\n                time.sleep(delay)\n\n    self._db_schema_config = model_spec\n    self._num_processes = num_processes\n    self._process_index = process_index\n\n    # Init and switch to correct database\n    codec_opts = get_codec_options()\n    self._db = Database(self._db, db_config.db_name, codec_options=codec_opts)\n\n    self.snapshots = SnapshotCollectionContainer(\n        self._db, db_config, model_spec, config.get(\"snapshots\", {})\n    )\n    self._snapshot_bucket_size = db_config.storage.snapshot_bucket_size\n\n    if process_index == 0:\n        self._init_database_schema()\n\n    self.schema_cleaner = SchemaCleaner(\n        self._db, self.get_module_cache(\"Schema\"), self._db_schema_config, config, self.log\n    )\n\n    self._on_entity_delete_one = []\n    self._on_entity_delete_many = []\n\n    self._raw_buffer_locks = {etype: threading.Lock() for etype in model_spec.entities}\n    self._raw_buffers = defaultdict(list)\n    self._master_buffer_locks = {etype: threading.Lock() for etype in model_spec.entities}\n    self._master_buffers = defaultdict(dict)\n\n    self._sched = Scheduler()\n    seconds = \",\".join(\n        f\"{int(i)}\" for i in range(60) if int(i - process_index) % min(num_processes, 3) == 0\n    )\n    self._sched.register(self._push_raw, second=seconds, misfire_grace_time=5)\n    self._sched.register(self._push_master, second=seconds, misfire_grace_time=5)\n\n    self.log.info(\"Database successfully initialized!\")\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Starts the database sync of raw datapoint inserts.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Starts the database sync of raw datapoint inserts.\"\"\"\n    self._sched.start()\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.stop","title":"stop","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stops the database sync, push remaining datapoints.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stops the database sync, push remaining datapoints.\"\"\"\n    self._sched.stop()\n    self._push_raw()\n    self._push_master()\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.register_on_entity_delete","title":"register_on_entity_delete","text":"<pre><code>register_on_entity_delete(f_one: Callable[[str, AnyEidT], None], f_many: Callable[[str, list[AnyEidT]], None])\n</code></pre> <p>Registers function to be called when entity is forcibly deleted.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def register_on_entity_delete(\n    self, f_one: Callable[[str, AnyEidT], None], f_many: Callable[[str, list[AnyEidT]], None]\n):\n    \"\"\"Registers function to be called when entity is forcibly deleted.\"\"\"\n    self._on_entity_delete_one.append(f_one)\n    self._on_entity_delete_many.append(f_many)\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.update_schema","title":"update_schema","text":"<pre><code>update_schema()\n</code></pre> <p>Checks whether schema saved in database is up-to-date and updates it if necessary.</p> <p>Will NOT perform any changes in master collections on conflicting model changes. Any such changes must be performed via the CLI.</p> <p>As this method modifies the schema collection, it should be called only by the main worker.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def update_schema(self):\n    \"\"\"\n    Checks whether schema saved in database is up-to-date and updates it if necessary.\n\n    Will NOT perform any changes in master collections on conflicting model changes.\n    Any such changes must be performed via the CLI.\n\n    As this method modifies the schema collection, it should be called only by the main worker.\n    \"\"\"\n    try:\n        self.schema_cleaner.safe_update_schema()\n    except ValueError as e:\n        raise DatabaseError(\"Schema update failed. Please run `dp3 schema-update`.\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.await_updated_schema","title":"await_updated_schema","text":"<pre><code>await_updated_schema()\n</code></pre> <p>Checks whether schema saved in database is up-to-date and awaits its update by the main worker on mismatch.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def await_updated_schema(self):\n    \"\"\"\n    Checks whether schema saved in database is up-to-date and awaits its update\n    by the main worker on mismatch.\n    \"\"\"\n    self.schema_cleaner.await_updated()\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.insert_datapoints","title":"insert_datapoints","text":"<pre><code>insert_datapoints(eid: AnyEidT, dps: list[DataPointBase], new_entity: bool = False) -&gt; None\n</code></pre> <p>Inserts datapoint to raw data collection and updates master record.</p> <p>Raises DatabaseError when insert or update fails.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def insert_datapoints(\n    self, eid: AnyEidT, dps: list[DataPointBase], new_entity: bool = False\n) -&gt; None:\n    \"\"\"Inserts datapoint to raw data collection and updates master record.\n\n    Raises DatabaseError when insert or update fails.\n    \"\"\"\n    if len(dps) == 0:\n        return\n\n    etype = dps[0].etype\n\n    # Check `etype`\n    self._assert_etype_exists(etype)\n    self._assert_eid_correct_dtype(etype, eid)\n\n    # Insert raw datapoints\n    dps_dicts = [dp.model_dump(exclude={\"attr_type\"}) for dp in dps]\n    with self._raw_buffer_locks[etype]:\n        self._raw_buffers[etype].extend(dps_dicts)\n\n    # Update master document\n    master_changes = {\"pushes\": defaultdict(list), \"$set\": {}}\n    for dp in dps:\n        attr_spec = self._db_schema_config.attr(etype, dp.attr)\n\n        if attr_spec.t in AttrType.PLAIN | AttrType.OBSERVATIONS and attr_spec.is_iterable:\n            v = [elem.model_dump() if isinstance(elem, BaseModel) else elem for elem in dp.v]\n        else:\n            v = dp.v.model_dump() if isinstance(dp.v, BaseModel) else dp.v\n\n        # Rewrite value of plain attribute\n        if attr_spec.t == AttrType.PLAIN:\n            master_changes[\"$set\"][dp.attr] = {\"v\": v, \"ts_last_update\": datetime.now()}\n\n        # Push new data of observation\n        if attr_spec.t == AttrType.OBSERVATIONS:\n            master_changes[\"pushes\"][dp.attr].append(\n                {\"t1\": dp.t1, \"t2\": dp.t2, \"v\": v, \"c\": dp.c}\n            )\n\n        # Push new data of timeseries\n        if attr_spec.t == AttrType.TIMESERIES:\n            master_changes[\"pushes\"][dp.attr].append({\"t1\": dp.t1, \"t2\": dp.t2, \"v\": v})\n\n    if new_entity:\n        master_changes[\"$set\"][\"#hash\"] = HASH(f\"{etype}:{eid}\")\n        master_changes[\"$set\"][\"#time_created\"] = datetime.now()\n\n    with self._master_buffer_locks[etype]:\n        if eid in self._master_buffers[etype]:\n            for attr, push_dps in master_changes[\"pushes\"].items():\n                if attr in self._master_buffers[etype][eid][\"pushes\"]:\n                    self._master_buffers[etype][eid][\"pushes\"][attr].extend(push_dps)\n                else:\n                    self._master_buffers[etype][eid][\"pushes\"][attr] = push_dps\n            self._master_buffers[etype][eid][\"$set\"].update(master_changes[\"$set\"])\n        else:\n            self._master_buffers[etype][eid] = master_changes\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.update_master_records","title":"update_master_records","text":"<pre><code>update_master_records(etype: str, eids: list[AnyEidT], records: list[dict]) -&gt; None\n</code></pre> <p>Replace master records of <code>etype</code>:<code>eid</code> with the provided <code>records</code>.</p> <p>Raises DatabaseError when update fails.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def update_master_records(self, etype: str, eids: list[AnyEidT], records: list[dict]) -&gt; None:\n    \"\"\"Replace master records of `etype`:`eid` with the provided `records`.\n\n    Raises DatabaseError when update fails.\n    \"\"\"\n    master_col = self._master_col(etype)\n    try:\n        res = master_col.bulk_write(\n            [\n                ReplaceOne({\"_id\": eid}, record, upsert=True)\n                for eid, record in zip(eids, records)\n            ],\n            ordered=False,\n        )\n        self.log.debug(\"Updated master records of %s (%s).\", etype, len(eids))\n        for error in res.bulk_api_result.get(\"writeErrors\", []):\n            self.log.error(\"Error in bulk write: %s\", error)\n    except Exception as e:\n        raise DatabaseError(f\"Update of master records failed: {e}\\n{records}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.extend_ttl","title":"extend_ttl","text":"<pre><code>extend_ttl(etype: str, eid: AnyEidT, ttl_tokens: dict[str, datetime])\n</code></pre> <p>Extends TTL of given <code>etype</code>:<code>eid</code> by <code>ttl_tokens</code>.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def extend_ttl(self, etype: str, eid: AnyEidT, ttl_tokens: dict[str, datetime]):\n    \"\"\"Extends TTL of given `etype`:`eid` by `ttl_tokens`.\"\"\"\n    extensions = {\n        f\"#ttl.{token_name}\": token_value for token_name, token_value in ttl_tokens.items()\n    }\n    with self._master_buffer_locks[etype]:\n        buf = self._master_buffers[etype]\n        if eid in buf:\n            if \"$max\" in buf[eid]:\n                for ttl_name, this_val in extensions.items():\n                    curr_val = buf[eid][\"$max\"].get(ttl_name, datetime.min)\n                    buf[eid][\"$max\"][ttl_name] = max(curr_val, this_val)\n            else:\n                self._master_buffers[etype][eid][\"$max\"] = extensions\n        else:\n            self._master_buffers[etype][eid] = {\"$max\": extensions}\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.remove_expired_ttls","title":"remove_expired_ttls","text":"<pre><code>remove_expired_ttls(etype: str, expired_eid_ttls: dict[AnyEidT, list[str]])\n</code></pre> <p>Removes expired TTL of given <code>etype</code>:<code>eid</code>.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def remove_expired_ttls(self, etype: str, expired_eid_ttls: dict[AnyEidT, list[str]]):\n    \"\"\"Removes expired TTL of given `etype`:`eid`.\"\"\"\n    master_col = self._master_col(etype)\n    try:\n        res = master_col.bulk_write(\n            [\n                UpdateOne(\n                    {\"_id\": eid},\n                    {\"$unset\": {f\"#ttl.{token_name}\": \"\" for token_name in expired_ttls}},\n                )\n                for eid, expired_ttls in expired_eid_ttls.items()\n            ]\n        )\n        self.log.debug(\n            \"Removed expired TTL of %s: (%s, modified %s).\",\n            etype,\n            len(expired_eid_ttls),\n            res.modified_count,\n        )\n    except Exception as e:\n        raise DatabaseError(f\"TTL update failed: {e}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.delete_eids","title":"delete_eids","text":"<pre><code>delete_eids(etype: str, eids: list[AnyEidT])\n</code></pre> <p>Delete master record and all snapshots of <code>etype</code>:<code>eids</code>.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def delete_eids(self, etype: str, eids: list[AnyEidT]):\n    \"\"\"Delete master record and all snapshots of `etype`:`eids`.\"\"\"\n    master_col = self._master_col(etype)\n    with self._master_buffer_locks[etype]:\n        for eid in eids:\n            if eid in self._master_buffers[etype]:\n                del self._master_buffers[etype][eid]\n    try:\n        res = master_col.delete_many({\"_id\": {\"$in\": eids}})\n        self.log.debug(\n            \"Deleted %s master records of %s (%s).\", res.deleted_count, etype, len(eids)\n        )\n        self.elog.log(\"record_removed\", count=res.deleted_count)\n    except Exception as e:\n        raise DatabaseError(f\"Delete of master record failed: {e}\\n{eids}\") from e\n    self.snapshots.delete_eids(etype, eids)\n\n    for f in self._on_entity_delete_many:\n        try:\n            f(etype, eids)\n        except Exception as e:\n            self.log.exception(\"Error in on_entity_delete_many callback %s: %s\", f, e)\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.delete_eid","title":"delete_eid","text":"<pre><code>delete_eid(etype: str, eid: AnyEidT)\n</code></pre> <p>Delete master record and all snapshots of <code>etype</code>:<code>eid</code>.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def delete_eid(self, etype: str, eid: AnyEidT):\n    \"\"\"Delete master record and all snapshots of `etype`:`eid`.\"\"\"\n    master_col = self._master_col(etype)\n    with self._master_buffer_locks[etype]:\n        if eid in self._master_buffers[etype]:\n            del self._master_buffers[etype][eid]\n    try:\n        master_col.delete_one({\"_id\": eid})\n        self.log.debug(\"Deleted master record of %s/%s.\", etype, eid)\n        self.elog.log(\"record_removed\")\n    except Exception as e:\n        raise DatabaseError(f\"Delete of master record failed: {e}\\n{eid}\") from e\n    self.snapshots.delete_eid(etype, eid)\n\n    for f in self._on_entity_delete_one:\n        try:\n            f(etype, eid)\n        except Exception as e:\n            self.log.exception(\"Error in on_entity_delete_one callback %s: %s\", f, e)\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.mark_all_entity_dps_t2","title":"mark_all_entity_dps_t2","text":"<pre><code>mark_all_entity_dps_t2(etype: str, attrs: list[str]) -&gt; UpdateResult\n</code></pre> <p>Updates the <code>min_t2s</code> of the master records of <code>etype</code> for all records.</p> <p>Periodically called for all <code>etype</code>s from HistoryManager.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def mark_all_entity_dps_t2(self, etype: str, attrs: list[str]) -&gt; UpdateResult:\n    \"\"\"\n    Updates the `min_t2s` of the master records of `etype` for all records.\n\n    Periodically called for all `etype`s from HistoryManager.\n    \"\"\"\n    master_col = self._master_col(etype)\n    try:\n        return master_col.update_many(\n            {},\n            [\n                {\n                    \"$set\": {\n                        attr_name: {\n                            \"$cond\": {\n                                \"if\": {\n                                    \"$eq\": [\n                                        {\"$size\": {\"$ifNull\": [f\"${attr_name}\", []]}},\n                                        0,\n                                    ]\n                                },\n                                \"then\": \"$$REMOVE\",\n                                \"else\": f\"${attr_name}\",\n                            }\n                        }\n                        for attr_name in attrs\n                    }\n                    | {\n                        f\"#min_t2s.{attr_name}\": {\n                            \"$cond\": {\n                                \"if\": {\n                                    \"$eq\": [\n                                        {\"$size\": {\"$ifNull\": [f\"${attr_name}\", []]}},\n                                        0,\n                                    ]\n                                },\n                                \"then\": \"$$REMOVE\",\n                                \"else\": {\"$min\": f\"${attr_name}.t2\"},\n                            }\n                        }\n                        for attr_name in attrs\n                    }\n                }\n            ],\n        )\n    except Exception as e:\n        raise DatabaseError(f\"Update of min_t2s failed: {e}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.delete_old_dps","title":"delete_old_dps","text":"<pre><code>delete_old_dps(etype: str, attr_name: str, t_old: datetime) -&gt; UpdateResult\n</code></pre> <p>Delete old datapoints from master collection.</p> <p>Periodically called for all <code>etype</code>s from HistoryManager.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def delete_old_dps(self, etype: str, attr_name: str, t_old: datetime) -&gt; UpdateResult:\n    \"\"\"Delete old datapoints from master collection.\n\n    Periodically called for all `etype`s from HistoryManager.\n    \"\"\"\n    master_col = self._master_col(etype, write_concern=WriteConcern(w=1))\n    try:\n        return master_col.update_many(\n            {f\"#min_t2s.{attr_name}\": {\"$lt\": t_old}},\n            [\n                {\n                    \"$set\": {\n                        attr_name: {\n                            \"$filter\": {\n                                \"input\": f\"${attr_name}\",\n                                \"cond\": {\"$gte\": [\"$$this.t2\", t_old]},\n                            }\n                        }\n                    }\n                },\n                {\n                    \"$set\": {\n                        f\"#min_t2s.{attr_name}\": {\n                            \"$cond\": {\n                                \"if\": {\n                                    \"$eq\": [\n                                        {\"$size\": {\"$ifNull\": [f\"${attr_name}\", []]}},\n                                        0,\n                                    ]\n                                },\n                                \"then\": \"$$REMOVE\",\n                                \"else\": {\"$min\": f\"${attr_name}.t2\"},\n                            }\n                        }\n                    },\n                },\n            ],\n        )\n    except Exception as e:\n        raise DatabaseError(f\"Delete of old datapoints failed: {e}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.delete_link_dps","title":"delete_link_dps","text":"<pre><code>delete_link_dps(etype: str, affected_eids: list[AnyEidT], attr_name: str, eid_to: AnyEidT) -&gt; None\n</code></pre> <p>Delete link datapoints from master collection.</p> <p>Called from LinkManager for deleted entities.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def delete_link_dps(\n    self, etype: str, affected_eids: list[AnyEidT], attr_name: str, eid_to: AnyEidT\n) -&gt; None:\n    \"\"\"Delete link datapoints from master collection.\n\n    Called from LinkManager for deleted entities.\n    \"\"\"\n    master_col = self._master_col(etype)\n    attr_type = self._db_schema_config.attr(etype, attr_name).t\n    filter_cond = {\"_id\": {\"$in\": affected_eids}}\n    try:\n        if attr_type == AttrType.OBSERVATIONS:\n            update_pull = {\"$pull\": {attr_name: {\"v.eid\": eid_to}}}\n            master_col.update_many(filter_cond, update_pull)\n        elif attr_type == AttrType.PLAIN:\n            update_unset = {\"$unset\": {attr_name: \"\"}}\n            master_col.update_many(filter_cond, update_unset)\n        else:\n            raise ValueError(f\"Unsupported attribute type: {attr_type}\")\n    except Exception as e:\n        raise DatabaseError(f\"Delete of link datapoints failed: {e}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.delete_many_link_dps","title":"delete_many_link_dps","text":"<pre><code>delete_many_link_dps(etypes: list[str], affected_eids: list[list[AnyEidT]], attr_names: list[str], eids_to: list[list[AnyEidT]]) -&gt; None\n</code></pre> <p>Delete link datapoints from master collection.</p> <p>Called from LinkManager for deleted entities, when deleting multiple entities.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def delete_many_link_dps(\n    self,\n    etypes: list[str],\n    affected_eids: list[list[AnyEidT]],\n    attr_names: list[str],\n    eids_to: list[list[AnyEidT]],\n) -&gt; None:\n    \"\"\"Delete link datapoints from master collection.\n\n    Called from LinkManager for deleted entities, when deleting multiple entities.\n    \"\"\"\n    try:\n        updates = []\n        for etype, affected_eid_list, attr_name, eid_to_list in zip(\n            etypes, affected_eids, attr_names, eids_to\n        ):\n            master_col = self._master_col(etype)\n            attr_type = self._db_schema_config.attr(etype, attr_name).t\n            filter_cond = {\"_id\": {\"$in\": affected_eid_list}}\n            if attr_type == AttrType.OBSERVATIONS:\n                update_pull = {\"$pull\": {attr_name: {\"v.eid\": {\"$in\": eid_to_list}}}}\n                updates.append(UpdateMany(filter_cond, update_pull))\n            elif attr_type == AttrType.PLAIN:\n                update_unset = {\"$unset\": {attr_name: \"\"}}\n                updates.append(UpdateMany(filter_cond, update_unset))\n            else:\n                raise ValueError(f\"Unsupported attribute type: {attr_type}\")\n            master_col.bulk_write(updates)\n    except Exception as e:\n        raise DatabaseError(f\"Delete of link datapoints failed: {e}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.get_master_record","title":"get_master_record","text":"<pre><code>get_master_record(etype: str, eid: AnyEidT, **kwargs) -&gt; dict\n</code></pre> <p>Get current master record for etype/eid.</p> <p>If doesn't exist, returns {}.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_master_record(self, etype: str, eid: AnyEidT, **kwargs) -&gt; dict:\n    \"\"\"Get current master record for etype/eid.\n\n    If doesn't exist, returns {}.\n    \"\"\"\n    self._assert_etype_exists(etype)\n    self._assert_eid_correct_dtype(etype, eid)\n\n    master_col = self._master_col(etype)\n\n    return master_col.find_one({\"_id\": eid}, **kwargs) or {}\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.ekey_exists","title":"ekey_exists","text":"<pre><code>ekey_exists(etype: str, eid: AnyEidT) -&gt; bool\n</code></pre> <p>Checks whether master record for etype/eid exists</p> Source code in <code>dp3/database/database.py</code> <pre><code>def ekey_exists(self, etype: str, eid: AnyEidT) -&gt; bool:\n    \"\"\"Checks whether master record for etype/eid exists\"\"\"\n    with self._master_buffer_locks[etype]:\n        in_cache = eid in self._master_buffers[etype]\n    return in_cache or bool(self.get_master_record(etype, eid, projection={\"_id\": 1}))\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.get_master_records","title":"get_master_records","text":"<pre><code>get_master_records(etype: str, **kwargs) -&gt; Cursor\n</code></pre> <p>Get cursor to current master records of etype.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_master_records(self, etype: str, **kwargs) -&gt; Cursor:\n    \"\"\"Get cursor to current master records of etype.\"\"\"\n    self._assert_etype_exists(etype)\n\n    master_col = self._master_col(etype)\n    return master_col.find({}, **kwargs)\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.get_worker_master_records","title":"get_worker_master_records","text":"<pre><code>get_worker_master_records(worker_index: int, worker_cnt: int, etype: str, query_filter: dict = None, **kwargs) -&gt; Cursor\n</code></pre> <p>Get cursor to current master records of etype.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_worker_master_records(\n    self, worker_index: int, worker_cnt: int, etype: str, query_filter: dict = None, **kwargs\n) -&gt; Cursor:\n    \"\"\"Get cursor to current master records of etype.\"\"\"\n    if etype not in self._db_schema_config.entities:\n        raise DatabaseError(f\"Entity '{etype}' does not exist\")\n\n    query_filter = {} if query_filter is None else query_filter\n    master_col = self._master_col(etype)\n    return master_col.find(\n        {\"#hash\": {\"$mod\": [worker_cnt, worker_index]}, **query_filter}, **kwargs\n    )\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.get_value_or_history","title":"get_value_or_history","text":"<pre><code>get_value_or_history(etype: str, attr_name: str, eid: AnyEidT, t1: Optional[datetime] = None, t2: Optional[datetime] = None) -&gt; dict\n</code></pre> <p>Gets current value and/or history of attribute for given <code>eid</code>.</p> <p>Depends on attribute type: - plain: just (current) value - observations: (current) value and history stored in master record (optionally filtered) - timeseries: just history stored in master record (optionally filtered)</p> <p>Returns dict with two keys: <code>current_value</code> and <code>history</code> (list of values).</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_value_or_history(\n    self,\n    etype: str,\n    attr_name: str,\n    eid: AnyEidT,\n    t1: Optional[datetime] = None,\n    t2: Optional[datetime] = None,\n) -&gt; dict:\n    \"\"\"Gets current value and/or history of attribute for given `eid`.\n\n    Depends on attribute type:\n    - plain: just (current) value\n    - observations: (current) value and history stored in master record (optionally filtered)\n    - timeseries: just history stored in master record (optionally filtered)\n\n    Returns dict with two keys: `current_value` and `history` (list of values).\n    \"\"\"\n    self._assert_etype_exists(etype)\n    self._assert_eid_correct_dtype(etype, eid)\n\n    attr_spec = self._db_schema_config.attr(etype, attr_name)\n\n    result = {\"current_value\": None, \"history\": []}\n\n    # Add current value to the result\n    if attr_spec.t == AttrType.PLAIN:\n        result[\"current_value\"] = (\n            self.get_master_record(etype, eid).get(attr_name, {}).get(\"v\", None)\n        )\n    elif attr_spec.t == AttrType.OBSERVATIONS:\n        result[\"current_value\"] = self.snapshots.get_latest_one(etype, eid).get(attr_name, None)\n\n    # Add history\n    if attr_spec.t == AttrType.OBSERVATIONS:\n        result[\"history\"] = self.get_observation_history(etype, attr_name, eid, t1, t2)\n    elif attr_spec.t == AttrType.TIMESERIES:\n        result[\"history\"] = self.get_timeseries_history(etype, attr_name, eid, t1, t2)\n\n    return result\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.estimate_count_eids","title":"estimate_count_eids","text":"<pre><code>estimate_count_eids(etype: str) -&gt; int\n</code></pre> <p>Estimates count of <code>eid</code>s in given <code>etype</code></p> Source code in <code>dp3/database/database.py</code> <pre><code>def estimate_count_eids(self, etype: str) -&gt; int:\n    \"\"\"Estimates count of `eid`s in given `etype`\"\"\"\n    self._assert_etype_exists(etype)\n\n    master_col = self._master_col(etype)\n    return master_col.estimated_document_count({})\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.save_metadata","title":"save_metadata","text":"<pre><code>save_metadata(time: datetime, metadata: dict, worker_id: Optional[int] = None)\n</code></pre> <p>Saves metadata dict under the caller module and passed timestamp.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def save_metadata(self, time: datetime, metadata: dict, worker_id: Optional[int] = None):\n    \"\"\"Saves metadata dict under the caller module and passed timestamp.\"\"\"\n    module = get_caller_id()\n    metadata[\"_id\"] = self._get_metadata_id(module, time, worker_id)\n    metadata[\"#module\"] = module\n    metadata[\"#time_created\"] = time\n    metadata[\"#last_update\"] = datetime.now()\n    try:\n        self._db[\"#metadata\"].insert_one(metadata)\n        self.log.debug(\"Inserted metadata %s: %s\", metadata[\"_id\"], metadata)\n    except Exception as e:\n        raise DatabaseError(f\"Insert of metadata failed: {e}\\n{metadata}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.update_metadata","title":"update_metadata","text":"<pre><code>update_metadata(time: datetime, metadata: dict, increase: dict = None, worker_id: Optional[int] = None)\n</code></pre> <p>Updates existing metadata of caller module and passed timestamp.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def update_metadata(\n    self, time: datetime, metadata: dict, increase: dict = None, worker_id: Optional[int] = None\n):\n    \"\"\"Updates existing metadata of caller module and passed timestamp.\"\"\"\n    module = get_caller_id()\n    metadata_id = self._get_metadata_id(module, time, worker_id)\n    metadata[\"#last_update\"] = datetime.now()\n\n    changes = {\"$set\": metadata} if increase is None else {\"$set\": metadata, \"$inc\": increase}\n\n    try:\n        res = self._db[\"#metadata\"].update_one({\"_id\": metadata_id}, changes, upsert=True)\n        self.log.debug(\n            \"Updated metadata %s, changes: %s, result: %s\", metadata_id, changes, res.raw_result\n        )\n    except Exception as e:\n        raise DatabaseError(f\"Update of metadata failed: {e}\\n{metadata_id}, {changes}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.get_observation_history","title":"get_observation_history","text":"<pre><code>get_observation_history(etype: str, attr_name: str, eid: AnyEidT, t1: datetime = None, t2: datetime = None, sort: int = None) -&gt; list[dict]\n</code></pre> <p>Get full (or filtered) history of observation attribute.</p> <p>This method is useful for displaying <code>eid</code>'s history on web. Also used to feed data into <code>get_timeseries_history()</code>.</p> <p>Parameters:</p> Name Type Description Default <code>etype</code> <code>str</code> <p>entity type</p> required <code>attr_name</code> <code>str</code> <p>name of attribute</p> required <code>eid</code> <code>AnyEidT</code> <p>id of entity, to which data-points correspond</p> required <code>t1</code> <code>datetime</code> <p>left value of time interval (inclusive)</p> <code>None</code> <code>t2</code> <code>datetime</code> <p>right value of time interval (inclusive)</p> <code>None</code> <code>sort</code> <code>int</code> <p>sort by timestamps - 0: ascending order by t1, 1: descending order by t2, None: don't sort</p> <code>None</code> <p>Returns:     list of dicts (reduced datapoints)</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_observation_history(\n    self,\n    etype: str,\n    attr_name: str,\n    eid: AnyEidT,\n    t1: datetime = None,\n    t2: datetime = None,\n    sort: int = None,\n) -&gt; list[dict]:\n    \"\"\"Get full (or filtered) history of observation attribute.\n\n    This method is useful for displaying `eid`'s history on web.\n    Also used to feed data into `get_timeseries_history()`.\n\n    Args:\n        etype: entity type\n        attr_name: name of attribute\n        eid: id of entity, to which data-points correspond\n        t1: left value of time interval (inclusive)\n        t2: right value of time interval (inclusive)\n        sort: sort by timestamps - 0: ascending order by t1, 1: descending order by t2,\n            None: don't sort\n    Returns:\n        list of dicts (reduced datapoints)\n    \"\"\"\n    t1 = datetime.fromtimestamp(0) if t1 is None else t1.replace(tzinfo=None)\n    t2 = datetime.now() if t2 is None else t2.replace(tzinfo=None)\n\n    # Get attribute history\n    mr = self.get_master_record(etype, eid)\n    attr_history = mr.get(attr_name, [])\n\n    # Filter\n    attr_history_filtered = [row for row in attr_history if row[\"t1\"] &lt;= t2 and row[\"t2\"] &gt;= t1]\n\n    # Sort\n    if sort == 1:\n        attr_history_filtered.sort(key=lambda row: row[\"t1\"])\n    elif sort == 2:\n        attr_history_filtered.sort(key=lambda row: row[\"t2\"], reverse=True)\n\n    return attr_history_filtered\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.get_timeseries_history","title":"get_timeseries_history","text":"<pre><code>get_timeseries_history(etype: str, attr_name: str, eid: AnyEidT, t1: datetime = None, t2: datetime = None, sort: int = None) -&gt; list[dict]\n</code></pre> <p>Get full (or filtered) history of timeseries attribute. Outputs them in format: <pre><code>    [\n        {\n            \"t1\": ...,\n            \"t2\": ...,\n            \"v\": {\n                \"series1\": ...,\n                \"series2\": ...\n            }\n        },\n        ...\n    ]\n</code></pre> This method is useful for displaying <code>eid</code>'s history on web.</p> <p>Parameters:</p> Name Type Description Default <code>etype</code> <code>str</code> <p>entity type</p> required <code>attr_name</code> <code>str</code> <p>name of attribute</p> required <code>eid</code> <code>AnyEidT</code> <p>id of entity, to which data-points correspond</p> required <code>t1</code> <code>datetime</code> <p>left value of time interval (inclusive)</p> <code>None</code> <code>t2</code> <code>datetime</code> <p>right value of time interval (inclusive)</p> <code>None</code> <code>sort</code> <code>int</code> <p>sort by timestamps - <code>0</code>: ascending order by <code>t1</code>, <code>1</code>: descending order by <code>t2</code>, <code>None</code>: don't sort</p> <code>None</code> <p>Returns:      list of dicts (reduced datapoints) - each represents just one point at time</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_timeseries_history(\n    self,\n    etype: str,\n    attr_name: str,\n    eid: AnyEidT,\n    t1: datetime = None,\n    t2: datetime = None,\n    sort: int = None,\n) -&gt; list[dict]:\n    \"\"\"Get full (or filtered) history of timeseries attribute.\n    Outputs them in format:\n    ```\n        [\n            {\n                \"t1\": ...,\n                \"t2\": ...,\n                \"v\": {\n                    \"series1\": ...,\n                    \"series2\": ...\n                }\n            },\n            ...\n        ]\n    ```\n    This method is useful for displaying `eid`'s history on web.\n\n    Args:\n        etype: entity type\n        attr_name: name of attribute\n        eid: id of entity, to which data-points correspond\n        t1: left value of time interval (inclusive)\n        t2: right value of time interval (inclusive)\n        sort: sort by timestamps - `0`: ascending order by `t1`, `1`: descending order by `t2`,\n            `None`: don't sort\n    Returns:\n         list of dicts (reduced datapoints) - each represents just one point at time\n    \"\"\"\n    t1 = datetime.fromtimestamp(0) if t1 is None else t1.replace(tzinfo=None)\n    t2 = datetime.now() if t2 is None else t2.replace(tzinfo=None)\n\n    attr_history = self.get_observation_history(etype, attr_name, eid, t1, t2, sort)\n    if not attr_history:\n        return []\n\n    attr_history_split = self._split_timeseries_dps(etype, attr_name, attr_history)\n\n    # Filter out rows outside [t1, t2] interval\n    attr_history_filtered = [\n        row for row in attr_history_split if row[\"t1\"] &lt;= t2 and row[\"t2\"] &gt;= t1\n    ]\n\n    return attr_history_filtered\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.move_raw_to_archive","title":"move_raw_to_archive","text":"<pre><code>move_raw_to_archive(etype: str)\n</code></pre> <p>Rename the current raw collection to archive collection.</p> <p>Multiple archive collections can exist for one entity type, though they are exported and dropped over time.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def move_raw_to_archive(self, etype: str):\n    \"\"\"Rename the current raw collection to archive collection.\n\n    Multiple archive collections can exist for one entity type,\n    though they are exported and dropped over time.\n    \"\"\"\n    raw_col_name = self._raw_col_name(etype)\n    archive_col_name = self._get_new_archive_col_name(etype)\n    try:\n        if self._db.list_collection_names(filter={\"name\": raw_col_name}):\n            self._db[raw_col_name].rename(archive_col_name)\n            return archive_col_name\n        return None\n    except Exception as e:\n        raise DatabaseError(f\"Move of raw collection failed: {e}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.get_archive","title":"get_archive","text":"<pre><code>get_archive(etype: str, after: datetime, before: datetime) -&gt; Iterator[Cursor]\n</code></pre> <p>Get archived raw datapoints where <code>t1</code> is in &lt;<code>after</code>, <code>before</code>).</p> <p>All plain datapoints will be returned (default).</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_archive(self, etype: str, after: datetime, before: datetime) -&gt; Iterator[Cursor]:\n    \"\"\"Get archived raw datapoints where `t1` is in &lt;`after`, `before`).\n\n    All plain datapoints will be returned (default).\n    \"\"\"\n    query_filter = {\"$or\": [{\"t1\": {\"$gte\": after, \"$lt\": before}}, {\"t1\": None}]}\n    for archive_col in self._archive_cols(etype):\n        try:\n            yield archive_col.find(query_filter)\n        except Exception as e:\n            raise DatabaseError(f\"Archive collection {archive_col} fetch failed: {e}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.delete_old_archived_dps","title":"delete_old_archived_dps","text":"<pre><code>delete_old_archived_dps(etype: str, before: datetime) -&gt; Iterator[DeleteResult]\n</code></pre> <p>Delete archived raw datapoints older than <code>before</code>.</p> <p>Deletes all plain datapoints if <code>plain</code> is <code>True</code> (default).</p> Source code in <code>dp3/database/database.py</code> <pre><code>def delete_old_archived_dps(self, etype: str, before: datetime) -&gt; Iterator[DeleteResult]:\n    \"\"\"Delete archived raw datapoints older than `before`.\n\n    Deletes all plain datapoints if `plain` is `True` (default).\n    \"\"\"\n    query_filter = {\"$or\": [{\"t1\": {\"$lt\": before}}, {\"t1\": None}]}\n    for archive_col in self._archive_cols(etype):\n        try:\n            yield archive_col.delete_many(query_filter)\n        except Exception as e:\n            raise DatabaseError(f\"Delete of old datapoints failed: {e}\") from e\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.drop_empty_archives","title":"drop_empty_archives","text":"<pre><code>drop_empty_archives(etype: str) -&gt; int\n</code></pre> <p>Drop empty archive collections.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def drop_empty_archives(self, etype: str) -&gt; int:\n    \"\"\"Drop empty archive collections.\"\"\"\n    dropped_count = 0\n    for col in self._archive_cols(etype):\n        try:\n            if col.estimated_document_count() &lt; 1000 and col.count_documents({}) == 0:\n                col.drop()\n                dropped_count += 1\n        except Exception as e:\n            raise DatabaseError(f\"Drop of empty archive failed: {e}\") from e\n    return dropped_count\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.get_module_cache","title":"get_module_cache","text":"<pre><code>get_module_cache(override_called_id: Optional[str] = None)\n</code></pre> <p>Return a persistent cache collection for given module name.</p> <p>Module name is determined automatically, but you can override it.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_module_cache(self, override_called_id: Optional[str] = None):\n    \"\"\"Return a persistent cache collection for given module name.\n\n    Module name is determined automatically, but you can override it.\n    \"\"\"\n    module = override_called_id or get_caller_id()\n    self.log.debug(\"Cache collection access: %s\", module)\n    return self._db[f\"#cache#{module}\"]\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.EntityDatabase.get_estimated_entity_count","title":"get_estimated_entity_count","text":"<pre><code>get_estimated_entity_count(entity_type: str) -&gt; int\n</code></pre> <p>Get count of entities of given type.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_estimated_entity_count(self, entity_type: str) -&gt; int:\n    \"\"\"Get count of entities of given type.\"\"\"\n    return self._master_col(entity_type).estimated_document_count()\n</code></pre>"},{"location":"reference/database/database/#dp3.database.database.get_caller_id","title":"get_caller_id","text":"<pre><code>get_caller_id()\n</code></pre> <p>Returns the name of the caller method's class, or function name if caller is not a method.</p> Source code in <code>dp3/database/database.py</code> <pre><code>def get_caller_id():\n    \"\"\"Returns the name of the caller method's class, or function name if caller is not a method.\"\"\"\n    caller = inspect.stack()[2]\n    if module := caller.frame.f_locals.get(\"self\"):\n        return module.__class__.__qualname__\n    return caller.function\n</code></pre>"},{"location":"reference/database/encodings/","title":"encodings","text":""},{"location":"reference/database/encodings/#dp3.database.encodings","title":"dp3.database.encodings","text":""},{"location":"reference/database/exceptions/","title":"exceptions","text":""},{"location":"reference/database/exceptions/#dp3.database.exceptions","title":"dp3.database.exceptions","text":""},{"location":"reference/database/magic/","title":"magic","text":""},{"location":"reference/database/magic/#dp3.database.magic","title":"dp3.database.magic","text":"<p>Magic strings are used to convert non-JSON-native types to their respective objects.</p> <p>For querying non-JSON-native types, you can use the following magic strings:</p> <ul> <li><code>\"$$IPv4{&lt;ip address&gt;}\"</code> - converts to IPv4Address object</li> <li><code>\"$$IPv6{&lt;ip address&gt;}\"</code> - converts to IPv6Address object</li> <li><code>\"$$int{&lt;value&gt;}\"</code> - may be necessary for filtering when <code>eid</code> data type is int</li> <li><code>\"$$Date{&lt;YYYY-mm-ddTHH:MM:ssZ&gt;}\"</code> - converts specified UTC date to UTC datetime object</li> <li><code>\"$$DateTs{&lt;POSIX timestamp&gt;}\"</code> - converts POSIX timestamp (int/float) to UTC datetime object</li> <li><code>\"$$MAC{&lt;mac address&gt;}\"</code> - converts to MACAddress object</li> </ul> <p>To query an IP prefix, use the following magic strings:</p> <ul> <li><code>\"$$IPv4Prefix{&lt;ip address&gt;/&lt;prefix length&gt;}\"</code> - matches prefix</li> <li><code>\"$$IPv6Prefix{&lt;ip address&gt;/&lt;prefix length&gt;}\"</code> - matches prefix</li> </ul> <p>To query a binary <code>_id</code>s of non-string snapshot buckets, use the following magic string:</p> <ul> <li> <p><code>\"$$Binary_ID{&lt;EID object | Valid magic string&gt;}\"</code></p> <ul> <li>converts to filter the exact EID object snapshots, only EID valid types are supported</li> </ul> </li> </ul> <p>There are no attribute name checks (may be added in the future).</p> <p>Generic filter examples:</p> <ul> <li><code>{\"attr1\": {\"$gt\": 10}, \"attr2\": {\"$lt\": 20}}</code></li> <li><code>{\"ip_attr\": \"$$IPv4{127.0.0.1}\"}</code> - converts to IPv4Address object, exact match</li> <li> <p><code>{\"ip_attr\": \"$$IPv4Prefix{127.0.0.0/24}\"}</code></p> <ul> <li>converts to <code>{\"ip_attr\": {\"$gte\": \"$$IPv4{127.0.0.0}\",   \"$lte\": \"$$IPv4{127.0.0.255}\"}}</code></li> </ul> </li> <li> <p><code>{\"ip_attr\": \"$$IPv6{::1}\"}</code> - converts to IPv6Address object, exact match</p> </li> <li> <p><code>{\"ip_attr\": \"$$IPv6Prefix{::1/64}\"}</code></p> <ul> <li>converts to <code>{\"ip_attr\": {\"$gte\": \"$$IPv6{::1}\",   \"$lte\": \"$$IPv6{::1:ffff:ffff:ffff:ffff}\"}}</code></li> </ul> </li> <li> <p><code>{\"_id\": \"$$Binary_ID{$$IPv4{127.0.0.1}}\"}</code></p> <ul> <li>converts to <code>{\"_id\": {\"$gte\": Binary(&lt;IP bytes + min timestamp&gt;),   \"$lt\": Binary(&lt;IP bytes + max timestamp&gt;)}}</code></li> </ul> </li> <li> <p><code>{\"id\": \"$$Binary_ID{$$IPv4Prefix{127.0.0.0/24}}\"}</code></p> <ul> <li>converts to <code>{\"_id\": {\"$gte\": Binary(&lt;IP 127.0.0.0 bytes + min timestamp&gt;),   \"$lte\": Binary(&lt;IP 127.0.0.255 bytes + max timestamp&gt;)}}</code></li> </ul> </li> <li> <p><code>{\"_time_created\": {\"$gte\": \"$$Date{2024-11-07T00:00:00Z}\"}}</code></p> <ul> <li>converts to <code>{\"_time_created\": datetime(2024, 11, 7, 0, 0, 0, tzinfo=timezone.utc)}</code></li> </ul> </li> <li> <p><code>{\"_time_created\": {\"$gte\": \"$$DateTs{1609459200}\"}}</code></p> <ul> <li>converts to <code>{\"_time_created\": datetime(2021, 1, 1, 0, 0, 0, tzinfo=timezone.utc)}</code></li> </ul> </li> <li> <p><code>{\"attr\": \"$$MAC{00:11:22:33:44:55}\"}</code> - converts to MACAddress object, exact match</p> </li> <li><code>{\"_id\": \"$$Binary_ID{$$MAC{Ab-cD-Ef-12-34-56}}\"}</code><ul> <li>converts to <code>{\"_id\": {\"$gte\": Binary(&lt;MAC bytes + min timestamp&gt;),     \"$lt\": Binary(&lt;MAC bytes + max timestamp&gt;)}}</code></li> </ul> </li> </ul>"},{"location":"reference/database/magic/#dp3.database.magic.search_and_replace","title":"search_and_replace","text":"<pre><code>search_and_replace(query: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Search and replace all occurrences of magic strings in the query.</p> <p>A helper function to <code>snapshots.get_latest</code></p> Source code in <code>dp3/database/magic.py</code> <pre><code>def search_and_replace(query: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Search and replace all occurrences of magic strings in the query.\n\n    A helper function to\n    [`snapshots.get_latest`][dp3.database.snapshots.TypedSnapshotCollection.get_latest]\n    \"\"\"\n    if isinstance(query, dict):\n        for key, value in query.items():\n            if isinstance(value, (dict, list)):\n                search_and_replace(value)\n            elif isinstance(value, str):\n                match = magic_regex.match(value)\n                if match:\n                    magic_type, magic_value = match.groups()\n                    if magic_type in magic_string_replacements:\n                        query[key] = magic_string_replacements[magic_type](magic_value, False)\n    if isinstance(query, list):\n        for item in query:\n            search_and_replace(item)\n    return query\n</code></pre>"},{"location":"reference/database/schema_cleaner/","title":"schema_cleaner","text":""},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner","title":"dp3.database.schema_cleaner","text":""},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner","title":"SchemaCleaner","text":"<pre><code>SchemaCleaner(db: Database, schema_col: Collection, model_spec: ModelSpec, config: HierarchicalDict, log: Logger = None)\n</code></pre> <p>Maintains a history of <code>model_spec</code> defined schema, updates the database when needed.</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>Database</code> <p>Target database mapping. (not just a database connection)</p> required <code>schema_col</code> <code>Collection</code> <p>Collection where schema history is stored.</p> required <code>model_spec</code> <code>ModelSpec</code> <p>Model specification.</p> required <code>log</code> <code>Logger</code> <p>Logger instance to serve as parent. If not provided, a new logger will be created.</p> <code>None</code> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def __init__(\n    self,\n    db: Database,\n    schema_col: Collection,\n    model_spec: ModelSpec,\n    config: HierarchicalDict,\n    log: Logger = None,\n):\n    if log is None:\n        self.log = logging.getLogger(\"SchemaCleaner\")\n    else:\n        self.log = log.getChild(\"SchemaCleaner\")\n\n    self._db = db\n    self._model_spec = model_spec\n    self._config = config\n    self.schemas = schema_col\n\n    self.storage = {\n        \"snapshot_bucket_size\": self._config.get(\"database.storage.snapshot_bucket_size\", 32)\n    }\n\n    self.migrations: dict[int, Callable[[dict], dict]] = {\n        2: self.migrate_schema_2_to_3,\n        3: self.migrate_schema_3_to_4,\n    }\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.get_current_schema_doc","title":"get_current_schema_doc","text":"<pre><code>get_current_schema_doc(infer: bool = False) -&gt; dict\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>infer</code> <code>bool</code> <p>Whether to infer the schema if it is not found in the database.</p> <code>False</code> <p>Returns:     Current schema</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def get_current_schema_doc(self, infer: bool = False) -&gt; dict:\n    \"\"\"\n    Args:\n        infer: Whether to infer the schema if it is not found in the database.\n    Returns:\n        Current schema\n    \"\"\"\n    schema_doc = self.schemas.find_one({}, sort=[(\"_id\", pymongo.DESCENDING)])\n    if schema_doc is None and infer:\n        self.log.info(\"No schema found, inferring initial schema from database\")\n        schema = self.infer_current_schema()\n        entity_id_types = self.construct_entity_types()\n        schema_doc = {\n            \"_id\": 0,\n            \"entity_id_types\": entity_id_types,\n            \"schema\": schema,\n            \"storage\": self.storage,\n            \"version\": SCHEMA_VERSION,\n        }\n    return schema_doc\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.safe_update_schema","title":"safe_update_schema","text":"<pre><code>safe_update_schema()\n</code></pre> <p>Checks whether schema saved in database is up-to-date and updates it if necessary.</p> <p>Will NOT perform any changes in master collections on conflicting model changes, but will raise an exception instead. Any such changes must be performed via the CLI.</p> <p>As this method modifies the schema collection, it should be called only by the main worker.</p> <p>The schema collection format is as follows:</p> <pre><code>    {\n        \"_id\": int,\n        entity_id_types: {\n            &lt;entity_type&gt;: &lt;entity_id_type&gt;,\n            ...\n        },\n        \"schema\": { ... },  # (1)!\n        \"storage\": {\n            \"snapshot_bucket_size\": int\n        },\n        \"version\": int\n    }\n</code></pre> <ol> <li>see schema construction</li> </ol> <p>Raises:</p> Type Description <code>ValueError</code> <p>If conflicting changes are detected.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def safe_update_schema(self):\n    \"\"\"\n    Checks whether schema saved in database is up-to-date and updates it if necessary.\n\n    Will NOT perform any changes in master collections on conflicting model changes,\n    but will raise an exception instead.\n    Any such changes must be performed via the CLI.\n\n    As this method modifies the schema collection, it should be called only by the main worker.\n\n    The schema collection format is as follows:\n\n    ```py\n        {\n            \"_id\": int,\n            entity_id_types: {\n                &lt;entity_type&gt;: &lt;entity_id_type&gt;,\n                ...\n            },\n            \"schema\": { ... },  # (1)!\n            \"storage\": {\n                \"snapshot_bucket_size\": int\n            },\n            \"version\": int\n        }\n    ```\n\n    1. see [schema construction][dp3.database.schema_cleaner.SchemaCleaner.construct_schema_doc]\n\n    Raises:\n        ValueError: If conflicting changes are detected.\n    \"\"\"\n    db_schema, config_schema, eid_updates, updates, deleted_entites = self.get_schema_status()\n\n    if db_schema[\"version\"] != config_schema[\"version\"]:\n        self.log.warning(\n            \"Schema version mismatch: %s (DB) != %s (config)\",\n            db_schema[\"version\"],\n            config_schema[\"version\"],\n        )\n        return self._error_on_conflict()\n\n    if db_schema[\"storage\"] != config_schema[\"storage\"]:\n        self.log.warning(\n            \"DB storage settings mismatch: %s (DB) != %s (config)\",\n            db_schema[\"storage\"],\n            config_schema[\"storage\"],\n        )\n        return self._error_on_conflict()\n\n    if eid_updates:\n        for entity in eid_updates:\n            self.log.warning(\n                \"Entity ID type mismatch for %s: %s (DB) -&gt; %s (config)\",\n                entity,\n                db_schema[\"entity_id_types\"][entity],\n                config_schema[\"entity_id_types\"][entity],\n            )\n        return self._error_on_conflict()\n\n    if db_schema[\"schema\"] == config_schema[\"schema\"]:\n        self.log.info(\"Schema OK!\")\n    elif not updates and not deleted_entites:\n        self.schemas.insert_one(config_schema)\n        self.log.info(\"Updated schema, OK now!\")\n    else:\n        return self._error_on_conflict()\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.get_schema_status","title":"get_schema_status","text":"<pre><code>get_schema_status() -&gt; tuple[dict, dict, dict, dict, list]\n</code></pre> <p>Gets the current schema status. <code>database_schema</code> is the schema document from the database. <code>configuration_schema</code> is the schema document constructed from the current configuration. <code>updates</code> is a dictionary of required updates to each entity.</p> <p>Returns:</p> Type Description <code>tuple[dict, dict, dict, dict, list]</code> <p>Tuple of (<code>database_schema</code>, <code>configuration_schema</code>, <code>updates</code>, <code>deleted_entites</code>).</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def get_schema_status(self) -&gt; tuple[dict, dict, dict, dict, list]:\n    \"\"\"\n    Gets the current schema status.\n    `database_schema` is the schema document from the database.\n    `configuration_schema` is the schema document constructed from the current configuration.\n    `updates` is a dictionary of required updates to each entity.\n\n    Returns:\n        Tuple of (`database_schema`, `configuration_schema`, `updates`, `deleted_entites`).\n    \"\"\"\n    schema_doc = self.get_current_schema_doc(infer=True)\n\n    current_schema = self.construct_schema_doc()\n    current_entity_types = self.construct_entity_types()\n\n    new_schema = {\n        \"_id\": schema_doc[\"_id\"],\n        \"entity_id_types\": current_entity_types,\n        \"schema\": current_schema,\n        \"storage\": self.storage,\n        \"version\": SCHEMA_VERSION,\n    }\n\n    if schema_doc == new_schema:\n        return schema_doc, schema_doc, {}, {}, []\n\n    new_schema[\"_id\"] += 1\n\n    eid_updates, updates, deleted_entites = self.detect_changes(\n        schema_doc, current_entity_types, current_schema\n    )\n    return schema_doc, new_schema, eid_updates, updates, deleted_entites\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.detect_changes","title":"detect_changes","text":"<pre><code>detect_changes(db_schema_doc: dict, current_entity_types: dict, current_schema: dict) -&gt; tuple[dict[str, str], dict[str, dict[str, dict[str, str]]], list[str]]\n</code></pre> <p>Detects changes between configured schema and the one saved in the database.</p> <p>Parameters:</p> Name Type Description Default <code>db_schema_doc</code> <code>dict</code> <p>Schema document from the database.</p> required <code>current_entity_types</code> <code>dict</code> <p>Entity ID types from the configuration.</p> required <code>current_schema</code> <code>dict</code> <p>Schema from the configuration.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, str], dict[str, dict[str, dict[str, str]]], list[str]]</code> <p>Tuple of required updates to each entity and list of deleted entites.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def detect_changes(\n    self, db_schema_doc: dict, current_entity_types: dict, current_schema: dict\n) -&gt; tuple[dict[str, str], dict[str, dict[str, dict[str, str]]], list[str]]:\n    \"\"\"\n    Detects changes between configured schema and the one saved in the database.\n\n    Args:\n        db_schema_doc: Schema document from the database.\n        current_entity_types: Entity ID types from the configuration.\n        current_schema: Schema from the configuration.\n\n    Returns:\n        Tuple of required updates to each entity and list of deleted entites.\n    \"\"\"\n    if db_schema_doc[\"version\"] != SCHEMA_VERSION:\n        self.log.info(\"Schema version changed, skipping detecting changes.\")\n        return {}, {}, []\n\n    eid_updates = {}\n    for entity, id_type in db_schema_doc[\"entity_id_types\"].items():\n        if entity not in current_schema:\n            self.log.info(\"Schema breaking change: Entity %s was deleted\", entity)\n            continue\n\n        if id_type != current_entity_types[entity]:\n            self.log.info(\n                \"Schema breaking change: Entity %s ID type changed: %s -&gt; %s\",\n                entity,\n                id_type,\n                current_entity_types[entity],\n            )\n            eid_updates[entity] = \"$drop\"\n\n    if db_schema_doc[\"schema\"] == current_schema:\n        return eid_updates, {}, []\n\n    updates = {}\n    deleted_entites = []\n    for entity, attributes in db_schema_doc[\"schema\"].items():\n        # Unset deleted entities\n        if entity not in current_schema:\n            self.log.info(\"Schema breaking change: Entity %s was deleted\", entity)\n            deleted_entites.append(entity)\n            continue\n\n        entity_updates = defaultdict(dict)\n        current_attributes = current_schema[entity]\n\n        # Unset deleted attributes\n        deleted = set(attributes) - set(current_attributes)\n        for attr in deleted:\n            self.log.info(\"Schema breaking change: Attribute %s.%s was deleted\", entity, attr)\n            entity_updates[\"$unset\"][attr] = \"\"\n\n        for attr, spec in attributes.items():\n            if attr in deleted:\n                continue\n            model = current_attributes[attr]\n\n            for key, val in spec.items():\n                ref = model[key]\n                if ref == val:\n                    continue\n\n                self.log.info(\n                    \"Schema breaking change: '%s' of %s.%s changed: %s -&gt; %s\",\n                    key,\n                    entity,\n                    attr,\n                    val,\n                    ref,\n                )\n                entity_updates[\"$unset\"][attr] = \"\"\n\n        if entity_updates:\n            updates[entity] = entity_updates\n\n    return eid_updates, updates, deleted_entites\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.infer_current_schema","title":"infer_current_schema","text":"<pre><code>infer_current_schema() -&gt; dict\n</code></pre> <p>Infers schema from current database state.</p> <p>Will detect the attribute type, i.e. whether it is a plain, timeseries or observations attribute.</p> <p>All other properties will be set based on configuration.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with inferred schema.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def infer_current_schema(self) -&gt; dict:\n    \"\"\"\n    Infers schema from current database state.\n\n    Will detect the attribute type, i.e. whether it is\n    a plain, timeseries or observations attribute.\n\n    All other properties will be set based on configuration.\n\n    Returns:\n        Dictionary with inferred schema.\n    \"\"\"\n\n    cols = self._db.list_collections(filter={\"name\": {\"$regex\": r\"^[^#]+#master$\"}})\n    col_names = [col[\"name\"] for col in cols]\n\n    entities = defaultdict(dict)\n    for name in col_names:\n        entity = name.split(\"#\", maxsplit=1)[0]\n        self.log.debug(\"Inferring schema for '%s':\", entity)\n        self.log.debug(\"%s: Testing attributes for history\", entity)\n        res = self._db[name].aggregate(\n            [\n                # Get each attribute as a separate document\n                {\"$project\": {\"items\": {\"$objectToArray\": \"$$ROOT\"}}},\n                {\"$unwind\": \"$items\"},\n                # Check if attribute has history\n                {\"$set\": {\"array\": {\"$isArray\": \"$items.v\"}}},\n                # Group by attribute name\n                {\"$group\": {\"_id\": \"$items.k\", \"array\": {\"$addToSet\": \"$array\"}}},\n                # Filter out non-attribute properties\n                {\"$match\": {\"_id\": {\"$regex\": ID_REGEX}}},\n                {\"$match\": {\"_id\": {\"$regex\": r\"^(?!_id$)\"}}},\n            ]\n        )\n        attributes = entities[entity]\n        undecided = []\n        for doc in res:\n            if doc[\"array\"] == [False]:\n                attributes[doc[\"_id\"]] = {\"t\": AttrType.PLAIN.value}\n            elif doc[\"array\"] == [True]:\n                attributes[doc[\"_id\"]] = {\n                    \"t\": AttrType.TIMESERIES.value | AttrType.OBSERVATIONS.value\n                }\n                undecided.append(doc[\"_id\"])\n            else:\n                self.log.error(\"Unknown attribute type: %s.%s\", entity, doc[\"_id\"])\n\n        if not undecided:\n            continue\n\n        self.log.debug(\"%s: Testing observations\", entity)\n        res = self._db[name].aggregate(\n            [\n                # Get each attribute as a separate document\n                {\"$project\": {\"items\": {\"$objectToArray\": \"$$ROOT\"}}},\n                {\"$unwind\": \"$items\"},\n                # Check if attribute has confidence (i.e. is observation)\n                {\"$match\": {\"items.k\": {\"$in\": undecided}, \"items.v.c\": {\"$exists\": True}}},\n                # Group by attribute name\n                {\"$group\": {\"_id\": \"$items.k\"}},\n            ]\n        )\n        for doc in res:\n            attributes[doc[\"_id\"]][\"t\"] = AttrType.OBSERVATIONS.value\n            undecided.remove(doc[\"_id\"])\n\n        if not undecided:\n            continue\n\n        self.log.debug(\"%s: Testing timeseries\", entity)\n        res = self._db[name].aggregate(\n            [\n                # Get each attribute as a separate document\n                {\"$project\": {\"items\": {\"$objectToArray\": \"$$ROOT\"}}},\n                {\"$unwind\": \"$items\"},\n                # Check if attribute has no confidence (i.e. is timeseries)\n                {\n                    \"$match\": {\n                        \"items.k\": {\"$in\": undecided},\n                        \"items.v.c\": {\"$exists\": False},\n                        \"items.v.0\": {\"$exists\": True},\n                    }\n                },\n                # Group by attribute name\n                {\"$group\": {\"_id\": \"$items.k\"}},\n            ]\n        )\n        for doc in res:\n            attributes[doc[\"_id\"]][\"t\"] = AttrType.TIMESERIES.value\n            undecided.remove(doc[\"_id\"])\n\n        if not undecided:\n            continue\n\n        self.log.debug(\"Resolving undecided attributes: %s\", undecided)\n        for attr, spec in attributes.items():\n            if attr in undecided and (entity, attr) in self._model_spec.attributes:\n                config_spec = self._model_spec.attr(entity, attr)\n                self.log.debug(\"Assuming configuration type correct for %s.%s\", entity, attr)\n                spec[\"t\"] = config_spec.t.value\n\n    for entity, attributes in entities.items():\n        for attr, spec in attributes.items():\n            if (entity, attr) in self._model_spec.attributes:\n                config_spec = self._model_spec.attr(entity, attr)\n                config_dict = self._construct_spec_dict(config_spec)\n                config_dict.update(spec)\n                spec.update(config_dict)\n            self.log.info(\"Inferred schema for %s.%s: %s\", entity, attr, spec)\n\n    return dict(entities)\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.construct_schema_doc","title":"construct_schema_doc","text":"<pre><code>construct_schema_doc()\n</code></pre> <p>Constructs schema document from current schema configuration.</p> <p>The schema document format is as follows:</p> <pre><code>    {\n        \"entity_type\": {\n            \"attribute\": {\n                \"t\": 1 | 2 | 4,\n                \"data_type\": &lt;data_type.type_info&gt; | None\n                \"timeseries_type\": &lt;timeseries_type&gt; | None\n                \"series\": dict[str, &lt;data_type.type_info&gt;] | None\n            }\n        }\n    }\n</code></pre> <p>where <code>t</code> is attribute type, and <code>data_type</code> is the data type string. <code>timeseries_type</code> is one of <code>regular</code>, <code>irregular</code> or <code>irregular_intervals</code>. <code>series</code> is a dictionary of series names and their data types.</p> <p><code>timeseries_type</code> and <code>series</code> are present only for timeseries attributes, <code>data_type</code> is present only for plain and observations attributes.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def construct_schema_doc(self):\n    \"\"\"\n    Constructs schema document from current schema configuration.\n\n    The schema document format is as follows:\n\n    ```py\n        {\n            \"entity_type\": {\n                \"attribute\": {\n                    \"t\": 1 | 2 | 4,\n                    \"data_type\": &lt;data_type.type_info&gt; | None\n                    \"timeseries_type\": &lt;timeseries_type&gt; | None\n                    \"series\": dict[str, &lt;data_type.type_info&gt;] | None\n                }\n            }\n        }\n    ```\n\n    where `t` is [attribute type][dp3.common.attrspec.AttrType],\n    and `data_type` is the data type string.\n    `timeseries_type` is one of `regular`, `irregular` or `irregular_intervals`.\n    `series` is a dictionary of series names and their data types.\n\n    `timeseries_type` and `series` are present only for timeseries attributes,\n    `data_type` is present only for plain and observations attributes.\n    \"\"\"\n    return {\n        entity_type: {\n            attr: self._construct_spec_dict(spec) for attr, spec in attributes.items()\n        }\n        for entity_type, attributes in self._model_spec.entity_attributes.items()\n    }\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.construct_entity_types","title":"construct_entity_types","text":"<pre><code>construct_entity_types() -&gt; dict\n</code></pre> <p>Constructs entity ID type dictionary from model specification.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with entity ID types.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def construct_entity_types(self) -&gt; dict:\n    \"\"\"\n    Constructs entity ID type dictionary from model specification.\n\n    Returns:\n        Dictionary with entity ID types.\n    \"\"\"\n    return {\n        entity: entity_spec.id_data_type.type_info\n        for entity, entity_spec in self._model_spec.entities.items()\n    }\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.await_updated","title":"await_updated","text":"<pre><code>await_updated()\n</code></pre> <p>Checks whether schema saved in database is up-to-date and awaits its update by the main worker on mismatch.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def await_updated(self):\n    \"\"\"\n    Checks whether schema saved in database is up-to-date and awaits its update\n    by the main worker on mismatch.\n    \"\"\"\n    self.log.info(\"Fetching current schema\")\n    schema_doc = self.get_current_schema_doc()\n    for delay in RECONNECT_DELAYS:\n        if schema_doc is not None:\n            break\n        self.log.info(\"Schema not found, will retry in %s seconds\", delay)\n        time.sleep(delay)\n        schema_doc = self.get_current_schema_doc()\n\n    if schema_doc is None:\n        raise ValueError(\"Unable to establish schema\")\n\n    db_schema, config_schema, *_ = self.get_schema_status()\n    for delay in RECONNECT_DELAYS:\n        if db_schema == config_schema:\n            break\n        self.log.info(\"Schema mismatch, will await update by main worker in %s seconds\", delay)\n        time.sleep(delay)\n        db_schema, config_schema, *_ = self.get_schema_status()\n\n    if db_schema != config_schema:\n        raise ValueError(\"Unable to establish matching schema\")\n\n    self.log.info(\"Schema OK!\")\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.get_index_name_by_keys","title":"get_index_name_by_keys","text":"<pre><code>get_index_name_by_keys(collection: str, keys: dict) -&gt; str\n</code></pre> <p>Gets index name by keys.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <code>str</code> <p>Collection name.</p> required <code>keys</code> <code>dict</code> <p>Index keys.</p> required <p>Returns:     Index name.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def get_index_name_by_keys(self, collection: str, keys: dict) -&gt; str:\n    \"\"\"\n    Gets index name by keys.\n\n    Args:\n        collection: Collection name.\n        keys: Index keys.\n    Returns:\n        Index name.\n    \"\"\"\n    for index in self._db[collection].list_indexes():\n        if index[\"key\"] == keys:\n            return index[\"name\"]\n    raise ValueError(f\"Index not found for {collection} with keys {keys}\")\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.update_storage","title":"update_storage","text":"<pre><code>update_storage(prev_storage: dict, curr_storage: dict)\n</code></pre> <p>Updates storage settings in schema.</p> <p>Parameters:</p> Name Type Description Default <code>prev_storage</code> <code>dict</code> <p>Previous storage settings.</p> required <code>curr_storage</code> <code>dict</code> <p>Current storage settings.</p> required Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def update_storage(self, prev_storage: dict, curr_storage: dict):\n    \"\"\"\n    Updates storage settings in schema.\n\n    Args:\n        prev_storage: Previous storage settings.\n        curr_storage: Current storage settings.\n    \"\"\"\n    self.log.info(\"Updating storage settings\")\n\n    if prev_storage[\"snapshot_bucket_size\"] != curr_storage[\"snapshot_bucket_size\"]:\n        self.log.info(\n            \"Snapshot bucket size changed: %s -&gt; %s\",\n            prev_storage[\"snapshot_bucket_size\"],\n            curr_storage[\"snapshot_bucket_size\"],\n        )\n\n        for entity in self._model_spec.entities:\n            snapshot_col = f\"{entity}#snapshots\"\n\n            # Bucket size decreased, fix indexes before updating\n            if prev_storage[\"snapshot_bucket_size\"] &gt; curr_storage[\"snapshot_bucket_size\"]:\n                self.fix_latest_bucket_index(snapshot_col, curr_storage[\"snapshot_bucket_size\"])\n\n            self.log.info(\"Updating %s\", entity)\n            self._db[snapshot_col].update_many(\n                {\"oversized\": False},\n                {\"$set\": {\"count\": curr_storage[\"snapshot_bucket_size\"]}},\n            )\n\n            # Bucket size increased, fix indexes after updating\n            if prev_storage[\"snapshot_bucket_size\"] &lt;= curr_storage[\"snapshot_bucket_size\"]:\n                self.fix_latest_bucket_index(snapshot_col, curr_storage[\"snapshot_bucket_size\"])\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.migrate","title":"migrate","text":"<pre><code>migrate(schema_doc: dict) -&gt; dict\n</code></pre> <p>Migrates schema to the latest version.</p> <p>Parameters:</p> Name Type Description Default <code>schema_doc</code> <code>dict</code> <p>Schema to migrate.</p> required <p>Returns:     Migrated schema.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def migrate(self, schema_doc: dict) -&gt; dict:\n    \"\"\"\n    Migrates schema to the latest version.\n\n    Args:\n        schema_doc: Schema to migrate.\n    Returns:\n        Migrated schema.\n    \"\"\"\n    if schema_doc[\"version\"] == SCHEMA_VERSION:\n        return schema_doc\n\n    for version in range(schema_doc[\"version\"], SCHEMA_VERSION):\n        self.log.info(\"Migrating schema from version %s to %s\", version, version + 1)\n        schema_doc = self.migrations[version](schema_doc)\n        self.schemas.insert_one(schema_doc)\n\n    return schema_doc\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.migrate_schema_2_to_3","title":"migrate_schema_2_to_3","text":"<pre><code>migrate_schema_2_to_3(schema: dict) -&gt; dict\n</code></pre> <p>Migrates schema from version 2 to version 3.</p> <p>This migration adds the storage setting for snapshot bucket size to the schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>Schema to migrate.</p> required <p>Returns:     Migrated schema.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>def migrate_schema_2_to_3(self, schema: dict) -&gt; dict:\n    \"\"\"\n    Migrates schema from version 2 to version 3.\n\n    This migration adds the storage setting for snapshot bucket size to the schema.\n\n    Args:\n        schema: Schema to migrate.\n    Returns:\n        Migrated schema.\n    \"\"\"\n    # Set created time for all snapshots, append last to history\n    bucket_size = self._config.get(\"database.storage.snapshot_bucket_size\", 32)\n\n    for entity in self._model_spec.entities:\n        self.log.info(\"Migrating %s\", entity)\n        snapshot_col = self._db[f\"{entity}#snapshots\"]\n        for doc in self._db[f\"{entity}#snapshots\"].find(\n            {\"_id\": {\"$not\": {\"$regex\": r\"_#\\d+$\"}}}\n        ):\n            if doc.get(\"oversized\", False):\n                ctime = doc[\"last\"].get(\"_time_created\", datetime.now())\n                snapshot_col.bulk_write(\n                    [\n                        InsertOne(\n                            {\n                                **doc,\n                                \"_id\": f\"{doc['_id']}_#{int(ctime.timestamp())}\",\n                                \"_time_created\": ctime,\n                                \"count\": 0,\n                            }\n                        ),\n                        DeleteOne({\"_id\": doc[\"_id\"]}),\n                    ]\n                )\n                continue\n\n            to_insert = []\n            for snapshot_bucket in batched([doc[\"last\"]] + doc[\"history\"], bucket_size):\n                ctime = snapshot_bucket[-1].get(\n                    \"_time_created\",\n                )\n                to_insert.append(\n                    {\n                        \"_id\": f\"{doc['_id']}_#{int(ctime.timestamp())}\",\n                        \"last\": snapshot_bucket[0],\n                        \"history\": snapshot_bucket,\n                        \"_time_created\": ctime,\n                        \"count\": len(snapshot_bucket),\n                        \"oversized\": False,\n                    }\n                )\n            snapshot_col.insert_many(to_insert)\n            snapshot_col.delete_one({\"_id\": doc[\"_id\"]})\n\n    # Update schema\n    schema[\"_id\"] += 1\n    schema[\"version\"] = 3\n    schema[\"storage\"] = {\"snapshot_bucket_size\": bucket_size}\n\n    return schema\n</code></pre>"},{"location":"reference/database/schema_cleaner/#dp3.database.schema_cleaner.SchemaCleaner.migrate_schema_3_to_4","title":"migrate_schema_3_to_4  <code>staticmethod</code>","text":"<pre><code>migrate_schema_3_to_4(schema: dict) -&gt; dict\n</code></pre> <p>Migrates schema from version 3 to version 4.</p> <p>This migration adds eid_data_type specification to the schema. As all entities were previously using string as their entity ID type, this is merely an accounting change. Actual data types are to be changed by following migrations.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict</code> <p>Schema to migrate.</p> required <p>Returns:     Migrated schema.</p> Source code in <code>dp3/database/schema_cleaner.py</code> <pre><code>@staticmethod\ndef migrate_schema_3_to_4(schema: dict) -&gt; dict:\n    \"\"\"\n    Migrates schema from version 3 to version 4.\n\n    This migration adds eid_data_type specification to the schema.\n    As all entities were previously using string as their entity ID type,\n    this is merely an accounting change.\n    Actual data types are to be changed by following migrations.\n\n    Args:\n        schema: Schema to migrate.\n    Returns:\n        Migrated schema.\n    \"\"\"\n    schema[\"_id\"] += 1\n    schema[\"version\"] = 4\n    schema[\"entity_id_types\"] = {entity: str(str) for entity in schema[\"schema\"]}\n\n    return schema\n</code></pre>"},{"location":"reference/database/snapshots/","title":"snapshots","text":""},{"location":"reference/database/snapshots/#dp3.database.snapshots","title":"dp3.database.snapshots","text":""},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection","title":"TypedSnapshotCollection","text":"<pre><code>TypedSnapshotCollection(db: Database, entity_type: str, db_config: MongoConfig, model_spec: ModelSpec, snapshots_config: dict)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Snapshot collection handler with eid type awareness.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def __init__(\n    self,\n    db: Database,\n    entity_type: str,\n    db_config: MongoConfig,\n    model_spec: ModelSpec,\n    snapshots_config: dict,\n):\n    self._db = db.with_options(codec_options=get_codec_options())\n\n    if entity_type not in model_spec.entities:\n        raise ValueError(f\"Entity type '{entity_type}' not found in model spec\")\n    self.entity_type = entity_type\n    self._col_name = f\"{entity_type}#snapshots\"\n    self._os_col_name = f\"{entity_type}#oversized_snapshots\"\n\n    self.attr_specs: dict[str, AttrSpecType] = model_spec.entity_attributes[entity_type]\n\n    self.log = logging.getLogger(f\"EntityDatabase.SnapshotCollection[{entity_type}]\")\n\n    self._normal_snapshot_eids = set()\n    self._oversized_snapshot_eids = set()\n    self._snapshot_bucket_size = db_config.storage.snapshot_bucket_size\n    self._bucket_delta = self._get_snapshot_bucket_delta(snapshots_config)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.get_latest_one","title":"get_latest_one","text":"<pre><code>get_latest_one(eid: AnyEidT) -&gt; dict\n</code></pre> <p>Get latest snapshot of given eid.</p> <p>If doesn't exist, returns {}.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def get_latest_one(self, eid: AnyEidT) -&gt; dict:\n    \"\"\"Get latest snapshot of given eid.\n\n    If doesn't exist, returns {}.\n    \"\"\"\n    return (\n        self._col().find_one(self._filter_from_eid(eid), {\"last\": 1}, sort=[(\"_id\", -1)]) or {}\n    ).get(\"last\", {})\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.get_latest","title":"get_latest","text":"<pre><code>get_latest(fulltext_filters: Optional[dict[str, str]] = None, generic_filter: Optional[dict[str, Any]] = None) -&gt; tuple[Cursor, int]\n</code></pre> <p>Get latest snapshots of given <code>etype</code>.</p> <p>This method is useful for displaying data on web.</p> <p>Returns only documents matching <code>generic_filter</code> and <code>fulltext_filters</code> (dictionary attribute - fulltext filter). Fulltext filters are interpreted as regular expressions. Only string values may be filtered this way. There's no validation that queried attribute can be fulltext filtered. Only plain and observation attributes with string-based data types can be queried. Array and set data types are supported as well as long as they are not multi value at the same time. If you need to filter EIDs, ensure the EID is string, then use attribute <code>eid</code>. Otherwise, use generic filter.</p> <p>Generic filter allows filtering using generic MongoDB query (including <code>$and</code>, <code>$or</code>, <code>$lt</code>, etc.). For querying non-JSON-native types, you can use magic strings, such as <code>\"$$IPv4{&lt;ip address&gt;}\"</code> for IPv4 addresses. The full spec with examples is in the magic strings module.</p> <p>Generic and fulltext filters are merged - fulltext overrides conflicting keys.</p> <p>Also returns total document count (after filtering).</p> <p>May raise <code>SnapshotCollectionError</code> if query is invalid.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def get_latest(\n    self,\n    fulltext_filters: Optional[dict[str, str]] = None,\n    generic_filter: Optional[dict[str, Any]] = None,\n) -&gt; tuple[Cursor, int]:\n    \"\"\"Get latest snapshots of given `etype`.\n\n    This method is useful for displaying data on web.\n\n    Returns only documents matching `generic_filter` and `fulltext_filters`\n    (dictionary attribute - fulltext filter).\n    Fulltext filters are interpreted as regular expressions.\n    Only string values may be filtered this way. There's no validation that queried attribute\n    can be fulltext filtered.\n    Only plain and observation attributes with string-based data types can be queried.\n    Array and set data types are supported as well as long as they are not multi value\n    at the same time.\n    If you need to filter EIDs, ensure the EID is string, then use attribute `eid`.\n    Otherwise, use generic filter.\n\n    Generic filter allows filtering using generic MongoDB query (including `$and`, `$or`,\n    `$lt`, etc.).\n    For querying non-JSON-native types, you can use magic strings, such as\n    `\"$$IPv4{&lt;ip address&gt;}\"` for IPv4 addresses. The full spec with examples is in the\n    [magic strings module][dp3.database.magic].\n\n    Generic and fulltext filters are merged - fulltext overrides conflicting keys.\n\n    Also returns total document count (after filtering).\n\n    May raise `SnapshotCollectionError` if query is invalid.\n    \"\"\"\n    snapshot_col = self._col()\n    query = self._prepare_latest_query(fulltext_filters or {}, generic_filter or {})\n\n    try:\n        return snapshot_col.find(query, {\"last\": 1}).sort(\n            [(\"_id\", pymongo.ASCENDING)]\n        ), snapshot_col.count_documents(query)\n    except OperationFailure as e:\n        raise SnapshotCollectionError(f\"Query is invalid: {e}\") from e\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.find_latest","title":"find_latest","text":"<pre><code>find_latest(fulltext_filters: Optional[dict[str, str]] = None, generic_filter: Optional[dict[str, Any]] = None) -&gt; Cursor\n</code></pre> <p>Find latest snapshots of given <code>etype</code>.</p> <p>See <code>get_latest</code> for more information.</p> <p>Returns only documents matching <code>generic_filter</code> and <code>fulltext_filters</code>, does not count them.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def find_latest(\n    self,\n    fulltext_filters: Optional[dict[str, str]] = None,\n    generic_filter: Optional[dict[str, Any]] = None,\n) -&gt; Cursor:\n    \"\"\"Find latest snapshots of given `etype`.\n\n    See [`get_latest`][dp3.database.snapshots.SnapshotCollectionContainer.get_latest]\n    for more information.\n\n    Returns only documents matching `generic_filter` and `fulltext_filters`,\n    does not count them.\n    \"\"\"\n    query = self._prepare_latest_query(fulltext_filters or {}, generic_filter or {})\n    try:\n        return self._col().find(query, {\"last\": 1}).sort([(\"_id\", pymongo.ASCENDING)])\n    except OperationFailure as e:\n        raise SnapshotCollectionError(f\"Query is invalid: {e}\") from e\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.count_latest","title":"count_latest","text":"<pre><code>count_latest(fulltext_filters: Optional[dict[str, str]] = None, generic_filter: Optional[dict[str, Any]] = None) -&gt; int\n</code></pre> <p>Count latest snapshots of given <code>etype</code>.</p> <p>See <code>get_latest</code> for more information.</p> <p>Returns only count of documents matching <code>generic_filter</code> and <code>fulltext_filters</code>.</p> <p>Note that this method may take much longer than <code>get_latest</code> on larger databases, as it does count all documents, not just return the first few.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def count_latest(\n    self,\n    fulltext_filters: Optional[dict[str, str]] = None,\n    generic_filter: Optional[dict[str, Any]] = None,\n) -&gt; int:\n    \"\"\"Count latest snapshots of given `etype`.\n\n    See [`get_latest`][dp3.database.snapshots.SnapshotCollectionContainer.get_latest]\n    for more information.\n\n    Returns only count of documents matching `generic_filter` and `fulltext_filters`.\n\n    Note that this method may take much longer than `get_latest` on larger databases,\n    as it does count all documents, not just return the first few.\n    \"\"\"\n    query = self._prepare_latest_query(fulltext_filters or {}, generic_filter or {})\n    try:\n        return self._col().count_documents(query)\n    except OperationFailure as e:\n        raise SnapshotCollectionError(f\"Query is invalid: {e}\") from e\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.get_by_eid","title":"get_by_eid","text":"<pre><code>get_by_eid(eid: AnyEidT, t1: Optional[datetime] = None, t2: Optional[datetime] = None) -&gt; Union[Cursor, CommandCursor]\n</code></pre> <p>Get all (or filtered) snapshots of given <code>eid</code>.</p> <p>This method is useful for displaying <code>eid</code>'s history on web.</p> <p>Parameters:</p> Name Type Description Default <code>eid</code> <code>AnyEidT</code> <p>id of entity, to which data-points correspond</p> required <code>t1</code> <code>Optional[datetime]</code> <p>left value of time interval (inclusive)</p> <code>None</code> <code>t2</code> <code>Optional[datetime]</code> <p>right value of time interval (inclusive)</p> <code>None</code> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def get_by_eid(\n    self, eid: AnyEidT, t1: Optional[datetime] = None, t2: Optional[datetime] = None\n) -&gt; Union[Cursor, CommandCursor]:\n    \"\"\"Get all (or filtered) snapshots of given `eid`.\n\n    This method is useful for displaying `eid`'s history on web.\n\n    Args:\n        eid: id of entity, to which data-points correspond\n        t1: left value of time interval (inclusive)\n        t2: right value of time interval (inclusive)\n    \"\"\"\n    snapshot_col = self._col()\n\n    # Find out if the snapshot is oversized\n    doc = (\n        snapshot_col.find(self._filter_from_eid(eid), {\"oversized\": 1})\n        .sort([(\"_id\", -1)])\n        .limit(1)\n    )\n    doc = next(doc, None)\n    if doc and doc.get(\"oversized\", False):\n        return self._get_oversized(eid, t1, t2)\n\n    query = {\"_time_created\": {}}\n    pipeline = [\n        {\"$match\": self._filter_from_eid(eid)},\n        {\"$unwind\": \"$history\"},\n        {\"$replaceRoot\": {\"newRoot\": \"$history\"}},\n    ]\n\n    # Filter by date\n    if t1:\n        query[\"_time_created\"][\"$gte\"] = t1\n    if t2:\n        query[\"_time_created\"][\"$lte\"] = t2\n\n    # Unset if empty\n    if query[\"_time_created\"]:\n        pipeline.append({\"$match\": query})\n    pipeline.append({\"$sort\": {\"_time_created\": pymongo.ASCENDING}})\n    return snapshot_col.aggregate(pipeline)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.get_distinct_val_count","title":"get_distinct_val_count","text":"<pre><code>get_distinct_val_count(attr: str) -&gt; dict[Any, int]\n</code></pre> <p>Counts occurrences of distinct values of given attribute in snapshots.</p> <p>Returns dictionary mapping value -&gt; count.</p> <p>Works for all plain and observation data types except <code>dict</code> and <code>json</code>.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def get_distinct_val_count(self, attr: str) -&gt; dict[Any, int]:\n    \"\"\"Counts occurrences of distinct values of given attribute in snapshots.\n\n    Returns dictionary mapping value -&gt; count.\n\n    Works for all plain and observation data types except `dict` and `json`.\n    \"\"\"\n    # Get attribute specification\n    try:\n        attr_spec = self.attr_specs[attr]\n    except KeyError as e:\n        raise SnapshotCollectionError(f\"Attribute '{attr}' does not exist\") from e\n\n    if attr_spec.t not in AttrType.PLAIN | AttrType.OBSERVATIONS:\n        raise SnapshotCollectionError(f\"Attribute '{attr}' isn't plain or observations\")\n\n    # Attribute data type must be primitive, array&lt;T&gt; or set&lt;T&gt;\n    if any(needle in attr_spec.data_type.root for needle in (\"dict\", \"json\")):\n        raise SnapshotCollectionError(\n            f\"Data type '{attr_spec.data_type}' of attribute '{attr}' is not processable\"\n        )\n\n    # Build aggregation query\n    attr_path = \"$last.\" + attr\n    unwinding = []\n\n    # Unwind array-like and multi value attributes\n    # If attribute is multi value array, unwind twice\n    if \"array\" in attr_spec.data_type.root or \"set\" in attr_spec.data_type.root:\n        unwinding.append({\"$unwind\": attr_path})\n    if attr_spec.t == AttrType.OBSERVATIONS and attr_spec.multi_value:\n        unwinding.append({\"$unwind\": attr_path})\n\n    # Group\n    agg_query_group_id = attr_path\n    if \"link\" in attr_spec.data_type.root:\n        agg_query_group_id += \".eid\"\n\n    agg_query = [\n        {\"$match\": {\"latest\": True}},\n        *unwinding,\n        {\"$group\": {\"_id\": agg_query_group_id, \"count\": {\"$sum\": 1}}},\n        {\"$sort\": {\"_id\": 1, \"count\": -1}},\n    ]\n    # Run aggregation\n    distinct_counts_cur = self._col().aggregate(agg_query)\n\n    distinct_counts = {x[\"_id\"]: x[\"count\"] for x in distinct_counts_cur}\n\n    if None in distinct_counts:\n        del distinct_counts[None]\n\n    return distinct_counts\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.save_one","title":"save_one","text":"<pre><code>save_one(snapshot: dict, ctime: datetime)\n</code></pre> <p>Saves snapshot to specified entity of current master document.</p> <p>Will move snapshot to oversized snapshots if the maintained bucket is too large.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def save_one(self, snapshot: dict, ctime: datetime):\n    \"\"\"Saves snapshot to specified entity of current master document.\n\n    Will move snapshot to oversized snapshots if the maintained bucket is too large.\n    \"\"\"\n    if \"eid\" not in snapshot:\n        self.log.error(\"Snapshot is missing 'eid' field: %s\", snapshot)\n        return\n    eid = snapshot[\"eid\"]\n    snapshot[\"_time_created\"] = ctime\n\n    snapshot_col = self._col()\n    os_col = self._os_col()\n\n    # Find out if the snapshot is oversized\n    normal, oversized = self._get_state({eid})\n    if normal:\n        try:\n            res = snapshot_col.update_one(\n                self._filter_from_eid(eid) | {\"count\": {\"$lt\": self._snapshot_bucket_size}},\n                {\n                    \"$set\": {\"last\": snapshot},\n                    \"$push\": {\"history\": {\"$each\": [snapshot], \"$position\": 0}},\n                    \"$inc\": {\"count\": 1},\n                    \"$setOnInsert\": {\n                        \"_id\": self._bucket_id(eid, ctime),\n                        \"_time_created\": ctime,\n                        \"oversized\": False,\n                        \"latest\": True,\n                    },\n                },\n                upsert=True,\n            )\n\n            if res.upserted_id is not None:\n                snapshot_col.update_many(\n                    self._filter_from_eid(eid)\n                    | {\"latest\": True, \"count\": self._snapshot_bucket_size},\n                    {\"$unset\": {\"latest\": 1}},\n                )\n        except (WriteError, OperationFailure, DocumentTooLarge) as e:\n            if e.code != BSON_OBJECT_TOO_LARGE:\n                raise e\n            # The snapshot is too large, move it to oversized snapshots\n            self.log.info(f\"Snapshot of {eid} is too large: {e}, marking as oversized.\")\n            self._migrate_to_oversized(eid, snapshot)\n            self._cache_snapshot_state(set(), normal)\n        except Exception as e:\n            raise SnapshotCollectionError(\n                f\"Insert of snapshot {eid} failed: {e}, {snapshot}\"\n            ) from e\n        return\n    elif oversized:\n        # Snapshot is already marked as oversized\n        snapshot_col.update_one(self._filter_from_eid(eid), {\"$set\": {\"last\": snapshot}})\n        os_col.insert_one(snapshot)\n        return\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.save_many","title":"save_many","text":"<pre><code>save_many(snapshots: list[dict], ctime: datetime)\n</code></pre> <p>Saves a list of snapshots of current master documents.</p> <p>All snapshots must belong to same entity type.</p> <p>Will move snapshots to oversized snapshots if the maintained bucket is too large. For better understanding, see <code>save()</code>.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def save_many(self, snapshots: list[dict], ctime: datetime):\n    \"\"\"\n    Saves a list of snapshots of current master documents.\n\n    All snapshots must belong to same entity type.\n\n    Will move snapshots to oversized snapshots if the maintained bucket is too large.\n    For better understanding, see `save()`.\n    \"\"\"\n\n    for snapshot in snapshots:\n        snapshot[\"_time_created\"] = ctime\n\n    snapshot_col = self._col()\n    os_col = self._os_col()\n\n    snapshots_by_eid = defaultdict(list)\n    for snapshot in snapshots:\n        if \"eid\" not in snapshot:\n            continue\n        if \"_id\" in snapshot:\n            del snapshot[\"_id\"]\n        snapshots_by_eid[snapshot[\"eid\"]].append(snapshot)\n\n    # Find out if any of the snapshots are oversized\n    normal, oversized = self._get_state(set(snapshots_by_eid.keys()))\n\n    upserts = []\n    update_originals: list[list[dict]] = []\n    oversized_inserts = []\n    oversized_updates = []\n\n    # A normal snapshot, shift the last snapshot to history and update last\n    for eid in normal:\n        upserts.append(\n            UpdateOne(\n                self._filter_from_eid(eid) | {\"count\": {\"$lt\": self._snapshot_bucket_size}},\n                {\n                    \"$set\": {\"last\": snapshots_by_eid[eid][-1]},\n                    \"$push\": {\"history\": {\"$each\": snapshots_by_eid[eid], \"$position\": 0}},\n                    \"$inc\": {\"count\": len(snapshots_by_eid[eid])},\n                    \"$setOnInsert\": {\n                        \"_id\": self._bucket_id(eid, ctime),\n                        \"_time_created\": ctime,\n                        \"oversized\": False,\n                        \"latest\": True,\n                    },\n                },\n                upsert=True,\n            )\n        )\n        update_originals.append(snapshots_by_eid[eid])\n\n    # Snapshot is already marked as oversized\n    for eid in oversized:\n        oversized_inserts.extend(snapshots_by_eid[eid])\n        oversized_updates.append(\n            UpdateOne(\n                self._filter_from_eid(eid),\n                {\"$set\": {\"last\": snapshots_by_eid[eid][-1]}},\n            )\n        )\n\n    new_oversized = set()\n\n    if upserts:\n        try:\n            res = snapshot_col.bulk_write(upserts, ordered=False)\n\n            # Unset latest snapshots if new snapshots were inserted\n            if res.upserted_count &gt; 0:\n                unset_latest_updates = []\n                for upsert_id in res.upserted_ids.values():\n                    unset_latest_updates.append(\n                        UpdateMany(\n                            self._filter_from_bid(upsert_id)\n                            | {\"latest\": True, \"count\": self._snapshot_bucket_size},\n                            {\"$unset\": {\"latest\": 1}},\n                        )\n                    )\n                up_res = snapshot_col.bulk_write(unset_latest_updates)\n                if up_res.modified_count != res.upserted_count:\n                    self.log.info(\n                        \"Upserted the first snapshot for %d entities.\",\n                        res.upserted_count - up_res.modified_count,\n                    )\n\n            if res.modified_count + res.upserted_count != len(upserts):\n                self.log.error(\n                    \"Some snapshots were not updated, %s != %s\",\n                    res.modified_count + res.upserted_count,\n                    len(upserts),\n                )\n        except (BulkWriteError, OperationFailure) as e:\n            self.log.info(\"Update of snapshots failed, will retry with oversize.\")\n            failed_indexes = [\n                err[\"index\"]\n                for err in e.details[\"writeErrors\"]\n                if err[\"code\"] == BSON_OBJECT_TOO_LARGE\n            ]\n            failed_snapshots = (update_originals[i] for i in failed_indexes)\n            for eid_snapshots in failed_snapshots:\n                eid = eid_snapshots[0][\"eid\"]\n                failed_snapshots = sorted(\n                    eid_snapshots, key=lambda s: s[\"_time_created\"], reverse=True\n                )\n                self._migrate_to_oversized(eid, failed_snapshots[0])\n                oversized_inserts.extend(failed_snapshots[1:])\n                new_oversized.add(eid)\n\n            if any(err[\"code\"] != BSON_OBJECT_TOO_LARGE for err in e.details[\"writeErrors\"]):\n                # Some other error occurred\n                raise e\n        except Exception as e:\n            raise SnapshotCollectionError(f\"Upsert of snapshots failed: {str(e)[:2048]}\") from e\n\n    # Update the oversized snapshots\n    if oversized_inserts:\n        try:\n            if oversized_updates:\n                snapshot_col.bulk_write(oversized_updates)\n            os_col.insert_many(oversized_inserts)\n        except Exception as e:\n            raise SnapshotCollectionError(f\"Insert of snapshots failed: {str(e)[:2048]}\") from e\n\n    # Cache the new state\n    self._cache_snapshot_state(set(), new_oversized)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.delete_old","title":"delete_old","text":"<pre><code>delete_old(t_old: datetime) -&gt; int\n</code></pre> <p>Delete old snapshots.</p> <p>Periodically called from HistoryManager.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def delete_old(self, t_old: datetime) -&gt; int:\n    \"\"\"Delete old snapshots.\n\n    Periodically called from HistoryManager.\n    \"\"\"\n    deleted = 0\n    try:\n        res = self._col().delete_many(\n            {\"_time_created\": {\"$lte\": t_old - self._bucket_delta}},\n        )\n        deleted += res.deleted_count * self._snapshot_bucket_size\n        res = self._os_col().delete_many({\"_time_created\": {\"$lt\": t_old}})\n        deleted += res.deleted_count\n    except Exception as e:\n        raise SnapshotCollectionError(f\"Delete of olds snapshots failed: {e}\") from e\n    return deleted\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.delete_eid","title":"delete_eid","text":"<pre><code>delete_eid(eid: AnyEidT)\n</code></pre> <p>Delete all snapshots of <code>eid</code>.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def delete_eid(self, eid: AnyEidT):\n    \"\"\"Delete all snapshots of `eid`.\"\"\"\n    try:\n        res = self._col().delete_many(self._filter_from_eid(eid))\n        del_cnt = res.deleted_count * self._snapshot_bucket_size\n        self.log.debug(\"deleted %s snapshots of %s/%s.\", del_cnt, self.entity_type, eid)\n\n        res = self._os_col().delete_many({\"eid\": eid})\n        self.log.debug(\n            \"Deleted %s oversized snapshots of %s/%s.\", res.deleted_count, self.entity_type, eid\n        )\n    except Exception as e:\n        raise SnapshotCollectionError(f\"Delete of failed: {e}\\n{eid}\") from e\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.TypedSnapshotCollection.delete_eids","title":"delete_eids","text":"<pre><code>delete_eids(eids: list[Any])\n</code></pre> <p>Delete all snapshots of <code>eids</code>.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def delete_eids(self, eids: list[Any]):\n    \"\"\"Delete all snapshots of `eids`.\"\"\"\n    try:\n        res = self._col().delete_many(self._filter_from_eids(eids))\n        del_cnt = res.deleted_count * self._snapshot_bucket_size\n        self.log.debug(\"Deleted %s snapshots of %s (%s).\", del_cnt, self.entity_type, len(eids))\n        res = self._os_col().delete_many({\"eid\": {\"$in\": eids}})\n        self.log.debug(\n            \"Deleted %s oversized snapshots of %s (%s).\",\n            res.deleted_count,\n            self.entity_type,\n            len(eids),\n        )\n    except Exception as e:\n        raise SnapshotCollectionError(f\"Delete of snapshots failed: {e}\\n{eids}\") from e\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.BinaryEidSnapshots","title":"BinaryEidSnapshots","text":"<pre><code>BinaryEidSnapshots(db: Database, entity_type: str, db_config: MongoConfig, model_spec: ModelSpec, snapshots_config: dict)\n</code></pre> <p>               Bases: <code>TypedSnapshotCollection</code>, <code>ABC</code></p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def __init__(\n    self,\n    db: Database,\n    entity_type: str,\n    db_config: MongoConfig,\n    model_spec: ModelSpec,\n    snapshots_config: dict,\n):\n    self._db = db.with_options(codec_options=get_codec_options())\n\n    if entity_type not in model_spec.entities:\n        raise ValueError(f\"Entity type '{entity_type}' not found in model spec\")\n    self.entity_type = entity_type\n    self._col_name = f\"{entity_type}#snapshots\"\n    self._os_col_name = f\"{entity_type}#oversized_snapshots\"\n\n    self.attr_specs: dict[str, AttrSpecType] = model_spec.entity_attributes[entity_type]\n\n    self.log = logging.getLogger(f\"EntityDatabase.SnapshotCollection[{entity_type}]\")\n\n    self._normal_snapshot_eids = set()\n    self._oversized_snapshot_eids = set()\n    self._snapshot_bucket_size = db_config.storage.snapshot_bucket_size\n    self._bucket_delta = self._get_snapshot_bucket_delta(snapshots_config)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer","title":"SnapshotCollectionContainer","text":"<pre><code>SnapshotCollectionContainer(db: Database, db_config: MongoConfig, model_spec: ModelSpec, snapshots_config: dict)\n</code></pre> <p>Container for all required snapshot collections, exposing the public interface.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def __init__(\n    self, db: Database, db_config: MongoConfig, model_spec: ModelSpec, snapshots_config: dict\n):\n    self._snapshot_collections = {}\n    for entity_type, entity_spec in model_spec.entities.items():\n        eid_type_name = entity_spec.id_data_type.root\n        typed_collection: type[TypedSnapshotCollection] = entity_type2collection[eid_type_name]\n        self._snapshot_collections[entity_type] = typed_collection(\n            db, entity_type, db_config, model_spec, snapshots_config\n        )\n\n    self.log = logging.getLogger(\"EntityDatabase.SnapshotCollections\")\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.save_one","title":"save_one","text":"<pre><code>save_one(entity_type: str, snapshot: dict, ctime: datetime) -&gt; None\n</code></pre> <p>Save snapshot to specified entity of current master document.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def save_one(self, entity_type: str, snapshot: dict, ctime: datetime) -&gt; None:\n    \"\"\"Save snapshot to specified entity of current master document.\"\"\"\n    return self[entity_type].save_one(snapshot, ctime)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.save_many","title":"save_many","text":"<pre><code>save_many(entity_type: str, snapshots: list[dict], ctime: datetime) -&gt; None\n</code></pre> <p>Saves a list of snapshots of current master documents.</p> <p>All snapshots must belong to same entity type.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def save_many(self, entity_type: str, snapshots: list[dict], ctime: datetime) -&gt; None:\n    \"\"\"\n    Saves a list of snapshots of current master documents.\n\n    All snapshots must belong to same entity type.\n    \"\"\"\n    return self[entity_type].save_many(snapshots, ctime)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.get_latest_one","title":"get_latest_one","text":"<pre><code>get_latest_one(entity_type: str, eid: AnyEidT) -&gt; dict\n</code></pre> <p>Get latest snapshot of given eid.</p> <p>If doesn't exist, returns {}.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def get_latest_one(self, entity_type: str, eid: AnyEidT) -&gt; dict:\n    \"\"\"Get latest snapshot of given eid.\n\n    If doesn't exist, returns {}.\n    \"\"\"\n    return self[entity_type].get_latest_one(eid)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.get_latest","title":"get_latest","text":"<pre><code>get_latest(entity_type: str, fulltext_filters: Optional[dict[str, str]] = None, generic_filter: Optional[dict[str, Any]] = None) -&gt; tuple[Cursor, int]\n</code></pre> <p>Get latest snapshots of given <code>etype</code>.</p> <p>This method is useful for displaying data on web.</p> <p>Returns only documents matching <code>generic_filter</code> and <code>fulltext_filters</code> (dictionary attribute - fulltext filter). Fulltext filters are interpreted as regular expressions. Only string values may be filtered this way. There's no validation that queried attribute can be fulltext filtered. Only plain and observation attributes with string-based data types can be queried. Array and set data types are supported as well as long as they are not multi value at the same time. If you need to filter EIDs, ensure the EID is string, then use attribute <code>eid</code>. Otherwise, use generic filter.</p> <p>Generic filter allows filtering using generic MongoDB query (including <code>$and</code>, <code>$or</code>, <code>$lt</code>, etc.). For querying non-JSON-native types, you can use magic strings, such as <code>\"$$IPv4{&lt;ip address&gt;}\"</code> for IPv4 addresses. The full spec with examples is in the magic strings module.</p> <p>Generic and fulltext filters are merged - fulltext overrides conflicting keys.</p> <p>Also returns total document count (after filtering).</p> <p>May raise <code>SnapshotCollectionError</code> if query is invalid.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def get_latest(\n    self,\n    entity_type: str,\n    fulltext_filters: Optional[dict[str, str]] = None,\n    generic_filter: Optional[dict[str, Any]] = None,\n) -&gt; tuple[Cursor, int]:\n    \"\"\"Get latest snapshots of given `etype`.\n\n    This method is useful for displaying data on web.\n\n    Returns only documents matching `generic_filter` and `fulltext_filters`\n    (dictionary attribute - fulltext filter).\n    Fulltext filters are interpreted as regular expressions.\n    Only string values may be filtered this way. There's no validation that queried attribute\n    can be fulltext filtered.\n    Only plain and observation attributes with string-based data types can be queried.\n    Array and set data types are supported as well as long as they are not multi value\n    at the same time.\n    If you need to filter EIDs, ensure the EID is string, then use attribute `eid`.\n    Otherwise, use generic filter.\n\n    Generic filter allows filtering using generic MongoDB query (including `$and`, `$or`,\n    `$lt`, etc.).\n    For querying non-JSON-native types, you can use magic strings, such as\n    `\"$$IPv4{&lt;ip address&gt;}\"` for IPv4 addresses. The full spec with examples is in the\n    [magic strings module][dp3.database.magic].\n\n    Generic and fulltext filters are merged - fulltext overrides conflicting keys.\n\n    Also returns total document count (after filtering).\n\n    May raise `SnapshotCollectionError` if query is invalid.\n    \"\"\"\n    return self[entity_type].get_latest(fulltext_filters, generic_filter)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.find_latest","title":"find_latest","text":"<pre><code>find_latest(entity_type: str, fulltext_filters: Optional[dict[str, str]] = None, generic_filter: Optional[dict[str, Any]] = None) -&gt; Cursor\n</code></pre> <p>Find latest snapshots of given <code>etype</code>.</p> <p>see <code>get_latest</code> for more information.</p> <p>Returns only documents matching <code>generic_filter</code> and <code>fulltext_filters</code>, does not count them.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def find_latest(\n    self,\n    entity_type: str,\n    fulltext_filters: Optional[dict[str, str]] = None,\n    generic_filter: Optional[dict[str, Any]] = None,\n) -&gt; Cursor:\n    \"\"\"Find latest snapshots of given `etype`.\n\n    see [`get_latest`][dp3.database.snapshots.SnapshotCollectionContainer.get_latest]\n    for more information.\n\n    Returns only documents matching `generic_filter` and `fulltext_filters`,\n    does not count them.\n    \"\"\"\n    return self[entity_type].find_latest(fulltext_filters, generic_filter)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.count_latest","title":"count_latest","text":"<pre><code>count_latest(entity_type: str, fulltext_filters: Optional[dict[str, str]] = None, generic_filter: Optional[dict[str, Any]] = None) -&gt; int\n</code></pre> <p>Count latest snapshots of given <code>etype</code>.</p> <p>see <code>get_latest</code> for more information.</p> <p>Returns only count of documents matching <code>generic_filter</code> and <code>fulltext_filters</code>.</p> <p>Note that this method may take much longer than <code>get_latest</code> on larger databases, as it does count all documents, not just return the first few.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def count_latest(\n    self,\n    entity_type: str,\n    fulltext_filters: Optional[dict[str, str]] = None,\n    generic_filter: Optional[dict[str, Any]] = None,\n) -&gt; int:\n    \"\"\"Count latest snapshots of given `etype`.\n\n    see [`get_latest`][dp3.database.snapshots.SnapshotCollectionContainer.get_latest]\n    for more information.\n\n    Returns only count of documents matching `generic_filter` and `fulltext_filters`.\n\n    Note that this method may take much longer than `get_latest` on larger databases,\n    as it does count all documents, not just return the first few.\n    \"\"\"\n    return self[entity_type].count_latest(fulltext_filters, generic_filter)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.get_by_eid","title":"get_by_eid","text":"<pre><code>get_by_eid(entity_type: str, eid: AnyEidT, t1: Optional[datetime] = None, t2: Optional[datetime] = None) -&gt; Union[Cursor, CommandCursor]\n</code></pre> <p>Get all (or filtered) snapshots of given <code>eid</code>.</p> <p>This method is useful for displaying <code>eid</code>'s history on web.</p> <p>Parameters:</p> Name Type Description Default <code>entity_type</code> <code>str</code> <p>name of entity type</p> required <code>eid</code> <code>AnyEidT</code> <p>id of entity, to which data-points correspond</p> required <code>t1</code> <code>Optional[datetime]</code> <p>left value of time interval (inclusive)</p> <code>None</code> <code>t2</code> <code>Optional[datetime]</code> <p>right value of time interval (inclusive)</p> <code>None</code> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def get_by_eid(\n    self,\n    entity_type: str,\n    eid: AnyEidT,\n    t1: Optional[datetime] = None,\n    t2: Optional[datetime] = None,\n) -&gt; Union[Cursor, CommandCursor]:\n    \"\"\"Get all (or filtered) snapshots of given `eid`.\n\n    This method is useful for displaying `eid`'s history on web.\n\n    Args:\n        entity_type: name of entity type\n        eid: id of entity, to which data-points correspond\n        t1: left value of time interval (inclusive)\n        t2: right value of time interval (inclusive)\n    \"\"\"\n    return self[entity_type].get_by_eid(eid, t1, t2)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.get_distinct_val_count","title":"get_distinct_val_count","text":"<pre><code>get_distinct_val_count(entity_type: str, attr: str) -&gt; dict[Any, int]\n</code></pre> <p>Counts occurrences of distinct values of given attribute in snapshots.</p> <p>Returns dictionary mapping value -&gt; count.</p> <p>Works for all plain and observation data types except <code>dict</code> and <code>json</code>.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def get_distinct_val_count(self, entity_type: str, attr: str) -&gt; dict[Any, int]:\n    \"\"\"Counts occurrences of distinct values of given attribute in snapshots.\n\n    Returns dictionary mapping value -&gt; count.\n\n    Works for all plain and observation data types except `dict` and `json`.\n    \"\"\"\n    return self[entity_type].get_distinct_val_count(attr)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.delete_old","title":"delete_old","text":"<pre><code>delete_old(t_old: datetime) -&gt; int\n</code></pre> <p>Delete old snapshots, may raise <code>SnapshotCollectionError</code>.</p> <p>Periodically called from HistoryManager. Returns:      number of deleted snapshots.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def delete_old(self, t_old: datetime) -&gt; int:\n    \"\"\"Delete old snapshots, may raise `SnapshotCollectionError`.\n\n    Periodically called from HistoryManager.\n    Returns:\n         number of deleted snapshots.\n    \"\"\"\n    deleted_total = 0\n    for collection in self._snapshot_collections.values():\n        try:\n            deleted_total += collection.delete_old(t_old)\n        except Exception as e:\n            raise SnapshotCollectionError(f\"Delete of old snapshots failed: {e}\") from e\n    return deleted_total\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.delete_eid","title":"delete_eid","text":"<pre><code>delete_eid(entity_type: str, eid: AnyEidT) -&gt; int\n</code></pre> <p>Delete snapshots of given <code>eids</code>.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def delete_eid(self, entity_type: str, eid: AnyEidT) -&gt; int:\n    \"\"\"Delete snapshots of given `eids`.\"\"\"\n    return self[entity_type].delete_eid(eid)\n</code></pre>"},{"location":"reference/database/snapshots/#dp3.database.snapshots.SnapshotCollectionContainer.delete_eids","title":"delete_eids","text":"<pre><code>delete_eids(entity_type: str, eids: list[Any]) -&gt; int\n</code></pre> <p>Delete snapshots of given <code>eids</code>.</p> Source code in <code>dp3/database/snapshots.py</code> <pre><code>def delete_eids(self, entity_type: str, eids: list[Any]) -&gt; int:\n    \"\"\"Delete snapshots of given `eids`.\"\"\"\n    return self[entity_type].delete_eids(eids)\n</code></pre>"},{"location":"reference/history_management/","title":"history_management","text":""},{"location":"reference/history_management/#dp3.history_management","title":"dp3.history_management","text":"<p>Module responsible for managing history saved in database, currently to clean old data.</p>"},{"location":"reference/history_management/history_manager/","title":"history_manager","text":""},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager","title":"dp3.history_management.history_manager","text":""},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.SnapshotCleaningConfig","title":"SnapshotCleaningConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for snapshot cleaning.</p> <p>Attributes:</p> Name Type Description <code>schedule</code> <code>CronExpression</code> <p>Schedule for snapshot cleaning.</p> <code>older_than</code> <code>ParsedTimedelta</code> <p>Snapshots older than this will be deleted.</p>"},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.DPArchivationConfig","title":"DPArchivationConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for datapoint archivation.</p> <p>Attributes:</p> Name Type Description <code>schedule</code> <code>CronExpression</code> <p>Schedule for datapoint archivation.</p> <code>older_than</code> <code>ParsedTimedelta</code> <p>Datapoints older than this will be archived.</p> <code>archive_dir</code> <code>Optional[str]</code> <p>Directory where to archive datapoints. Can be <code>None</code> to only delete them.</p>"},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.HistoryManagerConfig","title":"HistoryManagerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for history manager.</p> <p>Attributes:</p> Name Type Description <code>aggregation_schedule</code> <code>CronExpression</code> <p>Schedule for master document aggregation.</p> <code>datapoint_cleaning_schedule</code> <code>CronExpression</code> <p>Schedule for datapoint cleaning.</p> <code>mark_datapoints_schedule</code> <code>CronExpression</code> <p>Schedule for marking datapoints in master docs.</p> <code>snapshot_cleaning</code> <code>SnapshotCleaningConfig</code> <p>Configuration for snapshot cleaning.</p> <code>datapoint_archivation</code> <code>DPArchivationConfig</code> <p>Configuration for datapoint archivation.</p>"},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.HistoryManager","title":"HistoryManager","text":"<pre><code>HistoryManager(db: EntityDatabase, platform_config: PlatformConfig, registrar: CallbackRegistrar)\n</code></pre> Source code in <code>dp3/history_management/history_manager.py</code> <pre><code>def __init__(\n    self, db: EntityDatabase, platform_config: PlatformConfig, registrar: CallbackRegistrar\n) -&gt; None:\n    self.log = logging.getLogger(\"HistoryManager\")\n\n    self.db = db\n    self.model_spec = platform_config.model_spec\n    self.worker_index = platform_config.process_index\n    self.num_workers = platform_config.num_processes\n    self.config = HistoryManagerConfig.model_validate(\n        platform_config.config.get(\"history_manager\")\n    )\n\n    # Schedule master document aggregation\n    registrar.scheduler_register(\n        self.aggregate_master_docs, **self.config.aggregation_schedule.model_dump()\n    )\n\n    if platform_config.process_index != 0:\n        self.log.debug(\n            \"History management will be disabled in this worker to avoid race conditions.\"\n        )\n        return\n\n    # Schedule datapoints cleaning\n    datapoint_marking_schedule = self.config.mark_datapoints_schedule\n    registrar.scheduler_register(\n        self.mark_datapoints_in_master_docs, **datapoint_marking_schedule.model_dump()\n    )\n\n    datapoint_cleaning_schedule = self.config.datapoint_cleaning_schedule\n    registrar.scheduler_register(\n        self.delete_old_dps, **datapoint_cleaning_schedule.model_dump()\n    )\n\n    snapshot_cleaning_schedule = self.config.snapshot_cleaning.schedule\n    self.keep_snapshot_delta = self.config.snapshot_cleaning.older_than\n    registrar.scheduler_register(\n        self.delete_old_snapshots, **snapshot_cleaning_schedule.model_dump()\n    )\n\n    # Schedule datapoint archivation\n    archive_config = self.config.datapoint_archivation\n    self.keep_raw_delta = archive_config.older_than\n    if archive_config.archive_dir is not None:\n        self.log_dir = self._ensure_log_dir(archive_config.archive_dir)\n    else:\n        self.log_dir = None\n    registrar.scheduler_register(self.archive_old_dps, **archive_config.schedule.model_dump())\n</code></pre>"},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.HistoryManager.delete_old_dps","title":"delete_old_dps","text":"<pre><code>delete_old_dps()\n</code></pre> <p>Deletes old data points from master collection.</p> Source code in <code>dp3/history_management/history_manager.py</code> <pre><code>def delete_old_dps(self):\n    \"\"\"Deletes old data points from master collection.\"\"\"\n    self.log.debug(\"Deleting old records ...\")\n\n    for etype_attr, attr_conf in self.model_spec.attributes.items():\n        etype, attr_name = etype_attr\n        max_age = None\n\n        if attr_conf.t == AttrType.OBSERVATIONS:\n            max_age = attr_conf.history_params.max_age\n        elif attr_conf.t == AttrType.TIMESERIES:\n            max_age = attr_conf.timeseries_params.max_age\n\n        if not max_age:\n            continue\n\n        t_old = datetime.utcnow() - max_age\n\n        try:\n            self.db.delete_old_dps(etype, attr_name, t_old)\n        except DatabaseError as e:\n            self.log.error(e)\n</code></pre>"},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.HistoryManager.mark_datapoints_in_master_docs","title":"mark_datapoints_in_master_docs","text":"<pre><code>mark_datapoints_in_master_docs()\n</code></pre> <p>Marks the timestamps of all datapoints in master documents.</p> Source code in <code>dp3/history_management/history_manager.py</code> <pre><code>def mark_datapoints_in_master_docs(self):\n    \"\"\"Marks the timestamps of all datapoints in master documents.\"\"\"\n    self.log.debug(\"Marking the datapoint timestamps for all entity records ...\")\n\n    for entity, attr_conf in self.model_spec.entity_attributes.items():\n        attrs_to_mark = []\n        for attr, conf in attr_conf.items():\n            if conf.t in AttrType.OBSERVATIONS | AttrType.TIMESERIES:\n                attrs_to_mark.append(attr)\n\n        if not attrs_to_mark:\n            continue\n        try:\n            res = self.db.mark_all_entity_dps_t2(entity, attrs_to_mark)\n            self.log.debug(\"Marked %s records of %s\", res.modified_count, entity)\n        except DatabaseError as e:\n            self.log.error(e)\n</code></pre>"},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.HistoryManager.delete_old_snapshots","title":"delete_old_snapshots","text":"<pre><code>delete_old_snapshots()\n</code></pre> <p>Deletes old snapshots.</p> Source code in <code>dp3/history_management/history_manager.py</code> <pre><code>def delete_old_snapshots(self):\n    \"\"\"Deletes old snapshots.\"\"\"\n    t_old = datetime.now() - self.keep_snapshot_delta\n    self.log.debug(\"Deleting all snapshots before %s\", t_old)\n\n    deleted_total = 0\n    try:\n        deleted_total = self.db.snapshots.delete_old(t_old)\n    except DatabaseError as e:\n        self.log.exception(e)\n    self.log.debug(\"Deleted %s snapshots in total.\", deleted_total)\n</code></pre>"},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.HistoryManager.archive_old_dps","title":"archive_old_dps","text":"<pre><code>archive_old_dps()\n</code></pre> <p>Archives old data points from raw collection.</p> <p>Updates already saved archive files, if present.</p> Source code in <code>dp3/history_management/history_manager.py</code> <pre><code>def archive_old_dps(self):\n    \"\"\"\n    Archives old data points from raw collection.\n\n    Updates already saved archive files, if present.\n    \"\"\"\n\n    t_old = datetime.utcnow() - self.keep_raw_delta\n    self.log.debug(\"Archiving all records before %s ...\", t_old)\n\n    for etype in self.model_spec.entities:\n        res = self.db.move_raw_to_archive(etype)\n        if res:\n            self.log.info(\"Current %s raw collection was moved to archive: %s\", etype, res)\n\n    max_date, min_date, total_dps = self._get_raw_dps_summary(t_old)\n    if total_dps == 0:\n        self.log.debug(\"Found no datapoints to archive.\")\n        return\n    self.log.debug(\n        \"Found %s datapoints to archive in the range %s - %s\", total_dps, min_date, max_date\n    )\n\n    if self.log_dir is None:\n        self.log.debug(\"No archive directory specified, skipping archivation.\")\n    else:\n        min_date_string = min_date.strftime(\"%Y%m%dT%H%M%S\")\n        max_date_string = max_date.strftime(\"%Y%m%dT%H%M%S\")\n        date_logfile = self.log_dir / f\"dp-log-{min_date_string}--{max_date_string}.jsonl\"\n        datapoints = 0\n\n        with open(date_logfile, \"w\", encoding=\"utf-8\") as logfile:\n            for etype in self.model_spec.entities:\n                for result_cursor in self.db.get_archive(etype, after=min_date, before=t_old):\n                    for dp in result_cursor:\n                        logfile.write(f\"{json.dumps(self._reformat_dp(dp), cls=DP3Encoder)}\\n\")\n                        datapoints += 1\n\n        self.log.info(\"Archived %s datapoints to %s\", datapoints, date_logfile)\n        compress_file(date_logfile)\n        os.remove(date_logfile)\n        self.log.debug(\"Saved archive was compressed\")\n\n    deleted_count = 0\n    for etype in self.model_spec.entities:\n        for deleted_res in self.db.delete_old_archived_dps(etype, before=t_old):\n            deleted_count += deleted_res.deleted_count\n    self.log.info(\"Deleted %s datapoints\", deleted_count)\n\n    dropped_count = 0\n    for etype in self.model_spec.entities:\n        dropped_count += self.db.drop_empty_archives(etype)\n    if dropped_count:\n        self.log.info(\"Dropped %s empty archive collection(s)\", dropped_count)\n</code></pre>"},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.aggregate_multivalue_dp_history_on_equal","title":"aggregate_multivalue_dp_history_on_equal","text":"<pre><code>aggregate_multivalue_dp_history_on_equal(history: list[dict], spec: AttrSpecObservations)\n</code></pre> <p>Merge multivalue datapoints in the history with equal values and overlapping time validity.</p> <p>Avergages the confidence. Will keep a pool of \"active\" datapoints and merge them with the next datapoint if they have the same value and overlapping time validity.</p> FIXME <p>The average calculation only works for the current iteration, but for the next call of the algorithm, the count of aggregated datapoints is lost.</p> Source code in <code>dp3/history_management/history_manager.py</code> <pre><code>def aggregate_multivalue_dp_history_on_equal(history: list[dict], spec: AttrSpecObservations):\n    \"\"\"\n    Merge multivalue datapoints in the history with equal values and overlapping time validity.\n\n    Avergages the confidence.\n    Will keep a pool of \"active\" datapoints and merge them with the next datapoint\n    if they have the same value and overlapping time validity.\n\n    FIXME:\n      The average calculation only works for the current iteration,\n      but for the next call of the algorithm, the count of aggregated datapoints is lost.\n    \"\"\"\n    history = sorted(history, key=lambda x: x[\"t1\"])\n    aggregated_history = []\n    pre = spec.history_params.pre_validity\n    post = spec.history_params.post_validity\n\n    if spec.data_type.hashable:\n        current_dps = {}\n\n        for dp in history:\n            v = dp[\"v\"]\n            if v in current_dps:\n                current_dp = current_dps[v]\n                if current_dp[\"t2\"] + post &gt;= dp[\"t1\"] - pre:  # Merge with current_dp\n                    current_dp[\"t2\"] = max(dp[\"t2\"], current_dp[\"t2\"])\n                    current_dp[\"c\"] += dp[\"c\"]\n                    current_dp[\"cnt\"] += 1\n                else:  # No overlap, finalize current_dp and reset\n                    current_dp[\"c\"] /= current_dp[\"cnt\"]\n                    del current_dp[\"cnt\"]\n                    aggregated_history.append(current_dp)\n                    current_dps[v] = dp\n                    current_dps[v][\"cnt\"] = 1\n            else:  # New value, finalize initialize current_dp\n                current_dps[v] = dp\n                current_dps[v][\"cnt\"] = 1\n\n        for _v, current_dp in current_dps.items():  # Finalize remaining dps\n            current_dp[\"c\"] /= current_dp[\"cnt\"]\n            del current_dp[\"cnt\"]\n            aggregated_history.append(current_dp)\n        return aggregated_history\n    else:\n        current_dps = []\n\n        for dp in history:\n            v = dp[\"v\"]\n            for i, current_dp in enumerate(current_dps):\n                if current_dp[\"v\"] != v:\n                    continue\n\n                if current_dp[\"t2\"] + post &gt;= dp[\"t1\"] - pre:  # Merge with current_dp\n                    current_dp[\"t2\"] = max(dp[\"t2\"], current_dp[\"t2\"])\n                    current_dp[\"c\"] += dp[\"c\"]\n                    current_dp[\"cnt\"] += 1\n                else:  # No overlap, finalize current_dp and reset\n                    current_dp[\"c\"] /= current_dp[\"cnt\"]\n                    del current_dp[\"cnt\"]\n                    aggregated_history.append(current_dp)\n                    dp[\"cnt\"] = 1\n                    current_dps[i] = dp\n                break\n            else:  # New value, finalize initialize current_dp\n                dp[\"cnt\"] = 1\n                current_dps.append(dp)\n\n        for current_dp in current_dps:  # Finalize remaining dps\n            current_dp[\"c\"] /= current_dp[\"cnt\"]\n            del current_dp[\"cnt\"]\n            aggregated_history.append(current_dp)\n        return aggregated_history\n</code></pre>"},{"location":"reference/history_management/history_manager/#dp3.history_management.history_manager.aggregate_dp_history_on_equal","title":"aggregate_dp_history_on_equal","text":"<pre><code>aggregate_dp_history_on_equal(history: list[dict], spec: ObservationsHistoryParams)\n</code></pre> <p>Merge datapoints in the history with equal values and overlapping time validity.</p> <p>Avergages the confidence.</p> FIXME <p>The average calculation only works for the current iteration, but for the next call of the algorithm, the count of aggregated datapoints is lost.</p> Source code in <code>dp3/history_management/history_manager.py</code> <pre><code>def aggregate_dp_history_on_equal(history: list[dict], spec: ObservationsHistoryParams):\n    \"\"\"\n    Merge datapoints in the history with equal values and overlapping time validity.\n\n    Avergages the confidence.\n\n    FIXME:\n      The average calculation only works for the current iteration,\n      but for the next call of the algorithm, the count of aggregated datapoints is lost.\n    \"\"\"\n    history = sorted(history, key=lambda x: x[\"t1\"])\n    aggregated_history = []\n    current_dp = None\n    merged_cnt = 0\n    pre = spec.pre_validity\n    post = spec.post_validity\n\n    for dp in history:\n        if not current_dp:\n            current_dp = dp\n            merged_cnt += 1\n            continue\n\n        if current_dp[\"v\"] == dp[\"v\"] and current_dp[\"t2\"] + post &gt;= dp[\"t1\"] - pre:\n            current_dp[\"t2\"] = max(dp[\"t2\"], current_dp[\"t2\"])\n            current_dp[\"c\"] += dp[\"c\"]\n            merged_cnt += 1\n        else:\n            aggregated_history.append(current_dp)\n            current_dp[\"c\"] /= merged_cnt\n\n            merged_cnt = 1\n            current_dp = dp\n    if current_dp:\n        current_dp[\"c\"] /= merged_cnt\n        aggregated_history.append(current_dp)\n    return aggregated_history\n</code></pre>"},{"location":"reference/history_management/telemetry/","title":"telemetry","text":""},{"location":"reference/history_management/telemetry/#dp3.history_management.telemetry","title":"dp3.history_management.telemetry","text":""},{"location":"reference/history_management/telemetry/#dp3.history_management.telemetry.Telemetry","title":"Telemetry","text":"<pre><code>Telemetry(db: EntityDatabase, platform_config: PlatformConfig, registrar: CallbackRegistrar)\n</code></pre> Source code in <code>dp3/history_management/telemetry.py</code> <pre><code>def __init__(\n    self, db: EntityDatabase, platform_config: PlatformConfig, registrar: CallbackRegistrar\n) -&gt; None:\n    self.log = logging.getLogger(\"Telemetry\")\n\n    self.db = db\n    self.model_spec = platform_config.model_spec\n    # self.config = platform_config.config.get(\"telemetry\")  # No config for now\n    self.cache_col = self.db.get_module_cache(\"Telemetry\")\n\n    self.local_cache = {}\n    self.local_cache_lock = threading.Lock()\n\n    # Schedule master document aggregation\n    registrar.register_task_hook(\"on_task_start\", self.note_latest_src_timestamp)\n    mod = 30\n    proc_i = platform_config.process_index\n    n_proc = platform_config.num_processes\n    spread_proc_index = proc_i * (mod // n_proc) if n_proc &lt; mod else proc_i\n    seconds = \",\".join(f\"{int(i)}\" for i in range(60) if int(i - spread_proc_index) % mod == 0)\n    registrar.scheduler_register(\n        self.sync_to_db, second=seconds, minute=\"*\", hour=\"*\", misfire_grace_time=10\n    )\n</code></pre>"},{"location":"reference/history_management/telemetry/#dp3.history_management.telemetry.Telemetry.note_latest_src_timestamp","title":"note_latest_src_timestamp","text":"<pre><code>note_latest_src_timestamp(task: DataPointTask)\n</code></pre> <p>Note the latest timestamp of each source in the task</p> Source code in <code>dp3/history_management/telemetry.py</code> <pre><code>def note_latest_src_timestamp(self, task: DataPointTask):\n    \"\"\"Note the latest timestamp of each source in the task\"\"\"\n    latest_timestamps = {}\n    for dp in task.data_points:\n        has_timestamp = isinstance(dp, (DataPointObservationsBase, DataPointTimeseriesBase))\n        if dp.src is None or not has_timestamp:\n            continue\n        latest_timestamp = dp.t2 or dp.t1\n        latest_timestamps[dp.src] = latest_timestamp\n\n    if not latest_timestamps:\n        return\n\n    with self.local_cache_lock:\n        self.local_cache.update(latest_timestamps)\n</code></pre>"},{"location":"reference/history_management/telemetry/#dp3.history_management.telemetry.Telemetry.sync_to_db","title":"sync_to_db","text":"<pre><code>sync_to_db()\n</code></pre> <p>Sync local timestamp cache to database.</p> Source code in <code>dp3/history_management/telemetry.py</code> <pre><code>def sync_to_db(self):\n    \"\"\"Sync local timestamp cache to database.\"\"\"\n    with self.local_cache_lock:\n        synced_cache = self.local_cache\n        self.local_cache = {}\n\n    updates = [\n        UpdateOne(\n            {\"_id\": src},\n            [{\"$set\": {\"src_t\": {\"$max\": [\"$src_t\", latest_timestamp]}}}],\n        )\n        for src, latest_timestamp in synced_cache.items()\n    ]\n\n    if not updates:\n        return\n\n    try:\n        start = time.time()\n        res = self.cache_col.bulk_write(updates, ordered=False)\n        end = time.time()\n        self.log.debug(\n            \"Updating %s src_timestamp records: %s matched %s modified in %.4fs\",\n            len(updates),\n            res.matched_count,\n            res.modified_count,\n            (end - start),\n        )\n        if len(updates) != res.matched_count:\n            upserts = [\n                UpdateOne(\n                    {\"_id\": src},\n                    [{\"$set\": {\"_id\": src, \"src_t\": {\"$max\": [\"$src_t\", latest_timestamp]}}}],\n                    upsert=True,\n                )\n                for src, latest_timestamp in synced_cache.items()\n            ]\n            start = time.time()\n            res = self.cache_col.bulk_write(upserts, ordered=False)\n            end = time.time()\n            self.log.debug(\n                \"Upserting %s src_timestamp records: %s matched %s modified in %.4fs\",\n                len(upserts),\n                res.matched_count,\n                res.modified_count,\n                (end - start),\n            )\n    except Exception as e:\n        self.log.error(\"Error updating src_timestamp records: %s\", e)\n</code></pre>"},{"location":"reference/history_management/telemetry/#dp3.history_management.telemetry.TelemetryReader","title":"TelemetryReader","text":"<pre><code>TelemetryReader(db: EntityDatabase)\n</code></pre> <p>Reader of telemetry data</p> <p>Used by API. Not contained inside <code>Telemetry</code> class due to usage of <code>CallbackRegistrar</code> and all of it's requirements (doesn't make sense for API).</p> Source code in <code>dp3/history_management/telemetry.py</code> <pre><code>def __init__(self, db: EntityDatabase) -&gt; None:\n    self.db = db\n    self.cache_col = self.db.get_module_cache(\"Telemetry\")\n</code></pre>"},{"location":"reference/history_management/telemetry/#dp3.history_management.telemetry.TelemetryReader.get_sources_validity","title":"get_sources_validity","text":"<pre><code>get_sources_validity() -&gt; dict[str, datetime]\n</code></pre> <p>Return timestamps (datetimes) of current validity of all sources.</p> Source code in <code>dp3/history_management/telemetry.py</code> <pre><code>def get_sources_validity(self) -&gt; dict[str, datetime]:\n    \"\"\"Return timestamps (datetimes) of current validity of all sources.\"\"\"\n    src_data = self.cache_col.find({}).sort([(\"_id\", ASCENDING)])\n\n    return {src[\"_id\"]: src[\"src_t\"] for src in src_data}\n</code></pre>"},{"location":"reference/scripts/","title":"scripts","text":""},{"location":"reference/scripts/#dp3.scripts","title":"dp3.scripts","text":""},{"location":"reference/scripts/add_hashes/","title":"add_hashes","text":""},{"location":"reference/scripts/add_hashes/#dp3.scripts.add_hashes","title":"dp3.scripts.add_hashes","text":"<p>Simple script to add hashes to master records to allow for easier parallelization.</p> <p>Intended to be run only once to upgrade a database existing before 08-2023.</p>"},{"location":"reference/scripts/add_min_t2s/","title":"add_min_t2s","text":""},{"location":"reference/scripts/add_min_t2s/#dp3.scripts.add_min_t2s","title":"dp3.scripts.add_min_t2s","text":"<p>Simple script to add min_t2s to master records to allow for better indexing.</p> <p>Intended to be run only once to upgrade a database existing before 08-2024.</p>"},{"location":"reference/scripts/datapoint_log_converter/","title":"datapoint_log_converter","text":""},{"location":"reference/scripts/datapoint_log_converter/#dp3.scripts.datapoint_log_converter","title":"dp3.scripts.datapoint_log_converter","text":"<p>Converts legacy CSV DataPoint log format to JSON</p>"},{"location":"reference/scripts/datapoint_log_converter/#dp3.scripts.datapoint_log_converter.LegacyDataPointLoader","title":"LegacyDataPointLoader","text":"<pre><code>LegacyDataPointLoader(attr_config_dirname: str)\n</code></pre> <p>Loader of datapoint files as written by DP3 API receiver.</p> <p>Create a datapoint loader.</p> <p>attr_config_dirname: Directory with attribute configuration (same as for DP3)</p> Source code in <code>dp3/scripts/datapoint_log_converter.py</code> <pre><code>def __init__(self, attr_config_dirname: str):\n    \"\"\"\n    Create a datapoint loader.\n\n    attr_config_dirname: Directory with attribute configuration (same as for DP3)\n    \"\"\"\n    # Load attribute config\n    model_spec = ModelSpec(read_config_dir(attr_config_dirname))\n\n    # Prepare a table for data type conversion\n    # (to get data type from model_spec: model_spec[etype][\"attribs\"][attrname].data_type)\n    self.dt_conv = {}  # map (etype,attr_name) -&gt; conversion_function\n    for etype, spec in model_spec.items():\n        for aname, aspec in spec[\"attribs\"].items():\n            data_type = getattr(aspec, \"data_type\", None)\n            converter = json.loads if data_type is None else get_converter(str(data_type))\n            self.dt_conv[(etype, aname)] = converter\n\n    self.model_spec = model_spec\n</code></pre>"},{"location":"reference/scripts/datapoint_log_converter/#dp3.scripts.datapoint_log_converter.LegacyDataPointLoader.read_dp_file","title":"read_dp_file","text":"<pre><code>read_dp_file(filename: str) -&gt; pd.DataFrame\n</code></pre> <p>Read a file with ADiCT/DP3 datapoints into pandas DataFrame.</p> <p>Values of attributes in datapoints are validated and converted according to the attribute configuration passed to LegacyDataPointLoader constructor.</p> Source code in <code>dp3/scripts/datapoint_log_converter.py</code> <pre><code>def read_dp_file(self, filename: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Read a file with ADiCT/DP3 datapoints into pandas DataFrame.\n\n    Values of attributes in datapoints are validated and converted according\n    to the attribute configuration passed to LegacyDataPointLoader constructor.\n    \"\"\"\n    open_function = gzip.open if filename.endswith(\".gz\") else open\n\n    # Reformat datapoints file so \"val\" containing commas can be read properly.\n    #   Replace first 7 commas (i.e. all except those inside \"v\") with semicolon\n    #   Store as temporary file\n    tmp_name = (\n        f\"tmp-{'.'.join(os.path.basename(os.path.normpath(filename)).split(sep='.')[:-1])}\"\n    )\n    with open_function(filename, \"rb\") as infile, open(tmp_name, \"wb\") as outfile:\n        for line in infile:\n            outfile.write(line.replace(b\",\", b\";\", 7))\n    # Load the converted file\n    data = pd.read_csv(\n        tmp_name,\n        sep=\";\",\n        header=None,\n        names=self.COL_NAMES,\n        index_col=False,\n        converters={\"c\": float, \"v\": str},\n        escapechar=\"\\\\\",\n        # parse_dates=[\"t1\", \"t2\"],\n        # infer_datetime_format=True,\n    )\n    # Cleanup\n    if os.path.exists(tmp_name):\n        os.remove(tmp_name)\n\n    # Convert values to correct types according to model_spec\n    def convert_row(row):\n        try:\n            row[2] = self.dt_conv[(row[0], row[1])](row[2])\n        except KeyError as e:\n            raise KeyError(f\"No converter for {(row[0], row[1])}, with value {row[2]}.\") from e\n        except ValueError:\n            self.log.error(\"ValueError in conversion, v: %s\", row)\n            return row\n        return row\n\n    attrs = {entity_attr[1] for entity_attr in self.dt_conv}\n    conv_vals = data.loc[data[\"attr\"].isin(attrs), (\"type\", \"attr\", \"v\")].apply(\n        convert_row, axis=1, raw=True\n    )\n    if len(conv_vals) != len(data):\n        self.log.warning(\n            \"Dropped %s rows due to missing attributes in config\", len(data) - len(conv_vals)\n        )\n        self.log.info(\"Missing attrs: %s\", [x for x in data[\"attr\"].unique() if x not in attrs])\n    data[\"v\"] = conv_vals[\"v\"]\n    return data[data[\"attr\"].apply(lambda x: x in attrs)]\n</code></pre>"},{"location":"reference/scripts/datapoint_log_converter/#dp3.scripts.datapoint_log_converter.get_converter","title":"get_converter","text":"<pre><code>get_converter(attr_data_type: str) -&gt; Callable[[str], Any]\n</code></pre> <p>Return a function converting a string to given data type.</p> Source code in <code>dp3/scripts/datapoint_log_converter.py</code> <pre><code>def get_converter(attr_data_type: str) -&gt; Callable[[str], Any]:\n    \"\"\"Return a function converting a string to given data type.\"\"\"\n    # basic type\n    if attr_data_type in CONVERTERS:\n        return CONVERTERS[attr_data_type]\n    # array&lt;X&gt;, set&lt;X&gt;, dict&lt;X,Y,Z&gt;\n    if (\n        re.match(re_array, attr_data_type)\n        or re.match(re_set, attr_data_type)\n        or re.match(re_dict, attr_data_type)\n    ):\n        return json.loads\n    # link&lt;X&gt;\n    if re.match(re_link, attr_data_type):\n        return str\n    # category&lt;X; Y&gt;\n    if re.match(re_category, attr_data_type):\n        return str\n    raise ValueError(f\"No conversion function for attribute type '{attr_data_type}'\")\n</code></pre>"},{"location":"reference/scripts/datapoint_log_converter/#dp3.scripts.datapoint_log_converter.get_out_path","title":"get_out_path","text":"<pre><code>get_out_path(in_file_path, output_dir)\n</code></pre> <p>Return output file path based on the input file. Parses the date from input filename, fits date into prepared pattern: \"dp_log_{date}.json\".</p> Source code in <code>dp3/scripts/datapoint_log_converter.py</code> <pre><code>def get_out_path(in_file_path, output_dir):\n    \"\"\"\n    Return output file path based on the input file.\n    Parses the date from input filename, fits date into prepared pattern: \"dp_log_{date}.json\".\n    \"\"\"\n    date = in_file_path.split(\"-\")[-1]\n    if date.endswith(\".gz\"):\n        date = date[:-3]\n    out_filename = f\"dp_log_{date}.json\"\n    return os.path.join(output_dir, out_filename)\n</code></pre>"},{"location":"reference/scripts/dummy_sender/","title":"dummy_sender","text":""},{"location":"reference/scripts/dummy_sender/#dp3.scripts.dummy_sender","title":"dp3.scripts.dummy_sender","text":"<p>Simple datapoint sender script for testing local DP3 instance.</p>"},{"location":"reference/scripts/dummy_sender/#dp3.scripts.dummy_sender.batched","title":"batched","text":"<pre><code>batched(iterable, n)\n</code></pre> <p>Batch data into tuples of length n. The last batch may be shorter.</p> Source code in <code>dp3/scripts/dummy_sender.py</code> <pre><code>def batched(iterable, n):\n    \"\"\"Batch data into tuples of length n. The last batch may be shorter.\"\"\"\n    # batched('ABCDEFG', 3) --&gt; ABC DEF G\n    if n &lt; 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while batch := list(islice(it, n)):\n        yield batch\n</code></pre>"},{"location":"reference/scripts/migrate_snapshots/","title":"migrate_snapshots","text":""},{"location":"reference/scripts/migrate_snapshots/#dp3.scripts.migrate_snapshots","title":"dp3.scripts.migrate_snapshots","text":"<p>Simple script to migrate snapshots schema from flat single-snapshot documents to a nested history-last schema.</p> <p>Intended to be run only once to upgrade a database existing before 08-2024.</p>"},{"location":"reference/scripts/migrate_snapshots/#dp3.scripts.migrate_snapshots.save_snapshot","title":"save_snapshot","text":"<pre><code>save_snapshot(etype: str, snapshot: dict)\n</code></pre> <p>Saves snapshot to specified entity of current master document.</p> <p>Will move snapshot to oversized snapshots if the maintained bucket is too large.</p> Source code in <code>dp3/scripts/migrate_snapshots.py</code> <pre><code>def save_snapshot(etype: str, snapshot: dict):\n    \"\"\"Saves snapshot to specified entity of current master document.\n\n    Will move snapshot to oversized snapshots if the maintained bucket is too large.\n    \"\"\"\n    if \"eid\" not in snapshot:\n        return\n    eid = snapshot[\"eid\"]\n\n    if \"_id\" in snapshot:\n        print(f\"Removing _id {snapshot['_id']} from snapshot of {eid}\")\n        del snapshot[\"_id\"]\n\n    snapshot_col = f\"{etype}#snapshots\"\n    os_col = f\"{etype}#snapshots_oversized\"\n\n    # Find out if the snapshot is oversized\n    doc = db[snapshot_col].find_one({\"_id\": eid}, {\"oversized\": 1})\n    if doc is None:\n        # First snapshot of entity\n        db[snapshot_col].insert_one(\n            {\"_id\": eid, \"last\": snapshot, \"history\": [], \"oversized\": False, \"count\": 0}\n        )\n        print(f\"Inserted snapshot of {eid}\")\n        return\n    elif doc.get(\"oversized\", False):\n        # Snapshot is already marked as oversized\n        db[snapshot_col].update_one({\"_id\": eid}, {\"$set\": {\"last\": snapshot}})\n        db[os_col].insert_one(snapshot)\n        return\n\n    try:\n        # Update a normal snapshot bucket\n        res = db[snapshot_col].update_one(\n            {\"_id\": eid},\n            [\n                {\n                    \"$set\": {\n                        \"history\": {\n                            \"$concatArrays\": [\n                                [\"$last\"],\n                                \"$history\",\n                            ]\n                        },\n                        \"count\": {\"$sum\": [\"$count\", 1]},\n                    }\n                },\n                {\"$set\": {\"last\": {\"$literal\": snapshot}}},\n            ],\n        )\n        if res.modified_count == 0:\n            print(f\"Snapshot of {eid} was not updated, {res.raw_result}\")\n    except (WriteError, OperationFailure) as e:\n        if e.code != BSON_OBJECT_TOO_LARGE:\n            raise e\n        # The snapshot is too large, move it to oversized snapshots\n        print(f\"Snapshot of {eid} is too large: {e}, marking as oversized.\")\n        _migrate_to_oversized_snapshot(etype, eid, snapshot)\n    except Exception as e:\n        raise DatabaseError(f\"Insert of snapshot {eid} failed: {e}, {snapshot}\") from e\n</code></pre>"},{"location":"reference/scripts/migrate_snapshots/#dp3.scripts.migrate_snapshots.save_snapshots","title":"save_snapshots","text":"<pre><code>save_snapshots(etype: str, snapshots: list[dict])\n</code></pre> <p>Saves a list of snapshots of current master documents.</p> <p>All snapshots must belong to same entity type.</p> <p>Will move snapshots to oversized snapshots if the maintained bucket is too large. For better understanding, see <code>save_snapshot()</code>.</p> Source code in <code>dp3/scripts/migrate_snapshots.py</code> <pre><code>def save_snapshots(etype: str, snapshots: list[dict]):\n    \"\"\"\n    Saves a list of snapshots of current master documents.\n\n    All snapshots must belong to same entity type.\n\n    Will move snapshots to oversized snapshots if the maintained bucket is too large.\n    For better understanding, see `save_snapshot()`.\n\n    \"\"\"\n    snapshots_by_eid = defaultdict(list)\n    for snapshot in snapshots:\n        if \"eid\" not in snapshot:\n            continue\n        if \"_id\" in snapshot:\n            del snapshot[\"_id\"]\n        snapshots_by_eid[snapshot[\"eid\"]].append(snapshot)\n    print(f\"Saving {len(snapshots)} snapshots of {len(snapshots_by_eid)} entities of {etype}\")\n\n    snapshot_col = f\"{etype}#snapshots\"\n    os_col = f\"{etype}#snapshots_oversized\"\n\n    # Find out if any of the snapshots are oversized\n    docs = list(\n        db[snapshot_col].find(\n            {\"_id\": {\"$in\": list(snapshots_by_eid.keys())}}, {\"oversized\": 1, \"eid\": 1}\n        )\n    )\n\n    updates = []\n    update_originals = []\n    oversized_inserts = []\n    oversized_updates = []\n\n    for doc in docs:\n        eid = doc[\"_id\"]\n        if not doc.get(\"oversized\", False):\n            # A normal snapshot, shift the last snapshot to history and update last\n            updates.append(\n                UpdateOne(\n                    {\"_id\": eid},\n                    [\n                        {\n                            \"$set\": {\n                                \"history\": {\n                                    \"$concatArrays\": [\n                                        snapshots_by_eid[eid][:-1],\n                                        [\"$last\"],\n                                        \"$history\",\n                                    ]\n                                },\n                                \"count\": {\"$sum\": [len(snapshots_by_eid[eid]), \"$count\"]},\n                            }\n                        },\n                        {\"$set\": {\"last\": {\"$literal\": snapshots_by_eid[eid][-1]}}},\n                    ],\n                )\n            )\n            update_originals.append(snapshots_by_eid[eid])\n        else:\n            # Snapshot is already marked as oversized\n            oversized_inserts.extend(snapshots_by_eid[eid])\n            oversized_updates.append(\n                UpdateOne({\"_id\": eid}, {\"$set\": {\"last\": snapshots_by_eid[eid][-1]}})\n            )\n        del snapshots_by_eid[eid]\n\n    # The remaining snapshots are new\n    inserts = [\n        {\n            \"_id\": eid,\n            \"last\": eid_snapshots[-1],\n            \"history\": eid_snapshots[:-1],\n            \"oversized\": False,\n            \"count\": len(eid_snapshots) - 1,\n        }\n        for eid, eid_snapshots in snapshots_by_eid.items()\n    ]\n\n    if updates:\n        try:\n            res = db[snapshot_col].bulk_write(updates, ordered=False)\n            if res.modified_count != len(updates):\n                print(\n                    f\"Some snapshots were not updated, \"\n                    f\"{res.modified_count} != {len(snapshots_by_eid)}\"\n                )\n        except (BulkWriteError, OperationFailure) as e:\n            print(\"Update of snapshots failed, will retry with oversize.\")\n            failed_indexes = [\n                err[\"index\"]\n                for err in e.details[\"writeErrors\"]\n                if err[\"code\"] == BSON_OBJECT_TOO_LARGE\n            ]\n            failed_snapshots = (update_originals[i] for i in failed_indexes)\n            for eid_snapshots in failed_snapshots:\n                eid = eid_snapshots[0][\"eid\"]\n                failed_snapshots = sorted(\n                    eid_snapshots, key=lambda s: s[\"_time_created\"], reverse=True\n                )\n                _migrate_to_oversized_snapshot(etype, eid, failed_snapshots[0])\n                oversized_inserts.extend(failed_snapshots[1:])\n\n            if any(err[\"code\"] != BSON_OBJECT_TOO_LARGE for err in e.details[\"writeErrors\"]):\n                # Some other error occurred\n                raise e\n        except Exception as e:\n            raise DatabaseError(f\"Update of snapshots failed: {str(e)[:2048]}\") from e\n\n    if inserts:\n        try:\n            # Insert new snapshots\n            res = db[snapshot_col].insert_many(inserts, ordered=False)\n            if len(res.inserted_ids) != len(snapshots_by_eid):\n                print(\n                    f\"Some snapshots were not inserted, \"\n                    f\"{len(res.inserted_ids)} != {len(snapshots_by_eid)}\"\n                )\n        except (DocumentTooLarge, OperationFailure) as e:\n            print(f\"Snapshot too large: {e}\")\n            checked_inserts = []\n            oversized_inserts = []\n\n            # Filter out the oversized snapshots\n            for insert_doc in inserts:\n                bsize = len(bson.BSON.encode(insert_doc))\n                if bsize &lt; 16 * 1024 * 1024:\n                    checked_inserts.append(insert_doc)\n                else:\n                    eid = insert_doc[\"_id\"]\n                    checked_inserts.append(\n                        {\n                            \"_id\": eid,\n                            \"last\": insert_doc[\"last\"],\n                            \"oversized\": True,\n                            \"history\": [],\n                            \"count\": 0,\n                        }\n                    )\n                    oversized_inserts.extend(insert_doc[\"history\"] + [insert_doc[\"last\"]])\n            try:\n                db[snapshot_col].insert_many(checked_inserts, ordered=False)\n            except Exception as e:\n                raise DatabaseError(f\"Insert of snapshots failed: {e}\") from e\n        except Exception as e:\n            raise DatabaseError(f\"Insert of snapshot failed: {e}\") from e\n\n    # Update the oversized snapshots\n    if oversized_inserts:\n        try:\n            if oversized_updates:\n                db[snapshot_col].bulk_write(oversized_updates)\n            db[os_col].insert_many(oversized_inserts)\n        except Exception as e:\n            raise DatabaseError(f\"Insert of snapshots failed: {str(e)[:2048]}\") from e\n</code></pre>"},{"location":"reference/scripts/script_runner/","title":"script_runner","text":""},{"location":"reference/scripts/script_runner/#dp3.scripts.script_runner","title":"dp3.scripts.script_runner","text":"<p>A script runner entrypoint for DP3 scripts.</p>"},{"location":"reference/snapshots/","title":"snapshots","text":""},{"location":"reference/snapshots/#dp3.snapshots","title":"dp3.snapshots","text":"<p>SnapShooter, a module responsible for snapshot creation and running configured data correlation and fusion hooks, and Snapshot Hooks, which manage the registered hooks and their dependencies on one another.</p>"},{"location":"reference/snapshots/snapshooter/","title":"snapshooter","text":""},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter","title":"dp3.snapshots.snapshooter","text":"<p>Module managing creation of snapshots, enabling data correlation and saving snapshots to DB.</p> <ul> <li> <p>Snapshots are created periodically (user configurable period)</p> </li> <li> <p>When a snapshot is created, several things need to happen:</p> <ul> <li>all registered timeseries processing modules must be called</li> <li>this should result in <code>observations</code> or <code>plain</code> datapoints, which will be saved to db     and forwarded in processing</li> <li>current value must be computed for all observations</li> <li>load relevant section of observation's history and perform configured history analysis.     Result = plain values</li> <li>load plain attributes saved in master collection</li> <li>A record of described plain data makes a <code>profile</code></li> <li>Profile is additionally extended by related entities</li> <li>Callbacks for data correlation and fusion should happen here</li> <li>Save the complete results into database as snapshots</li> </ul> </li> </ul>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter","title":"SnapShooter","text":"<pre><code>SnapShooter(db: EntityDatabase, task_queue_writer: TaskQueueWriter, platform_config: PlatformConfig, scheduler: Scheduler, elog: Optional[EventGroupType] = None)\n</code></pre> <p>Class responsible for creating entity snapshots.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def __init__(\n    self,\n    db: EntityDatabase,\n    task_queue_writer: TaskQueueWriter,\n    platform_config: PlatformConfig,\n    scheduler: Scheduler,\n    elog: Optional[EventGroupType] = None,\n) -&gt; None:\n    self.log = logging.getLogger(\"SnapShooter\")\n\n    self.db = db\n    self.task_queue_writer = task_queue_writer\n    self.model_spec = platform_config.model_spec\n    self.entity_relation_attrs = defaultdict(dict)\n    for (entity, attr), _ in self.model_spec.relations.items():\n        self.entity_relation_attrs[entity][attr] = True\n    for entity in self.model_spec.entities:\n        self.entity_relation_attrs[entity][\"_id\"] = True\n\n    self.worker_index = platform_config.process_index\n    self.worker_cnt = platform_config.num_processes\n    self.config = SnapShooterConfig.model_validate(platform_config.config.get(\"snapshots\"))\n\n    self.elog = elog or DummyEventGroup()\n\n    self._timeseries_hooks = SnapshotTimeseriesHookContainer(self.log, self.model_spec, elog)\n    self._correlation_hooks = SnapshotCorrelationHookContainer(self.log, self.model_spec, elog)\n    self._init_hooks: list[Callable[[], list[DataPointTask]]] = []\n    self._finalize_hooks: list[Callable[[], list[DataPointTask]]] = []\n\n    queue = f\"{platform_config.app_name}-worker-{platform_config.process_index}-snapshots\"\n    self.snapshot_queue_reader = TaskQueueReader(\n        callback=self.process_snapshot_task,\n        parse_task=Snapshot.get_validator(self.model_spec),\n        app_name=platform_config.app_name,\n        worker_index=platform_config.process_index,\n        rabbit_config=platform_config.config.get(\"processing_core.msg_broker\", {}),\n        queue=queue,\n        priority_queue=False,\n        parent_logger=self.log,\n    )\n    scheduler.register(self.snapshot_queue_reader.watchdog, second=\"10,40\")\n\n    self.snapshot_entities = [\n        entity for entity, spec in self.model_spec.entities.items() if spec.snapshot\n    ]\n    self.log.info(\"Snapshots will be created for entities: %s\", self.snapshot_entities)\n    self.link_mirrors = defaultdict(dict)\n    for (entity, attr), spec in self.model_spec.relations.items():\n        if spec.is_relation and spec.is_mirrored:\n            self.link_mirrors[spec.relation_to][spec.mirror_as] = (entity, attr)\n    self.log.info(\"Link mirrors: %s\", self.link_mirrors)\n\n    # Get link cache\n    self.cache = self.db.get_module_cache(\"Link\")\n\n    if platform_config.process_index != 0:\n        self.log.debug(\n            \"Snapshot task creation will be disabled in this worker to avoid race conditions.\"\n        )\n        self.snapshot_queue_writer = None\n        return\n\n    self.snapshot_queue_writer = TaskQueueWriter(\n        platform_config.app_name,\n        platform_config.num_processes,\n        platform_config.config.get(\"processing_core.msg_broker\"),\n        f\"{platform_config.app_name}-main-snapshot-exchange\",\n        parent_logger=self.log,\n    )\n\n    # Schedule snapshot period\n    snapshot_cron = self.config.creation_rate.model_dump(exclude_none=True)\n    scheduler.register(self.make_snapshots, **snapshot_cron)\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Connect to RabbitMQ and start consuming from TaskQueue.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def start(self):\n    \"\"\"Connect to RabbitMQ and start consuming from TaskQueue.\"\"\"\n    self.log.info(\"Connecting to RabbitMQ\")\n    self.snapshot_queue_reader.connect()\n    self.snapshot_queue_reader.check()  # check presence of needed queues\n    if self.snapshot_queue_writer is not None:\n        self.snapshot_queue_writer.connect()\n        self.snapshot_queue_writer.check()  # check presence of needed exchanges\n\n    self.snapshot_queue_reader.start()\n    self.log.info(\"Following used links detected: %s\", self._correlation_hooks.used_links)\n    self.log.info(\"SnapShooter started.\")\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop consuming from TaskQueue, disconnect from RabbitMQ.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def stop(self):\n    \"\"\"Stop consuming from TaskQueue, disconnect from RabbitMQ.\"\"\"\n    self.snapshot_queue_reader.stop()\n\n    if self.snapshot_queue_writer is not None:\n        self.snapshot_queue_writer.disconnect()\n    self.snapshot_queue_reader.disconnect()\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.register_timeseries_hook","title":"register_timeseries_hook","text":"<pre><code>register_timeseries_hook(hook: Callable[[str, str, list[dict]], list[DataPointTask]], entity_type: str, attr_type: str)\n</code></pre> <p>Registers passed timeseries hook to be called during snapshot creation.</p> <p>Binds hook to specified <code>entity_type</code> and <code>attr_type</code> (though same hook can be bound multiple times).</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[str, str, list[dict]], list[DataPointTask]]</code> <p><code>hook</code> callable should expect entity_type, attr_type and attribute history as arguments and return a list of <code>DataPointTask</code> objects.</p> required <code>entity_type</code> <code>str</code> <p>specifies entity type</p> required <code>attr_type</code> <code>str</code> <p>specifies attribute type</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If entity_type and attr_type do not specify a valid timeseries attribute, a ValueError is raised.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def register_timeseries_hook(\n    self,\n    hook: Callable[[str, str, list[dict]], list[DataPointTask]],\n    entity_type: str,\n    attr_type: str,\n):\n    \"\"\"\n    Registers passed timeseries hook to be called during snapshot creation.\n\n    Binds hook to specified `entity_type` and `attr_type` (though same hook can be bound\n    multiple times).\n\n    Args:\n        hook: `hook` callable should expect entity_type, attr_type and attribute\n            history as arguments and return a list of `DataPointTask` objects.\n        entity_type: specifies entity type\n        attr_type: specifies attribute type\n\n    Raises:\n        ValueError: If entity_type and attr_type do not specify a valid timeseries attribute,\n            a ValueError is raised.\n    \"\"\"\n    self._timeseries_hooks.register(hook, entity_type, attr_type)\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.register_correlation_hook","title":"register_correlation_hook","text":"<pre><code>register_correlation_hook(hook: Callable[[str, dict], Union[None, list[DataPointTask]]], entity_type: str, depends_on: list[list[str]], may_change: list[list[str]])\n</code></pre> <p>Registers passed hook to be called during snapshot creation.</p> <p>Binds hook to specified entity_type (though same hook can be bound multiple times).</p> <p><code>entity_type</code> and attribute specifications are validated, <code>ValueError</code> is raised on failure.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[str, dict], Union[None, list[DataPointTask]]]</code> <p><code>hook</code> callable should expect entity type as str and its current values, including linked entities, as dict Can optionally return a list of DataPointTask objects to perform.</p> required <code>entity_type</code> <code>str</code> <p>specifies entity type</p> required <code>depends_on</code> <code>list[list[str]]</code> <p>each item should specify an attribute that is depended on in the form of a path from the specified entity_type to individual attributes (even on linked entities).</p> required <code>may_change</code> <code>list[list[str]]</code> <p>each item should specify an attribute that <code>hook</code> may change. specification format is identical to <code>depends_on</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>On failure of specification validation.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def register_correlation_hook(\n    self,\n    hook: Callable[[str, dict], Union[None, list[DataPointTask]]],\n    entity_type: str,\n    depends_on: list[list[str]],\n    may_change: list[list[str]],\n):\n    \"\"\"\n    Registers passed hook to be called during snapshot creation.\n\n    Binds hook to specified entity_type (though same hook can be bound multiple times).\n\n    `entity_type` and attribute specifications are validated, `ValueError` is raised on failure.\n\n    Args:\n        hook: `hook` callable should expect entity type as str\n            and its current values, including linked entities, as dict\n            Can optionally return a list of DataPointTask objects to perform.\n        entity_type: specifies entity type\n        depends_on: each item should specify an attribute that is depended on\n            in the form of a path from the specified entity_type to individual attributes\n            (even on linked entities).\n        may_change: each item should specify an attribute that `hook` may change.\n            specification format is identical to `depends_on`.\n\n    Raises:\n        ValueError: On failure of specification validation.\n    \"\"\"\n    self._correlation_hooks.register(hook, entity_type, depends_on, may_change)\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.register_run_init_hook","title":"register_run_init_hook","text":"<pre><code>register_run_init_hook(hook: Callable[[], list[DataPointTask]])\n</code></pre> <p>Registers passed hook to be called before a run of  snapshot creation begins.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[], list[DataPointTask]]</code> <p><code>hook</code> callable should expect no arguments and return a list of DataPointTask objects to perform.</p> required Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def register_run_init_hook(self, hook: Callable[[], list[DataPointTask]]):\n    \"\"\"\n    Registers passed hook to be called before a run of  snapshot creation begins.\n\n    Args:\n        hook: `hook` callable should expect no arguments and\n            return a list of DataPointTask objects to perform.\n    \"\"\"\n    self._init_hooks.append(hook)\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.register_run_finalize_hook","title":"register_run_finalize_hook","text":"<pre><code>register_run_finalize_hook(hook: Callable[[], list[DataPointTask]])\n</code></pre> <p>Registers passed hook to be called after a run of  snapshot creation ends.</p> <p>Parameters:</p> Name Type Description Default <code>hook</code> <code>Callable[[], list[DataPointTask]]</code> <p><code>hook</code> callable should expect no arguments and return a list of DataPointTask objects to perform.</p> required Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def register_run_finalize_hook(self, hook: Callable[[], list[DataPointTask]]):\n    \"\"\"\n    Registers passed hook to be called after a run of  snapshot creation ends.\n\n    Args:\n        hook: `hook` callable should expect no arguments and\n            return a list of DataPointTask objects to perform.\n    \"\"\"\n    self._finalize_hooks.append(hook)\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.make_snapshots","title":"make_snapshots","text":"<pre><code>make_snapshots()\n</code></pre> <p>Creates snapshots for all entities currently active in database.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def make_snapshots(self):\n    \"\"\"Creates snapshots for all entities currently active in database.\"\"\"\n    time = datetime.utcnow()\n    self.db.save_metadata(\n        time,\n        {\n            \"task_creation_start\": time,\n            \"entities\": 0,\n            \"components\": 0,\n            \"workers_finished\": 0,\n            \"linked_finished\": False,\n        },\n    )\n    # Broadcast run start\n    with entity_type_context(self.model_spec):\n        self.snapshot_queue_writer.broadcast_task(\n            task=Snapshot(type=SnapshotMessageType.run_start, time=time)\n        )\n\n    # distribute list of possibly linked entities to all workers\n    cached = self.get_cached_link_entity_ids()\n    self.log.debug(\"Broadcasting %s cached linked entities\", len(cached))\n    with entity_type_context(self.model_spec):\n        self.snapshot_queue_writer.broadcast_task(\n            task=Snapshot(entities=cached, time=time, type=SnapshotMessageType.linked_entities)\n        )\n\n    # Load links only for a reduced set of entities\n    self.log.debug(\"Loading linked entities.\")\n\n    times = {}\n    counts = {\"entities\": 0, \"components\": 0}\n    try:\n        linked_entities = self.get_linked_entities(time, cached)\n        times[\"components_loaded\"] = datetime.utcnow()\n\n        for i, linked_entities_component in enumerate(linked_entities):\n            counts[\"entities\"] += len(linked_entities_component)\n            counts[\"components\"] += 1\n\n            with entity_type_context(self.model_spec):\n                self.snapshot_queue_writer.put_task(\n                    task=Snapshot(\n                        entities=linked_entities_component,\n                        time=time,\n                        type=SnapshotMessageType.task,\n                        final=(i + 1 == len(linked_entities)),\n                    )\n                )\n\n        if len(linked_entities) == 0:\n            self.db.update_metadata(time, metadata={\"linked_finished\": True})\n    except pymongo.errors.CursorNotFound as err:\n        self.log.exception(err)\n    finally:\n        times[\"task_creation_end\"] = datetime.utcnow()\n        self.db.update_metadata(\n            time,\n            metadata=times,\n            increase=counts,\n        )\n\n    # Broadcast run end\n    with entity_type_context(self.model_spec):\n        self.snapshot_queue_writer.broadcast_task(\n            task=Snapshot(type=SnapshotMessageType.run_end, time=time)\n        )\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.get_linked_entities","title":"get_linked_entities","text":"<pre><code>get_linked_entities(time: datetime, cached_linked_entities: list[tuple[str, str]])\n</code></pre> <p>Get weakly connected components from entity graph.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def get_linked_entities(self, time: datetime, cached_linked_entities: list[tuple[str, str]]):\n    \"\"\"Get weakly connected components from entity graph.\"\"\"\n    visited_entities = set()\n    entity_to_component = {}\n    linked_components = []\n    for etype, eid in cached_linked_entities:\n        typed_eid = self.model_spec.parse_eid(etype, eid)\n        master_record = self.db.get_master_record(\n            etype, typed_eid, projection=self.entity_relation_attrs[etype]\n        ) or {\"_id\": typed_eid}\n\n        if not self.config.keep_empty and len(master_record) == 1:\n            continue\n\n        if (etype, master_record[\"_id\"]) not in visited_entities:\n            # Get entities linked by current entity\n            current_values = self.get_values_at_time(etype, master_record, time)\n            linked_entities = self.load_linked_entity_ids(etype, current_values, time)\n\n            # Set linked as visited\n            visited_entities.update(linked_entities)\n\n            # Update component, take all connected components into account\n            have_component = linked_entities &amp; set(entity_to_component.keys())\n            if have_component:\n                for entity in have_component:\n                    component = entity_to_component[entity]\n                    linked_entities.update(component)\n            entity_to_component.update({entity: linked_entities for entity in linked_entities})\n\n    # Make a list of unique components\n    visited_entities.clear()\n    for entity, component in entity_to_component.items():\n        if entity in visited_entities:\n            continue\n        visited_entities.update(component)\n        linked_components.append(component)\n\n    return linked_components\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.process_snapshot_task","title":"process_snapshot_task","text":"<pre><code>process_snapshot_task(msg_id, task: Snapshot)\n</code></pre> <p>Acknowledges the received message and makes a snapshot according to the <code>task</code>.</p> <p>This function should not be called directly, but set as callback for TaskQueueReader.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def process_snapshot_task(self, msg_id, task: Snapshot):\n    \"\"\"\n    Acknowledges the received message and makes a snapshot according to the `task`.\n\n    This function should not be called directly, but set as callback for TaskQueueReader.\n    \"\"\"\n    if not self.snapshot_queue_reader.ack(msg_id):\n        self.log.warning(\"Acking message %s failed, will not process.\", msg_id)\n        return\n    if task.type == SnapshotMessageType.task:\n        self.make_snapshot(task)\n    elif task.type == SnapshotMessageType.linked_entities:\n        self.make_snapshots_by_hash(task)\n    elif task.type == SnapshotMessageType.run_start:\n        self.log.debug(\"Run start, running init hooks\")\n        self._run_hooks(self._init_hooks)\n    elif task.type == SnapshotMessageType.run_end:\n        self.log.debug(\"Run end, running finalize hooks\")\n        self._run_hooks(self._finalize_hooks)\n    else:\n        raise ValueError(\"Unknown SnapshotMessageType.\")\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.make_snapshots_by_hash","title":"make_snapshots_by_hash","text":"<pre><code>make_snapshots_by_hash(task: Snapshot)\n</code></pre> <p>Make snapshots for all entities with routing key belonging to this worker.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def make_snapshots_by_hash(self, task: Snapshot):\n    \"\"\"\n    Make snapshots for all entities with routing key belonging to this worker.\n    \"\"\"\n    self.log.debug(\"Creating snapshots for worker portion by hash.\")\n    have_links = set(task.entities)\n    entity_cnt = 0\n    for etype in self.snapshot_entities:\n        records_cursor = self.db.get_worker_master_records(\n            self.worker_index, self.worker_cnt, etype, no_cursor_timeout=True\n        )\n        for attempt in range(RETRY_COUNT):\n            try:\n                entity_cnt += self.make_linkless_snapshots(\n                    etype, records_cursor, task.time, have_links\n                )\n            except Exception as err:\n                self.log.exception(\"Uncaught exception while creating snapshots: %s\", err)\n                if attempt &lt; RETRY_COUNT - 1:\n                    self.log.info(\"Retrying snapshot creation for '%s' due to errors.\", etype)\n                continue\n            finally:\n                records_cursor.close()\n            break\n        else:\n            self.log.error(\n                \"Failed to create snapshots for '%s' after %s attempts.\", etype, attempt + 1\n            )\n    self.db.update_metadata(\n        task.time,\n        metadata={},\n        increase={\"entities\": entity_cnt, \"components\": entity_cnt, \"workers_finished\": 1},\n        worker_id=0,\n    )\n    self.log.debug(\"Worker snapshot creation done.\")\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.make_linkless_snapshot","title":"make_linkless_snapshot","text":"<pre><code>make_linkless_snapshot(entity_type: str, master_record: dict, time: datetime)\n</code></pre> <p>Make a snapshot for given entity <code>master_record</code> and <code>time</code>.</p> <p>Runs timeseries and correlation hooks. The resulting snapshot is saved into DB.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def make_linkless_snapshot(self, entity_type: str, master_record: dict, time: datetime):\n    \"\"\"\n    Make a snapshot for given entity `master_record` and `time`.\n\n    Runs timeseries and correlation hooks.\n    The resulting snapshot is saved into DB.\n    \"\"\"\n    self.run_timeseries_processing(entity_type, master_record)\n    values = self.get_values_at_time(entity_type, master_record, time)\n    self.add_mirrored_links(entity_type, values)\n    entity_values = {(entity_type, master_record[\"_id\"]): values}\n\n    tasks = self._correlation_hooks.run(entity_values)\n    for task in tasks:\n        self.task_queue_writer.put_task(task)\n\n    assert len(entity_values) == 1, \"Expected a single entity.\"\n    for record in entity_values.values():\n        return record\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.add_mirrored_links","title":"add_mirrored_links","text":"<pre><code>add_mirrored_links(entity_type: str, values: dict)\n</code></pre> <p>This function adds mirrored links to the dict with current values of an entity.</p> <p>The links are added in the same format as normal links, i.e. as a list of dicts.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def add_mirrored_links(self, entity_type: str, values: dict):\n    \"\"\"\n    This function adds mirrored links to the dict with current values of an entity.\n\n    The links are added in the same format as normal links, i.e. as a list of dicts.\n    \"\"\"\n    if entity_type in self.link_mirrors:\n        for mirror_name, (etype, attr) in self.link_mirrors[entity_type].items():\n            link_sources = self.cache.aggregate(\n                [\n                    {\n                        \"$match\": {\n                            \"to\": f\"{entity_type}#{values['eid']}\",\n                            \"using_attr\": f\"{etype}#{attr}\",\n                        }\n                    },\n                    {\"$project\": {\"from\": 1}},\n                ]\n            )\n            parsed = parse_eids_from_cache(\n                self.model_spec, [doc[\"from\"] for doc in link_sources]\n            )\n            values[mirror_name] = [{\"eid\": eid} for eid in parsed]\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.make_snapshot","title":"make_snapshot","text":"<pre><code>make_snapshot(task: Snapshot)\n</code></pre> <p>Make a snapshot for entities and time specified by <code>task</code>.</p> <p>Runs timeseries and correlation hooks. The resulting snapshots are saved into DB.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def make_snapshot(self, task: Snapshot):\n    \"\"\"\n    Make a snapshot for entities and time specified by `task`.\n\n    Runs timeseries and correlation hooks.\n    The resulting snapshots are saved into DB.\n    \"\"\"\n    entity_values = {}\n    for entity_type, entity_id in task.entities:\n        record = self.db.get_master_record(entity_type, entity_id) or {\"_id\": entity_id}\n        if not self.config.keep_empty and len(record) == 1:\n            continue\n\n        self.run_timeseries_processing(entity_type, record)\n        values = self.get_values_at_time(entity_type, record, task.time)\n        self.add_mirrored_links(entity_type, values)\n        entity_values[entity_type, entity_id] = values\n\n    self.link_loaded_entities(entity_values)\n    created_tasks = self._correlation_hooks.run(entity_values)\n    for created_task in created_tasks:\n        self.task_queue_writer.put_task(created_task)\n\n    # unlink entities again\n    for (rtype, _rid), record in entity_values.items():\n        for attr, value in record.items():\n            if (rtype, attr) not in self.model_spec.relations:\n                continue\n            spec = self.model_spec.relations[rtype, attr]\n            if spec.t == AttrType.OBSERVATIONS and spec.multi_value:\n                for val in value:\n                    self._remove_record_from_value(spec, val)\n            else:\n                self._remove_record_from_value(spec, value)\n\n    for (rtype, _rid), record in entity_values.items():\n        if len(record) == 1 and not self.config.keep_empty:\n            continue\n        self.db.snapshots.save_one(rtype, record, task.time)\n\n    if task.final:\n        self.db.update_metadata(task.time, metadata={\"linked_finished\": True}, worker_id=0)\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.run_timeseries_processing","title":"run_timeseries_processing","text":"<pre><code>run_timeseries_processing(entity_type, master_record)\n</code></pre> <ul> <li>all registered timeseries processing modules must be called</li> <li>this should result in <code>observations</code> or <code>plain</code> datapoints, which will be saved to db     and forwarded in processing</li> </ul> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def run_timeseries_processing(self, entity_type, master_record):\n    \"\"\"\n    - all registered timeseries processing modules must be called\n      - this should result in `observations` or `plain` datapoints, which will be saved to db\n        and forwarded in processing\n    \"\"\"\n    tasks = []\n    for attr, attr_spec in self.model_spec.entity_attributes[entity_type].items():\n        if attr_spec.t == AttrType.TIMESERIES and attr in master_record:\n            new_tasks = self._timeseries_hooks.run(entity_type, attr, master_record[attr])\n            tasks.extend(new_tasks)\n\n    self.extend_master_record(entity_type, master_record, tasks)\n    for task in tasks:\n        self.task_queue_writer.put_task(task)\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.extend_master_record","title":"extend_master_record  <code>staticmethod</code>","text":"<pre><code>extend_master_record(etype, master_record, new_tasks: list[DataPointTask])\n</code></pre> <p>Update existing master record with datapoints from new tasks</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>@staticmethod\ndef extend_master_record(etype, master_record, new_tasks: list[DataPointTask]):\n    \"\"\"Update existing master record with datapoints from new tasks\"\"\"\n    for task in new_tasks:\n        for datapoint in task.data_points:\n            if datapoint.etype != etype:\n                continue\n            dp_dict = datapoint.dict(include={\"v\", \"t1\", \"t2\", \"c\"})\n            if datapoint.attr in master_record:\n                master_record[datapoint.attr].append()\n            else:\n                master_record[datapoint.attr] = [dp_dict]\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.load_linked_entity_ids","title":"load_linked_entity_ids","text":"<pre><code>load_linked_entity_ids(entity_type: str, current_values: dict, time: datetime)\n</code></pre> <p>Loads the subgraph of entities linked to the current entity, returns a list of their types and ids.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def load_linked_entity_ids(self, entity_type: str, current_values: dict, time: datetime):\n    \"\"\"\n    Loads the subgraph of entities linked to the current entity,\n    returns a list of their types and ids.\n    \"\"\"\n    loaded_entity_ids = {(entity_type, current_values[\"eid\"])}\n    linked_entity_ids_to_process = (\n        self.get_linked_entity_ids(entity_type, current_values) - loaded_entity_ids\n    )\n\n    while linked_entity_ids_to_process:\n        entity_identifiers = linked_entity_ids_to_process.pop()\n        linked_etype, linked_eid = entity_identifiers\n        relevant_attributes = self.entity_relation_attrs[linked_etype]\n        record = self.db.get_master_record(\n            linked_etype, linked_eid, projection=relevant_attributes\n        ) or {\"_id\": linked_eid}\n        linked_values = self.get_values_at_time(linked_etype, record, time)\n\n        linked_entity_ids_to_process.update(\n            self.get_linked_entity_ids(entity_type, linked_values) - set(loaded_entity_ids)\n        )\n        loaded_entity_ids.add((linked_etype, linked_eid))\n\n    return loaded_entity_ids\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.get_linked_entity_ids","title":"get_linked_entity_ids","text":"<pre><code>get_linked_entity_ids(entity_type: str, current_values: dict) -&gt; set[tuple[str, str]]\n</code></pre> <p>Returns a set of tuples (entity_type, entity_id) identifying entities linked by <code>current_values</code>.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def get_linked_entity_ids(self, entity_type: str, current_values: dict) -&gt; set[tuple[str, str]]:\n    \"\"\"\n    Returns a set of tuples (entity_type, entity_id) identifying entities linked by\n    `current_values`.\n    \"\"\"\n    related_entity_ids = set()\n    for attr, val in current_values.items():\n        if (entity_type, attr) not in self._correlation_hooks.used_links:\n            continue\n        attr_spec = self.model_spec.relations[entity_type, attr]\n        if attr_spec.t == AttrType.OBSERVATIONS and attr_spec.multi_value:\n            for v in val:\n                related_entity_ids.update(self._get_link_entity_ids(attr_spec, v))\n        else:\n            related_entity_ids.update(self._get_link_entity_ids(attr_spec, val))\n    return related_entity_ids\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.get_value_at_time","title":"get_value_at_time","text":"<pre><code>get_value_at_time(attr_spec: AttrSpecObservations, attr_history, time: datetime) -&gt; tuple[Any, float]\n</code></pre> <p>Get current value of an attribute from its history. Assumes <code>multi_value = False</code>.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def get_value_at_time(\n    self, attr_spec: AttrSpecObservations, attr_history, time: datetime\n) -&gt; tuple[Any, float]:\n    \"\"\"Get current value of an attribute from its history. Assumes `multi_value = False`.\"\"\"\n    return max(\n        (\n            (point[\"v\"], self.extrapolate_confidence(point, time, attr_spec.history_params))\n            for point in attr_history\n        ),\n        key=lambda val_conf: val_conf[1],\n        default=(None, 0.0),\n    )\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.get_multi_value_at_time","title":"get_multi_value_at_time","text":"<pre><code>get_multi_value_at_time(attr_spec: AttrSpecObservations, attr_history, time: datetime) -&gt; tuple[list, list[float]]\n</code></pre> <p>Get current value of a multi_value attribute from its history.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>def get_multi_value_at_time(\n    self, attr_spec: AttrSpecObservations, attr_history, time: datetime\n) -&gt; tuple[list, list[float]]:\n    \"\"\"Get current value of a multi_value attribute from its history.\"\"\"\n    if attr_spec.data_type.hashable:\n        values_with_confidence = defaultdict(float)\n        for point in attr_history:\n            value = point[\"v\"]\n            confidence = self.extrapolate_confidence(point, time, attr_spec.history_params)\n            if confidence &gt; 0.0 and values_with_confidence[value] &lt; confidence:\n                values_with_confidence[value] = confidence\n        return list(values_with_confidence.keys()), list(values_with_confidence.values())\n    else:\n        values = []\n        confidence_list = []\n        for point in attr_history:\n            value = point[\"v\"]\n            confidence = self.extrapolate_confidence(point, time, attr_spec.history_params)\n            if value in values:\n                i = values.index(value)\n                confidence_list[i] = max(confidence_list[i], confidence)\n            elif confidence &gt; 0.0:\n                values.append(value)\n                confidence_list.append(confidence)\n        return values, confidence_list\n</code></pre>"},{"location":"reference/snapshots/snapshooter/#dp3.snapshots.snapshooter.SnapShooter.extrapolate_confidence","title":"extrapolate_confidence  <code>staticmethod</code>","text":"<pre><code>extrapolate_confidence(datapoint: dict, time: datetime, history_params: ObservationsHistoryParams) -&gt; float\n</code></pre> <p>Get the confidence value at given time.</p> Source code in <code>dp3/snapshots/snapshooter.py</code> <pre><code>@staticmethod\ndef extrapolate_confidence(\n    datapoint: dict, time: datetime, history_params: ObservationsHistoryParams\n) -&gt; float:\n    \"\"\"Get the confidence value at given time.\"\"\"\n    t1 = datapoint[\"t1\"]\n    t2 = datapoint[\"t2\"]\n    base_confidence = datapoint[\"c\"]\n\n    if time &lt; t1:\n        if time &lt;= t1 - history_params.pre_validity:\n            return 0.0\n        return base_confidence * (1 - (t1 - time) / history_params.pre_validity)\n    if time &lt;= t2:\n        return base_confidence  # completely inside the (strict) interval\n    if time &gt;= t2 + history_params.post_validity:\n        return 0.0\n    return base_confidence * (1 - (time - t2) / history_params.post_validity)\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/","title":"snapshot_hooks","text":""},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks","title":"dp3.snapshots.snapshot_hooks","text":"<p>Module managing registered hooks and their dependencies on one another.</p>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.SnapshotTimeseriesHookContainer","title":"SnapshotTimeseriesHookContainer","text":"<pre><code>SnapshotTimeseriesHookContainer(log: Logger, model_spec: ModelSpec, elog: EventGroupType)\n</code></pre> <p>Container for timeseries analysis hooks</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def __init__(self, log: logging.Logger, model_spec: ModelSpec, elog: EventGroupType):\n    self.log = log.getChild(\"TimeseriesHooks\")\n    self.elog = elog\n    self.model_spec = model_spec\n\n    self._hooks = defaultdict(list)\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.SnapshotTimeseriesHookContainer.register","title":"register","text":"<pre><code>register(hook: Callable[[str, str, list[dict]], list[DataPointTask]], entity_type: str, attr_type: str)\n</code></pre> <p>Registers passed timeseries hook to be called during snapshot creation.</p> <p>Binds hook to specified entity_type and attr_type (though same hook can be bound multiple times). If entity_type and attr_type do not specify a valid timeseries attribute, a ValueError is raised. Args:     hook: <code>hook</code> callable should expect entity_type, attr_type and attribute         history as arguments and return a list of <code>Task</code> objects.     entity_type: specifies entity type     attr_type: specifies attribute type</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def register(\n    self,\n    hook: Callable[[str, str, list[dict]], list[DataPointTask]],\n    entity_type: str,\n    attr_type: str,\n):\n    \"\"\"\n    Registers passed timeseries hook to be called during snapshot creation.\n\n    Binds hook to specified entity_type and attr_type (though same hook can be bound\n    multiple times).\n    If entity_type and attr_type do not specify a valid timeseries attribute,\n    a ValueError is raised.\n    Args:\n        hook: `hook` callable should expect entity_type, attr_type and attribute\n            history as arguments and return a list of `Task` objects.\n        entity_type: specifies entity type\n        attr_type: specifies attribute type\n    \"\"\"\n    if (entity_type, attr_type) not in self.model_spec.attributes:\n        raise ValueError(f\"Attribute '{attr_type}' of entity '{entity_type}' does not exist.\")\n    spec = self.model_spec.attributes[entity_type, attr_type]\n    if spec.t != AttrType.TIMESERIES:\n        raise ValueError(f\"'{entity_type}.{attr_type}' is not a timeseries, but '{spec.t}'\")\n    self._hooks[entity_type, attr_type].append(hook)\n    self.log.debug(f\"Added hook: '{get_func_name(hook)}'\")\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.SnapshotTimeseriesHookContainer.run","title":"run","text":"<pre><code>run(entity_type: str, attr_type: str, attr_history: list[dict]) -&gt; list[DataPointTask]\n</code></pre> <p>Runs registered hooks.</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def run(\n    self, entity_type: str, attr_type: str, attr_history: list[dict]\n) -&gt; list[DataPointTask]:\n    \"\"\"Runs registered hooks.\"\"\"\n    tasks = []\n    with task_context(self.model_spec):\n        for hook in self._hooks[entity_type, attr_type]:\n            try:\n                new_tasks = hook(entity_type, attr_type, attr_history)\n                tasks.extend(new_tasks)\n            except Exception as e:\n                self.elog.log(\"module_error\")\n                self.log.error(f\"Error during running hook {hook}: {e}\")\n    return tasks\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.SnapshotCorrelationHookContainer","title":"SnapshotCorrelationHookContainer","text":"<pre><code>SnapshotCorrelationHookContainer(log: Logger, model_spec: ModelSpec, elog: EventGroupType)\n</code></pre> <p>Container for data fusion and correlation hooks.</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def __init__(self, log: logging.Logger, model_spec: ModelSpec, elog: EventGroupType):\n    self.log = log.getChild(\"CorrelationHooks\")\n    self.elog = elog\n    self.model_spec = model_spec\n\n    self._hooks: defaultdict[str, list[tuple[str, Callable]]] = defaultdict(list)\n    self._short_hook_ids: dict = {}\n\n    self._dependency_graph = DependencyGraph(self.log)\n    self.used_links = set()\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.SnapshotCorrelationHookContainer.register","title":"register","text":"<pre><code>register(hook: Callable[[str, dict], Union[None, list[DataPointTask]]], entity_type: str, depends_on: list[list[str]], may_change: list[list[str]]) -&gt; str\n</code></pre> <p>Registers passed hook to be called during snapshot creation.</p> <p>Binds hook to specified entity_type (though same hook can be bound multiple times).</p> <p>If entity_type and attribute specifications are validated and ValueError is raised on failure. Args:     hook: <code>hook</code> callable should expect entity type as str         and its current values, including linked entities, as dict.         Can optionally return a list of DataPointTask objects to perform.     entity_type: specifies entity type     depends_on: each item should specify an attribute that is depended on         in the form of a path from the specified entity_type to individual attributes         (even on linked entities).     may_change: each item should specify an attribute that <code>hook</code> may change.         specification format is identical to <code>depends_on</code>. Returns:     Generated hook id.</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def register(\n    self,\n    hook: Callable[[str, dict], Union[None, list[DataPointTask]]],\n    entity_type: str,\n    depends_on: list[list[str]],\n    may_change: list[list[str]],\n) -&gt; str:\n    \"\"\"\n    Registers passed hook to be called during snapshot creation.\n\n    Binds hook to specified entity_type (though same hook can be bound multiple times).\n\n    If entity_type and attribute specifications are validated\n    and ValueError is raised on failure.\n    Args:\n        hook: `hook` callable should expect entity type as str\n            and its current values, including linked entities, as dict.\n            Can optionally return a list of DataPointTask objects to perform.\n        entity_type: specifies entity type\n        depends_on: each item should specify an attribute that is depended on\n            in the form of a path from the specified entity_type to individual attributes\n            (even on linked entities).\n        may_change: each item should specify an attribute that `hook` may change.\n            specification format is identical to `depends_on`.\n    Returns:\n        Generated hook id.\n    \"\"\"\n\n    if entity_type not in self.model_spec.entities:\n        raise ValueError(f\"Entity '{entity_type}' does not exist.\")\n\n    self.used_links |= self._validate_attr_paths(entity_type, depends_on)\n    self.used_links |= self._validate_attr_paths(entity_type, may_change)\n\n    depends_on = self._get_attr_path_destinations(entity_type, depends_on)\n    may_change = self._get_attr_path_destinations(entity_type, may_change)\n\n    hook_args = f\"({entity_type}, [{','.join(depends_on)}], [{','.join(may_change)}])\"\n    hook_id = f\"{get_func_name(hook)}{hook_args}\"\n    self._short_hook_ids[hook_id] = hook_args\n    self._dependency_graph.add_hook_dependency(hook_id, depends_on, may_change)\n\n    self._hooks[entity_type].append((hook_id, hook))\n    self._restore_hook_order(self._hooks[entity_type])\n\n    self.log.info(f\"Added hook: '{hook_id}'\")\n    return hook_id\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.SnapshotCorrelationHookContainer.run","title":"run","text":"<pre><code>run(entities: dict) -&gt; list[DataPointTask]\n</code></pre> <p>Runs registered hooks.</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def run(self, entities: dict) -&gt; list[DataPointTask]:\n    \"\"\"Runs registered hooks.\"\"\"\n    entity_types = {etype for etype, _ in entities}\n    hook_subset = [\n        (hook_id, hook, etype) for etype in entity_types for hook_id, hook in self._hooks[etype]\n    ]\n    topological_order = self._dependency_graph.topological_order\n    hook_subset.sort(key=lambda x: topological_order.index(x[0]))\n    entities_by_etype = defaultdict(dict)\n    for (etype, eid), values in entities.items():\n        entities_by_etype[etype][eid] = values\n\n    created_tasks = []\n\n    with task_context(self.model_spec):\n        for hook_id, hook, etype in hook_subset:\n            short_id = hook_id if len(hook_id) &lt; 160 else self._short_hook_ids[hook_id]\n            for eid, entity_values in entities_by_etype[etype].items():\n                self.log.debug(\"Running hook %s on entity %s\", short_id, eid)\n                try:\n                    tasks = hook(etype, entity_values)\n                    if tasks is not None and tasks:\n                        created_tasks.extend(tasks)\n                except Exception as e:\n                    self.elog.log(\"module_error\")\n                    self.log.error(f\"Error during running hook {hook_id}: {e}\")\n                    self.log.exception(e)\n\n    return created_tasks\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.GraphVertex","title":"GraphVertex  <code>dataclass</code>","text":"<pre><code>GraphVertex(adj: list = list(), in_degree: int = 0, type: str = 'attr')\n</code></pre> <p>Vertex in a graph of dependencies</p>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.DependencyGraph","title":"DependencyGraph","text":"<pre><code>DependencyGraph(log)\n</code></pre> <p>Class representing a graph of dependencies between correlation hooks.</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def __init__(self, log):\n    self.log = log.getChild(\"DependencyGraph\")\n\n    # dictionary of adjacency lists for each edge\n    self._vertices = defaultdict(GraphVertex)\n    self.topological_order = []\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.DependencyGraph.add_hook_dependency","title":"add_hook_dependency","text":"<pre><code>add_hook_dependency(hook_id: str, depends_on: list[str], may_change: list[str])\n</code></pre> <p>Add hook to dependency graph and recalculate if any cycles are created.</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def add_hook_dependency(self, hook_id: str, depends_on: list[str], may_change: list[str]):\n    \"\"\"Add hook to dependency graph and recalculate if any cycles are created.\"\"\"\n    if hook_id in self._vertices:\n        raise ValueError(f\"Hook id '{hook_id}' already present in the vertices.\")\n    for path in depends_on:\n        self.add_edge(path, hook_id)\n    for path in may_change:\n        self.add_edge(hook_id, path)\n    self._vertices[hook_id].type = \"hook\"\n    try:\n        self.topological_sort()\n    except ValueError as err:\n        raise ValueError(f\"Hook {hook_id} introduces a circular dependency.\") from err\n    self.check_multiple_writes()\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.DependencyGraph.add_edge","title":"add_edge","text":"<pre><code>add_edge(id_from: Hashable, id_to: Hashable)\n</code></pre> <p>Add oriented edge between specified vertices.</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def add_edge(self, id_from: Hashable, id_to: Hashable):\n    \"\"\"Add oriented edge between specified vertices.\"\"\"\n    self._vertices[id_from].adj.append(id_to)\n    # Ensure vertex with 'id_to' exists to avoid iteration errors later.\n    _ = self._vertices[id_to]\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.DependencyGraph.calculate_in_degrees","title":"calculate_in_degrees","text":"<pre><code>calculate_in_degrees()\n</code></pre> <p>Calculate number of incoming edges for each vertex. Time complexity O(V + E).</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def calculate_in_degrees(self):\n    \"\"\"Calculate number of incoming edges for each vertex. Time complexity O(V + E).\"\"\"\n    for vertex_node in self._vertices.values():\n        vertex_node.in_degree = 0\n\n    for vertex_node in self._vertices.values():\n        for adjacent_name in vertex_node.adj:\n            self._vertices[adjacent_name].in_degree += 1\n</code></pre>"},{"location":"reference/snapshots/snapshot_hooks/#dp3.snapshots.snapshot_hooks.DependencyGraph.topological_sort","title":"topological_sort","text":"<pre><code>topological_sort()\n</code></pre> <p>Implementation of Kahn's algorithm for topological sorting. Raises ValueError if there is a cycle in the graph.</p> <p>See https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm</p> Source code in <code>dp3/snapshots/snapshot_hooks.py</code> <pre><code>def topological_sort(self):\n    \"\"\"\n    Implementation of Kahn's algorithm for topological sorting.\n    Raises ValueError if there is a cycle in the graph.\n\n    See https://en.wikipedia.org/wiki/Topological_sorting#Kahn's_algorithm\n    \"\"\"\n    self.calculate_in_degrees()\n    queue = [(node_id, node) for node_id, node in self._vertices.items() if node.in_degree == 0]\n    topological_order = []\n    processed_vertices_cnt = 0\n\n    while queue:\n        curr_node_id, curr_node = queue.pop(0)\n        topological_order.append(curr_node_id)\n\n        # Decrease neighbouring nodes' in-degree by 1\n        for neighbor in curr_node.adj:\n            neighbor_node = self._vertices[neighbor]\n            neighbor_node.in_degree -= 1\n            # If in-degree becomes zero, add it to queue\n            if neighbor_node.in_degree == 0:\n                queue.append((neighbor, neighbor_node))\n\n        processed_vertices_cnt += 1\n\n    if processed_vertices_cnt != len(self._vertices):\n        raise ValueError(\"Dependency graph contains a cycle.\")\n    else:\n        self.topological_order = topological_order\n        return topological_order\n</code></pre>"},{"location":"reference/task_processing/","title":"task_processing","text":""},{"location":"reference/task_processing/#dp3.task_processing","title":"dp3.task_processing","text":"<p>Module responsible for task distribution, processing and running configured hooks. Task distribution is possible due to the task queue.</p>"},{"location":"reference/task_processing/task_distributor/","title":"task_distributor","text":""},{"location":"reference/task_processing/task_distributor/#dp3.task_processing.task_distributor","title":"dp3.task_processing.task_distributor","text":""},{"location":"reference/task_processing/task_distributor/#dp3.task_processing.task_distributor.TaskDistributor","title":"TaskDistributor","text":"<pre><code>TaskDistributor(task_executor: TaskExecutor, platform_config: PlatformConfig, registrar: CallbackRegistrar, daemon_stop_lock: Lock)\n</code></pre> <p>TaskDistributor uses task queues to distribute tasks between all running processes.</p> <p>Tasks are assigned to worker processes based on hash of entity key, so each entity is always processed by the same worker. Therefore, all requests modifying a particular entity are done sequentially and no locking is necessary.</p> <p>Tasks that are assigned to the current process are passed to <code>task_executor</code> for execution.</p> <p>Parameters:</p> Name Type Description Default <code>platform_config</code> <code>PlatformConfig</code> <p>Platform config</p> required <code>task_executor</code> <code>TaskExecutor</code> <p>Instance of TaskExecutor</p> required <code>registrar</code> <code>CallbackRegistrar</code> <p>Interface for callback registration</p> required <code>daemon_stop_lock</code> <code>Lock</code> <p>Lock used to control when the program stops. (see dp3.worker)</p> required Source code in <code>dp3/task_processing/task_distributor.py</code> <pre><code>def __init__(\n    self,\n    task_executor: TaskExecutor,\n    platform_config: PlatformConfig,\n    registrar: CallbackRegistrar,\n    daemon_stop_lock: threading.Lock,\n) -&gt; None:\n    assert (\n        0 &lt;= platform_config.process_index &lt; platform_config.num_processes\n    ), \"process index must be smaller than number of processes\"\n\n    self.log = logging.getLogger(\"TaskDistributor\")\n\n    self.process_index = platform_config.process_index\n    self.num_processes = platform_config.num_processes\n    self.model_spec = platform_config.model_spec\n    self.daemon_stop_lock = daemon_stop_lock\n\n    self.rabbit_params = platform_config.config.get(\"processing_core.msg_broker\", {})\n\n    self.entity_types = list(\n        platform_config.config.get(\"db_entities\").keys()\n    )  # List of configured entity types\n\n    self.running = False\n\n    # List of worker threads for processing the update requests\n    self._worker_threads = []\n    self.num_threads = platform_config.config.get(\"processing_core.worker_threads\", 8)\n\n    # Internal queues for each worker\n    self._queues = [queue.Queue(10) for _ in range(self.num_threads)]\n\n    # Connections to main task queue\n    # Reader - reads tasks from a pair of queues (one pair per process)\n    # and distributes them to worker threads\n    self._task_queue_reader = TaskQueueReader(\n        callback=self._distribute_task,\n        parse_task=partial(parse_data_point_task, model_spec=self.model_spec),\n        app_name=platform_config.app_name,\n        worker_index=self.process_index,\n        rabbit_config=self.rabbit_params,\n    )\n    registrar.scheduler_register(self._task_queue_reader.watchdog, second=\"20,50\")\n\n    # Writer - allows modules to write new tasks\n    self._task_queue_writer = TaskQueueWriter(\n        platform_config.app_name, self.num_processes, self.rabbit_params\n    )\n    self.task_executor = task_executor\n    # Object to store thread-local data (e.g. worker-thread index)\n    # (each thread sees different object contents)\n    self._current_thread_data = threading.local()\n\n    # Number of restarts of threads by watchdog\n    self._watchdog_restarts = 0\n    # Register watchdog to scheduler\n    registrar.scheduler_register(self._watchdog, second=\"*/30\")\n</code></pre>"},{"location":"reference/task_processing/task_distributor/#dp3.task_processing.task_distributor.TaskDistributor.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Run the worker threads and start consuming from TaskQueue.</p> Source code in <code>dp3/task_processing/task_distributor.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Run the worker threads and start consuming from TaskQueue.\"\"\"\n    self.log.info(\"Connecting to RabbitMQ\")\n    self._task_queue_reader.connect()\n    self._task_queue_reader.check()  # check presence of needed queues\n    self._task_queue_writer.connect()\n    self._task_queue_writer.check()  # check presence of needed exchanges\n\n    self.log.info(f\"Starting {self.num_threads} worker threads\")\n    self.running = True\n    self._worker_threads = [\n        threading.Thread(\n            target=self._worker_func, args=(i,), name=f\"Worker-{self.process_index}-{i}\"\n        )\n        for i in range(self.num_threads)\n    ]\n    for worker in self._worker_threads:\n        worker.start()\n\n    self.log.info(\"Starting consuming tasks from main queue\")\n    self._task_queue_reader.start()\n</code></pre>"},{"location":"reference/task_processing/task_distributor/#dp3.task_processing.task_distributor.TaskDistributor.stop","title":"stop","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stop the worker threads.</p> Source code in <code>dp3/task_processing/task_distributor.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop the worker threads.\"\"\"\n    self.log.info(\"Waiting for worker threads to finish their current tasks ...\")\n    # Thread for printing debug messages about worker status\n    threading.Thread(target=self._dbg_worker_status_print, daemon=True).start()\n\n    # Stop receiving new tasks from global queue\n    self._task_queue_reader.stop()\n\n    # Signalize stop to worker threads\n    self.running = False\n\n    # Wait until all workers stopped\n    for worker in self._worker_threads:\n        worker.join()\n\n    self._task_queue_reader.disconnect()\n    self._task_queue_writer.disconnect()\n\n    # Cleanup\n    self._worker_threads = []\n</code></pre>"},{"location":"reference/task_processing/task_distributor/#dp3.task_processing.task_distributor.TaskDistributor.push_new_tasks","title":"push_new_tasks","text":"<pre><code>push_new_tasks(new_tasks)\n</code></pre> <p>Push new tasks (resulting from hooks) to the priority queue.</p> <p>(priority queue is not limited in size, so put_task() never blocks; the normal queue has limited size, so if it was used here, a deadlock could occur if all workers try to push new tasks to a full queue)</p> Source code in <code>dp3/task_processing/task_distributor.py</code> <pre><code>def push_new_tasks(self, new_tasks):\n    \"\"\"Push new tasks (resulting from hooks) to the priority queue.\n\n    (priority queue is not limited in size, so put_task() never blocks; the normal\n    queue has limited size, so if it was used here, a deadlock could occur if all\n    workers try to push new tasks to a full queue)\n    \"\"\"\n    try:\n        for task in new_tasks:\n            self._task_queue_writer.put_task(task, priority=True)\n    except Exception as e:\n        self.log.error(f\"Failed to push tasks created from hooks: {e}\")\n</code></pre>"},{"location":"reference/task_processing/task_executor/","title":"task_executor","text":""},{"location":"reference/task_processing/task_executor/#dp3.task_processing.task_executor","title":"dp3.task_processing.task_executor","text":""},{"location":"reference/task_processing/task_executor/#dp3.task_processing.task_executor.TaskExecutor","title":"TaskExecutor","text":"<pre><code>TaskExecutor(db: EntityDatabase, platform_config: PlatformConfig, elog: EventGroupType, elog_by_src: EventGroupType)\n</code></pre> <p>TaskExecutor manages updates of entity records, which are being read from task queue (via parent <code>TaskDistributor</code>)</p> <p>Parameters:</p> Name Type Description Default <code>db</code> <code>EntityDatabase</code> <p>Instance of EntityDatabase</p> required <code>platform_config</code> <code>PlatformConfig</code> <p>Current platform configuration.</p> required Source code in <code>dp3/task_processing/task_executor.py</code> <pre><code>def __init__(\n    self,\n    db: EntityDatabase,\n    platform_config: PlatformConfig,\n    elog: EventGroupType,\n    elog_by_src: EventGroupType,\n) -&gt; None:\n    # initialize task distribution\n\n    self.log = logging.getLogger(\"TaskExecutor\")\n\n    # Get list of configured entity types\n    self.entity_types = list(platform_config.model_spec.entities.keys())\n    self.log.debug(f\"Configured entity types: {self.entity_types}\")\n\n    self.model_spec = platform_config.model_spec\n    self.db = db\n\n    # Event logging\n    self.elog = elog\n    self.elog_by_src = elog_by_src\n    # Print warning if some event group is not configured\n    not_configured_groups = []\n    if isinstance(self.elog, DummyEventGroup):\n        not_configured_groups.append(\"te\")\n    if isinstance(self.elog_by_src, DummyEventGroup):\n        not_configured_groups.append(\"tasks_by_src\")\n    if not_configured_groups:\n        self.log.warning(\n            \"EventCountLogger: No configuration for event group(s) \"\n            f\"'{','.join(not_configured_groups)}' found, \"\n            \"such events will not be logged (check event_logging.yml)\"\n        )\n\n    # Hooks\n    self._task_generic_hooks = TaskGenericHooksContainer(self.log, self.elog)\n    self._task_entity_hooks = {}\n    self._task_attr_hooks = {}\n\n    for entity in self.model_spec.entities:\n        self._task_entity_hooks[entity] = TaskEntityHooksContainer(\n            entity, self.model_spec, self.log, self.elog\n        )\n\n    for entity, attr in self.model_spec.attributes:\n        attr_type = self.model_spec.attributes[entity, attr].t\n        self._task_attr_hooks[entity, attr] = TaskAttrHooksContainer(\n            entity, attr, attr_type, self.model_spec, self.log, self.elog\n        )\n</code></pre>"},{"location":"reference/task_processing/task_executor/#dp3.task_processing.task_executor.TaskExecutor.register_task_hook","title":"register_task_hook","text":"<pre><code>register_task_hook(hook_type: str, hook: Callable)\n</code></pre> <p>Registers one of available task hooks</p> <p>See: <code>TaskGenericHooksContainer</code> in <code>task_hooks.py</code></p> Source code in <code>dp3/task_processing/task_executor.py</code> <pre><code>def register_task_hook(self, hook_type: str, hook: Callable):\n    \"\"\"Registers one of available task hooks\n\n    See: [`TaskGenericHooksContainer`][dp3.task_processing.task_hooks.TaskGenericHooksContainer]\n    in `task_hooks.py`\n    \"\"\"\n    self._task_generic_hooks.register(hook_type, hook)\n</code></pre>"},{"location":"reference/task_processing/task_executor/#dp3.task_processing.task_executor.TaskExecutor.register_entity_hook","title":"register_entity_hook","text":"<pre><code>register_entity_hook(hook_type: str, hook: Callable, entity: str)\n</code></pre> <p>Registers one of available task entity hooks</p> <p>See: <code>TaskEntityHooksContainer</code> in <code>task_hooks.py</code></p> Source code in <code>dp3/task_processing/task_executor.py</code> <pre><code>def register_entity_hook(self, hook_type: str, hook: Callable, entity: str):\n    \"\"\"Registers one of available task entity hooks\n\n    See: [`TaskEntityHooksContainer`][dp3.task_processing.task_hooks.TaskEntityHooksContainer]\n    in `task_hooks.py`\n    \"\"\"\n    self._task_entity_hooks[entity].register(hook_type, hook)\n</code></pre>"},{"location":"reference/task_processing/task_executor/#dp3.task_processing.task_executor.TaskExecutor.register_attr_hook","title":"register_attr_hook","text":"<pre><code>register_attr_hook(hook_type: str, hook: Callable, entity: str, attr: str)\n</code></pre> <p>Registers one of available task attribute hooks</p> <p>See: <code>TaskAttrHooksContainer</code> in <code>task_hooks.py</code></p> Source code in <code>dp3/task_processing/task_executor.py</code> <pre><code>def register_attr_hook(self, hook_type: str, hook: Callable, entity: str, attr: str):\n    \"\"\"Registers one of available task attribute hooks\n\n    See: [`TaskAttrHooksContainer`][dp3.task_processing.task_hooks.TaskAttrHooksContainer]\n    in `task_hooks.py`\n    \"\"\"\n    self._task_attr_hooks[entity, attr].register(hook_type, hook)\n</code></pre>"},{"location":"reference/task_processing/task_executor/#dp3.task_processing.task_executor.TaskExecutor.process_task","title":"process_task","text":"<pre><code>process_task(task: DataPointTask) -&gt; tuple[bool, list[DataPointTask]]\n</code></pre> <p>Main processing function - push datapoint values, running all registered hooks.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>DataPointTask</code> <p>Task object to process.</p> required <p>Returns:     True if a new record was created, False otherwise,     and a list of new tasks created by hooks</p> Source code in <code>dp3/task_processing/task_executor.py</code> <pre><code>def process_task(self, task: DataPointTask) -&gt; tuple[bool, list[DataPointTask]]:\n    \"\"\"\n    Main processing function - push datapoint values, running all registered hooks.\n\n    Args:\n        task: Task object to process.\n    Returns:\n        True if a new record was created, False otherwise,\n        and a list of new tasks created by hooks\n    \"\"\"\n    self.log.debug(f\"Received new task {task.etype}/{task.eid}, starting processing!\")\n\n    new_tasks = []\n\n    # Run on_task_start hook\n    self._task_generic_hooks.run_on_start(task)\n\n    # Check existence of etype\n    if task.etype not in self.entity_types:\n        self.log.error(f\"Task {task.etype}/{task.eid}: Unknown entity type!\")\n        self.elog.log(\"task_processing_error\")\n        return False, new_tasks\n\n    # Check existence of eid\n    try:\n        ekey_exists = self.db.ekey_exists(task.etype, task.eid)\n    except DatabaseError as e:\n        self.log.error(f\"Task {task.etype}/{task.eid}: DB error: {e}\")\n        self.elog.log(\"task_processing_error\")\n        return False, new_tasks\n\n    new_entity = not ekey_exists\n\n    if task.delete:\n        return self._process_delete(task, new_entity)\n\n    if new_entity:\n        # Run allow_entity_creation hook\n        if not self._task_entity_hooks[task.etype].run_allow_creation(task.eid, task):\n            self.log.debug(\n                f\"Task {task.etype}/{task.eid}: hooks decided not to create new eid record\"\n            )\n            return False, new_tasks\n\n        # Run on_entity_creation hook\n        new_tasks += self._task_entity_hooks[task.etype].run_on_creation(task.eid, task)\n\n    # Extend TTL\n    if task.ttl_tokens:\n        self.db.extend_ttl(task.etype, task.eid, task.ttl_tokens)\n\n    # Insert into database\n    try:\n        self.db.insert_datapoints(task.eid, task.data_points, new_entity=new_entity)\n        self.log.debug(f\"Task {task.etype}/{task.eid}: All changes written to DB\")\n    except DatabaseError as e:\n        self.log.error(f\"Task {task.etype}/{task.eid}: DB error: {e}\")\n        self.elog.log(\"task_processing_error\")\n        return False, new_tasks\n\n    # Run attribute hooks\n    for dp in task.data_points:\n        new_tasks += self._task_attr_hooks[dp.etype, dp.attr].run_on_new(dp.eid, dp)\n\n    # Log the processed task\n    self.elog.log(\"task_processed\")\n    for dp in task.data_points:\n        if dp.src:\n            self.elog_by_src.log(dp.src)\n    if new_entity:\n        self.elog.log(\"record_created\")\n\n    self.log.debug(f\"Secondary modules created {len(new_tasks)} new tasks.\")\n\n    return new_entity, new_tasks\n</code></pre>"},{"location":"reference/task_processing/task_executor/#dp3.task_processing.task_executor.TaskExecutor.refresh_on_entity_creation","title":"refresh_on_entity_creation","text":"<pre><code>refresh_on_entity_creation(etype: str, worker_id: int, worker_cnt: int) -&gt; list[DataPointTask]\n</code></pre> <p>Runs the <code>on_entity_creation</code> hook for all entities of given type.</p> <p>Returns:</p> Type Description <code>list[DataPointTask]</code> <p>List of new tasks created by hooks</p> Source code in <code>dp3/task_processing/task_executor.py</code> <pre><code>def refresh_on_entity_creation(\n    self, etype: str, worker_id: int, worker_cnt: int\n) -&gt; list[DataPointTask]:\n    \"\"\"Runs the `on_entity_creation` hook for all entities of given type.\n\n    Returns:\n        List of new tasks created by hooks\n    \"\"\"\n    if etype not in self.entity_types:\n        self.log.error(f\"Refresh {etype} on_entity_creation: Unknown entity type!\")\n        self.elog.log(\"task_processing_error\")\n\n    new_tasks = []\n\n    projection = {\"_id\": True}\n    for master_record in self.db.get_worker_master_records(\n        worker_id, worker_cnt, etype, projection=projection\n    ):\n        with task_context(self.model_spec):\n            task = DataPointTask(etype=etype, eid=master_record[\"_id\"])\n            self.log.debug(f\"Refreshing {etype}/{task.eid}\")\n            new_tasks += self._task_entity_hooks[task.etype].run_on_creation(task.eid, task)\n\n    return new_tasks\n</code></pre>"},{"location":"reference/task_processing/task_hooks/","title":"task_hooks","text":""},{"location":"reference/task_processing/task_hooks/#dp3.task_processing.task_hooks","title":"dp3.task_processing.task_hooks","text":""},{"location":"reference/task_processing/task_hooks/#dp3.task_processing.task_hooks.TaskGenericHooksContainer","title":"TaskGenericHooksContainer","text":"<pre><code>TaskGenericHooksContainer(log: Logger, elog: EventGroupType)\n</code></pre> <p>Container for generic hooks</p> <p>Possible hooks:</p> <ul> <li><code>on_task_start</code>: receives Task, no return value requirements</li> </ul> Source code in <code>dp3/task_processing/task_hooks.py</code> <pre><code>def __init__(self, log: logging.Logger, elog: EventGroupType):\n    self.log = log.getChild(\"genericHooks\")\n    self.elog = elog\n\n    self._on_start = []\n</code></pre>"},{"location":"reference/task_processing/task_hooks/#dp3.task_processing.task_hooks.TaskEntityHooksContainer","title":"TaskEntityHooksContainer","text":"<pre><code>TaskEntityHooksContainer(entity: str, model_spec: ModelSpec, log: Logger, elog: EventGroupType)\n</code></pre> <p>Container for entity hooks</p> <p>Possible hooks:</p> <ul> <li><code>allow_entity_creation</code>: receives eid and Task, may prevent entity record creation (by       returning False)</li> <li><code>on_entity_creation</code>: receives eid and Task, may return list of DataPointTasks</li> </ul> Source code in <code>dp3/task_processing/task_hooks.py</code> <pre><code>def __init__(\n    self, entity: str, model_spec: ModelSpec, log: logging.Logger, elog: EventGroupType\n):\n    self.entity = entity\n    self.log = log.getChild(f\"entityHooks.{entity}\")\n    self.elog = elog\n    self.model_spec = model_spec\n\n    self._allow_creation = []\n    self._on_creation = []\n</code></pre>"},{"location":"reference/task_processing/task_hooks/#dp3.task_processing.task_hooks.TaskAttrHooksContainer","title":"TaskAttrHooksContainer","text":"<pre><code>TaskAttrHooksContainer(entity: str, attr: str, attr_type: AttrType, model_spec: ModelSpec, log: Logger, elog: EventGroupType)\n</code></pre> <p>Container for attribute hooks</p> <p>Possible hooks:</p> <ul> <li><code>on_new_plain</code>, <code>on_new_observation</code>, <code>on_new_ts_chunk</code>:     receives eid and DataPointBase, may return a list of DataPointTasks</li> </ul> Source code in <code>dp3/task_processing/task_hooks.py</code> <pre><code>def __init__(\n    self,\n    entity: str,\n    attr: str,\n    attr_type: AttrType,\n    model_spec: ModelSpec,\n    log: logging.Logger,\n    elog: EventGroupType,\n):\n    self.entity = entity\n    self.attr = attr\n    self.log = log.getChild(f\"attributeHooks.{entity}.{attr}\")\n    self.elog = elog\n    self.model_spec = model_spec\n\n    if attr_type == AttrType.PLAIN:\n        self.on_new_hook_type = \"on_new_plain\"\n    elif attr_type == AttrType.OBSERVATIONS:\n        self.on_new_hook_type = \"on_new_observation\"\n    elif attr_type == AttrType.TIMESERIES:\n        self.on_new_hook_type = \"on_new_ts_chunk\"\n    else:\n        raise ValueError(f\"Invalid attribute type '{attr_type}'\")\n\n    self._on_new = []\n</code></pre>"},{"location":"reference/task_processing/task_queue/","title":"task_queue","text":""},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue","title":"dp3.task_processing.task_queue","text":"<p>Functions to work with the main task queue (RabbitMQ)</p> <p>There are two queues for each worker process: - \"normal\" queue for tasks added by other components, this has a limit of 100   tasks. - \"priority\" one for tasks added by workers themselves, this has no limit since   workers mustn't be stopped by waiting for the queue.</p> <p>These queues are presented as a single one by this wrapper. The TaskQueueReader first looks into the \"priority\" queue and only if there is no task waiting, it reads the normal one.</p> <p>Tasks are distributed to worker processes (and threads) by hash of the entity which is to be modified. The destination queue is decided by the message source, so each source must know how many worker processes are there.</p> <p>Exchange and queues must be declared externally!</p> <p>Related configuration keys and their defaults: (should be part of global DP3 config files) <pre><code>rabbitmq:\n  host: localhost\n  port: 5672\n  virtual_host: /\n  username: guest\n  password: guest\n\nworker_processes: 1\n</code></pre></p>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.RobustAMQPConnection","title":"RobustAMQPConnection","text":"<pre><code>RobustAMQPConnection(rabbit_config: dict = None)\n</code></pre> <p>Common TaskQueue wrapper, handles connection to RabbitMQ server with automatic reconnection. TaskQueueWriter and TaskQueueReader are derived from this.</p> <p>Parameters:</p> Name Type Description Default <code>rabbit_config</code> <code>dict</code> <p>RabbitMQ connection parameters, dict with following keys (all optional): host, port, virtual_host, username, password</p> <code>None</code> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def __init__(self, rabbit_config: dict = None) -&gt; None:\n    rabbit_config = {} if rabbit_config is None else rabbit_config\n    self.log = logging.getLogger(\"RobustAMQPConnection\")\n    self.conn_params = {\n        \"hostname\": rabbit_config.get(\"host\", \"localhost\"),\n        \"port\": int(rabbit_config.get(\"port\", 5672)),\n        \"virtual_host\": rabbit_config.get(\"virtual_host\", \"/\"),\n        \"username\": rabbit_config.get(\"username\", \"guest\"),\n        \"password\": rabbit_config.get(\"password\", \"guest\"),\n    }\n    self.connection: amqpstorm.Connection = None\n    self.channel: amqpstorm.Channel = None\n    self._connection_id = 0\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.RobustAMQPConnection.connect","title":"connect","text":"<pre><code>connect() -&gt; None\n</code></pre> <p>Create a connection (or reconnect after error).</p> <p>If connection can't be established, try it again indefinitely.</p> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def connect(self) -&gt; None:\n    \"\"\"Create a connection (or reconnect after error).\n\n    If connection can't be established, try it again indefinitely.\n    \"\"\"\n    if self.connection:\n        self.connection.close()\n    self._connection_id += 1\n\n    attempts = 0\n    while True:\n        attempts += 1\n        try:\n            self.connection = amqpstorm.Connection(**self.conn_params)\n            self.log.debug(\n                \"AMQP connection created, server: \"\n                \"'{hostname}:{port}/{virtual_host}'\".format_map(self.conn_params)\n            )\n            if attempts &gt; 1:\n                # This was a repeated attempt, print success message with ERROR level\n                self.log.error(\"... it's OK now, we're successfully connected!\")\n\n            self.channel = self.connection.channel()\n            self.channel.confirm_deliveries()\n            self.channel.basic.qos(PREFETCH_COUNT)\n            break\n        except amqpstorm.AMQPError as e:\n            sleep_time = RECONNECT_DELAYS[min(attempts, len(RECONNECT_DELAYS)) - 1]\n            self.log.error(\n                f\"RabbitMQ connection error (will try to reconnect in {sleep_time}s): {e}\"\n            )\n            time.sleep(sleep_time)\n        except KeyboardInterrupt:\n            break\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueWriter","title":"TaskQueueWriter","text":"<pre><code>TaskQueueWriter(app_name: str, workers: int = 1, rabbit_config: dict = None, exchange: str = None, priority_exchange: str = None, parent_logger: Logger = None)\n</code></pre> <p>               Bases: <code>RobustAMQPConnection</code></p> <p>Writes tasks into main Task Queue</p> <p>Parameters:</p> Name Type Description Default <code>app_name</code> <code>str</code> <p>DP3 application name (used as prefix for RMQ queues and exchanges)</p> required <code>workers</code> <code>int</code> <p>Number of worker processes in the system</p> <code>1</code> <code>rabbit_config</code> <code>dict</code> <p>RabbitMQ connection parameters, dict with following keys (all optional): host, port, virtual_host, username, password</p> <code>None</code> <code>exchange</code> <code>str</code> <p>Name of the exchange to write tasks to (default: <code>\"&lt;app-name&gt;-main-task-exchange\"</code>)</p> <code>None</code> <code>priority_exchange</code> <code>str</code> <p>Name of the exchange to write priority tasks to (default: <code>\"&lt;app-name&gt;-priority-task-exchange\"</code>)</p> <code>None</code> <code>parent_logger</code> <code>Logger</code> <p>Logger to inherit prefix from.</p> <code>None</code> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def __init__(\n    self,\n    app_name: str,\n    workers: int = 1,\n    rabbit_config: dict = None,\n    exchange: str = None,\n    priority_exchange: str = None,\n    parent_logger: logging.Logger = None,\n) -&gt; None:\n    rabbit_config = {} if rabbit_config is None else rabbit_config\n    assert isinstance(workers, int) and workers &gt;= 1, \"count of workers must be positive number\"\n    assert isinstance(exchange, str) or exchange is None, \"exchange argument has to be string!\"\n    assert (\n        isinstance(priority_exchange, str) or priority_exchange is None\n    ), \"priority_exchange has to be string\"\n\n    super().__init__(rabbit_config)\n\n    if parent_logger is not None:\n        self.log = parent_logger.getChild(\"TaskQueueWriter\")\n    else:\n        self.log = logging.getLogger(\"TaskQueueWriter\")\n\n    if exchange is None:\n        exchange = DEFAULT_EXCHANGE.format(app_name)\n    if priority_exchange is None:\n        priority_exchange = DEFAULT_PRIORITY_EXCHANGE.format(app_name)\n\n    self.workers = workers\n    self.exchange = exchange\n    self.exchange_pri = priority_exchange\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueWriter.check","title":"check","text":"<pre><code>check() -&gt; bool\n</code></pre> <p>Check that needed exchanges are declared, return True or raise RuntimeError.</p> <p>If needed exchanges are not declared, reconnect and try again. (max 5 times)</p> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def check(self) -&gt; bool:\n    \"\"\"\n    Check that needed exchanges are declared, return True or raise RuntimeError.\n\n    If needed exchanges are not declared, reconnect and try again. (max 5 times)\n    \"\"\"\n    for attempt, sleep_time in enumerate(RECONNECT_DELAYS):\n        if self.check_exchange_existence(self.exchange) and self.check_exchange_existence(\n            self.exchange_pri\n        ):\n            return True\n        self.log.warning(\n            \"RabbitMQ exchange configuration doesn't match (attempt %d of %d, retrying in %ds)\",\n            attempt + 1,\n            len(RECONNECT_DELAYS),\n            sleep_time,\n        )\n        time.sleep(sleep_time)\n        self.disconnect()\n        self.connect()\n    if not self.check_exchange_existence(self.exchange):\n        raise ExchangeNotDeclared(self.exchange)\n    if not self.check_exchange_existence(self.exchange_pri):\n        raise ExchangeNotDeclared(self.exchange_pri)\n    return True\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueWriter.broadcast_task","title":"broadcast_task","text":"<pre><code>broadcast_task(task: Task, priority: bool = False) -&gt; None\n</code></pre> <p>Broadcast task to all workers</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>prepared task</p> required <code>priority</code> <code>bool</code> <p>if true, the task is placed into priority queue (should only be used internally by workers)</p> <code>False</code> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def broadcast_task(self, task: Task, priority: bool = False) -&gt; None:\n    \"\"\"\n    Broadcast task to all workers\n\n    Args:\n        task: prepared task\n        priority: if true, the task is placed into priority queue\n            (should only be used internally by workers)\n    \"\"\"\n    if not self.channel:\n        self.connect()\n\n    task_str = str(task) if len(str(task)) &lt; 500 else f\"{str(task)[:500]}...(truncated)\"\n    self.log.debug(f\"Received new broadcast task: {task_str}\")\n\n    body = task.as_message()\n    exchange = self.exchange_pri if priority else self.exchange\n\n    for routing_key in range(self.workers):\n        self._send_message(routing_key, exchange, body)\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueWriter.put_task","title":"put_task","text":"<pre><code>put_task(task: Task, priority: bool = False) -&gt; None\n</code></pre> <p>Put task (update_request) to the queue of corresponding worker</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>prepared task</p> required <code>priority</code> <code>bool</code> <p>if true, the task is placed into priority queue (should only be used internally by workers)</p> <code>False</code> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def put_task(self, task: Task, priority: bool = False) -&gt; None:\n    \"\"\"\n    Put task (update_request) to the queue of corresponding worker\n\n    Args:\n        task: prepared task\n        priority: if true, the task is placed into priority queue\n            (should only be used internally by workers)\n    \"\"\"\n    if not self.channel:\n        self.connect()\n\n    task_str = str(task) if len(str(task)) &lt; 500 else f\"{str(task)[:500]}...(truncated)\"\n    self.log.debug(f\"Received new task: {task_str}\")\n\n    # Prepare routing key\n    body = task.as_message()\n    # index of the worker to send the task to\n    routing_key = task.hashed_routing_key() % self.workers\n\n    exchange = self.exchange_pri if priority else self.exchange\n    self._send_message(routing_key, exchange, body)\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueReader","title":"TaskQueueReader","text":"<pre><code>TaskQueueReader(callback: Callable, parse_task: Callable[[str], Task], app_name: str, worker_index: int = 0, rabbit_config: dict = None, queue: str = None, priority_queue: Union[str, bool] = None, parent_logger: Logger = None)\n</code></pre> <p>               Bases: <code>RobustAMQPConnection</code></p> <p>TaskQueueReader consumes messages from two RabbitMQ queues (normal and priority one for given worker) and passes them to the given callback function.</p> <p>Tasks from the priority queue are passed before the normal ones.</p> <p>Each received message must be acknowledged by calling <code>.ack(msg_tag)</code>.</p> <p>Parameters:</p> Name Type Description Default <code>callback</code> <code>Callable</code> <p>Function called when a message is received, prototype: func(tag, Task)</p> required <code>parse_task</code> <code>Callable[[str], Task]</code> <p>Function called to parse message body into a task, prototype: func(body) -&gt; Task</p> required <code>app_name</code> <code>str</code> <p>DP3 application name (used as prefix for RMQ queues and exchanges)</p> required <code>worker_index</code> <code>int</code> <p>index of this worker (filled into DEFAULT_QUEUE string using .format() method)</p> <code>0</code> <code>rabbit_config</code> <code>dict</code> <p>RabbitMQ connection parameters, dict with following keys (all optional): host, port, virtual_host, username, password</p> <code>None</code> <code>queue</code> <code>str</code> <p>Name of RabbitMQ queue to read from (default: <code>\"&lt;app-name&gt;-worker-&lt;index&gt;\"</code>)</p> <code>None</code> <code>priority_queue</code> <code>Union[str, bool]</code> <p>Name of RabbitMQ queue to read from (priority messages) or <code>False</code> to disable. (default: <code>\"&lt;app-name&gt;-worker-&lt;index&gt;-pri\"</code>)</p> <code>None</code> <code>parent_logger</code> <code>Logger</code> <p>Logger to inherit prefix from.</p> <code>None</code> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def __init__(\n    self,\n    callback: Callable,\n    parse_task: Callable[[str], Task],\n    app_name: str,\n    worker_index: int = 0,\n    rabbit_config: dict = None,\n    queue: str = None,\n    priority_queue: Union[str, bool] = None,\n    parent_logger: logging.Logger = None,\n) -&gt; None:\n    rabbit_config = {} if rabbit_config is None else rabbit_config\n    assert callable(callback), \"callback must be callable object\"\n    assert (\n        isinstance(worker_index, int) and worker_index &gt;= 0\n    ), \"worker_index must be positive number\"\n    assert isinstance(queue, str) or queue is None, \"queue must be string\"\n    assert (\n        isinstance(priority_queue, str) or priority_queue is None or priority_queue is False\n    ), \"priority_queue must be string or False to disable\"\n\n    super().__init__(rabbit_config)\n\n    if parent_logger is not None:\n        self.log = parent_logger.getChild(\"TaskQueueReader\")\n    else:\n        self.log = logging.getLogger(\"TaskQueueReader\")\n\n    self.callback = callback\n    self.parse_task = parse_task\n\n    if queue is None:\n        queue = DEFAULT_QUEUE.format(app_name, worker_index)\n    if priority_queue is None:\n        priority_queue = DEFAULT_PRIORITY_QUEUE.format(app_name, worker_index)\n    elif priority_queue is False:\n        priority_queue = None\n    self.queue_name = queue\n    self.priority_queue_name = priority_queue\n    self.worker_index = worker_index\n\n    self.running = False\n\n    self._consuming_thread = None\n    self._processing_thread = None\n\n    # Receive messages into 2 temporary queues\n    # (max length should be equal to prefetch_count set in RabbitMQReader)\n    self.cache = collections.deque()\n    self.cache_pri = collections.deque()\n    self.cache_full = threading.Event()  # signalize there's something in the cache\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueReader.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start receiving tasks.</p> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start receiving tasks.\"\"\"\n    if self.running:\n        raise RuntimeError(\"Already running\")\n\n    if not self.connection:\n        self.connect()\n\n    self.log.info(\"Starting TaskQueueReader\")\n\n    # Start thread for message consuming from server\n    self.running = True\n    self._consuming_thread = threading.Thread(target=self._consuming_thread_func)\n    thread_n = self._consuming_thread.name.split(\"-\")[-1]\n    self._consuming_thread.name = f\"Consumer-{self.worker_index}-{thread_n}\"\n    self._consuming_thread.start()\n\n    # Start thread for message processing and passing to user's callback\n    self._processing_thread = threading.Thread(target=self._msg_processing_thread_func)\n    thread_n = self._processing_thread.name.split(\"-\")[-1]\n    self._processing_thread.name = f\"Processor-{self.worker_index}-{thread_n}\"\n    self._processing_thread.start()\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueReader.stop","title":"stop","text":"<pre><code>stop() -&gt; None\n</code></pre> <p>Stop receiving tasks.</p> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def stop(self) -&gt; None:\n    \"\"\"Stop receiving tasks.\"\"\"\n    if not self.running:\n        raise RuntimeError(\"Not running\")\n\n    self.running = False\n    self._stop_consuming_thread()\n    self._stop_processing_thread()\n    self.log.info(\"TaskQueueReader stopped\")\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueReader.reconnect","title":"reconnect","text":"<pre><code>reconnect() -&gt; None\n</code></pre> <p>Clear local message cache and reconnect to RabbitMQ server.</p> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def reconnect(self) -&gt; None:\n    \"\"\"Clear local message cache and reconnect to RabbitMQ server.\"\"\"\n    self.cache.clear()\n    self.cache_pri.clear()\n\n    self.connect()\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueReader.check","title":"check","text":"<pre><code>check() -&gt; bool\n</code></pre> <p>Check that needed queues are declared, return True or raise RuntimeError.</p> <p>If needed queues are not declared, reconnect and try again. (max 5 times)</p> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def check(self) -&gt; bool:\n    \"\"\"\n    Check that needed queues are declared, return True or raise RuntimeError.\n\n    If needed queues are not declared, reconnect and try again. (max 5 times)\n    \"\"\"\n\n    for attempt, sleep_time in enumerate(RECONNECT_DELAYS):\n        if self.check_queue_existence(self.queue_name) and self.check_queue_existence(\n            self.priority_queue_name\n        ):\n            return True\n        self.log.warning(\n            \"RabbitMQ queue configuration doesn't match (attempt %d of %d, retrying in %ds)\",\n            attempt + 1,\n            len(RECONNECT_DELAYS),\n            sleep_time,\n        )\n        time.sleep(sleep_time)\n        self.disconnect()\n        self.connect()\n    if not self.check_queue_existence(self.queue_name):\n        raise QueueNotDeclared(self.queue_name)\n    if not self.check_queue_existence(self.priority_queue_name):\n        raise QueueNotDeclared(self.priority_queue_name)\n    return True\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueReader.ack","title":"ack","text":"<pre><code>ack(msg_tag: tuple[int, int]) -&gt; bool\n</code></pre> <p>Acknowledge processing of the message/task</p> <p>Will reconnect by itself on channel error. Args:     msg_tag: Message tag received as the first param of the callback function. Returns:     Whether the message was acknowledged successfully and can be processed further.</p> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def ack(self, msg_tag: tuple[int, int]) -&gt; bool:\n    \"\"\"Acknowledge processing of the message/task\n\n    Will reconnect by itself on channel error.\n    Args:\n        msg_tag: Message tag received as the first param of the callback function.\n    Returns:\n        Whether the message was acknowledged successfully and can be processed further.\n    \"\"\"\n    conn_id, msg_tag = msg_tag\n    if conn_id != self._connection_id:\n        return False\n    try:\n        self.channel.basic.ack(delivery_tag=msg_tag)\n    except amqpstorm.AMQPChannelError as why:\n        self.log.error(\"Channel error while acknowledging message: %s\", why)\n        self.reconnect()\n        return False\n    return True\n</code></pre>"},{"location":"reference/task_processing/task_queue/#dp3.task_processing.task_queue.TaskQueueReader.watchdog","title":"watchdog","text":"<pre><code>watchdog()\n</code></pre> <p>Check whether both threads are running and perform a reset if not.</p> <p>Register to be called periodically by scheduler.</p> Source code in <code>dp3/task_processing/task_queue.py</code> <pre><code>def watchdog(self):\n    \"\"\"\n    Check whether both threads are running and perform a reset if not.\n\n    Register to be called periodically by scheduler.\n    \"\"\"\n    proc = self._processing_thread.is_alive()\n    cons = self._consuming_thread.is_alive()\n\n    if not proc or not cons:\n        self.log.error(\n            \"Dead threads detected, processing=%s, consuming=%s, restarting TaskQueueReader.\",\n            \"alive\" if proc else \"dead\",\n            \"alive\" if cons else \"dead\",\n        )\n        self._stop_consuming_thread()\n        self._stop_processing_thread()\n\n        self.channel.close()\n        self.channel = None\n        self.cache.clear()\n        self.cache_pri.clear()\n\n        self.connect()\n        self.start()\n</code></pre>"}]}